{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogitLens Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the LogitLens tool to analyze intermediate representations in transformer models. LogitLens lets you project hidden states from any layer through the output embedding to see what 'token' would be predicted at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: /orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine\n"
     ]
    }
   ],
   "source": [
    "from easyroutine.utils import path_to_parents\n",
    "path_to_parents(1)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/dssc/francescortu/VisualComp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/u/dssc/francescortu/VisualComp/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from easyroutine.interpretability import HookedModel, ExtractionConfig\n",
    "from easyroutine.interpretability.tools import LogitLens\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up\n",
    "\n",
    "First, we'll load a small model. For this tutorial, we'll use a tiny test model, but you can replace this with any model you're interested in studying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/dssc/francescortu/VisualComp/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/13/25 15:47:39] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:47:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">839</span> - HookedModel_stdout - INFO - Model loaded in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         devices. First device: cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/13/25 15:47:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m15:47:39\u001b[0m,\u001b[1;36m839\u001b[0m - HookedModel_stdout - INFO - Model loaded in \u001b[1;36m1\u001b[0m  \u001b]8;id=760429;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=330394;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         devices. First device: cu\u001b[1;92mda:0\u001b[0m                                            \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:47:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">845</span> - HookedModel_stdout - INFO -                    <a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                     The model is using the custom eager          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         attention implementation that support attention matrix hooks because I   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         get config.attn_impelemntation == <span style=\"color: #008000; text-decoration-color: #008000\">'custom_eager'</span>. If you don't want      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         this, you can call HookedModel.restore_original_modules.                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                     However, we reccomend using this             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         implementation since the base one do not contains attention matrix hook  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         resulting in unexpected behaviours.                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m15:47:39\u001b[0m,\u001b[1;36m845\u001b[0m - HookedModel_stdout - INFO -                    \u001b]8;id=346375;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=164228;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m                                     The model is using the custom eager          \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         attention implementation that support attention matrix hooks because I   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         get config.attn_impelemntation == \u001b[32m'custom_eager'\u001b[0m. If you don't want      \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         this, you can call HookedModel.restore_original_modules.                 \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                     However, we reccomend using this             \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         implementation since the base one do not contains attention matrix hook  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         resulting in unexpected behaviours.                                      \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                  \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:47:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">848</span> - HookedModel_stdout - INFO - Setting custom     <a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         modules.                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m15:47:39\u001b[0m,\u001b[1;36m848\u001b[0m - HookedModel_stdout - INFO - Setting custom     \u001b]8;id=360975;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=754820;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         modules.                                                                 \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the tutorial we'll use a tiny test model\n",
    "model = HookedModel.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n",
    "\n",
    "# In practice, you can use any model you want, for example:\n",
    "# model = HookedModel.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize LogitLens\n",
    "\n",
    "The LogitLens tool needs access to the model's unembedding matrix (the output embedding weights) and the final layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LogitLens instance from our model\n",
    "logit_lens = LogitLens.from_model(model)\n",
    "\n",
    "# You can also create it directly from a model name\n",
    "# logit_lens = LogitLens.from_model_name(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing Input and Extracting Activations\n",
    "\n",
    "Now we'll prepare some input data and extract activations from the model. For a real-world analysis, you'd use meaningful text instead of random tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "# For real analysis, use a meaningful prompt\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:47:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">912</span> - HookedModel_stdout - INFO - Extracting cache   <a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m15:47:39\u001b[0m,\u001b[1;36m912\u001b[0m - HookedModel_stdout - INFO - Extracting cache   \u001b]8;id=524725;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=796928;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:47:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">914</span> - HookedModel_stdout - INFO - Forward pass       <a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         started                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m15:47:39\u001b[0m,\u001b[1;36m914\u001b[0m - HookedModel_stdout - INFO - Forward pass       \u001b]8;id=445440;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=548334;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         started                                                                  \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting cache::   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting cache:: 100%|██████████| 2/2 [00:00<00:00,  2.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/13/25 15:47:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">15:47:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">707</span> - HookedModel_stdout - INFO - Forward pass       <a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         finished - started to aggregate different batch                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/13/25 15:47:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m15:47:40\u001b[0m,\u001b[1;36m707\u001b[0m - HookedModel_stdout - INFO - Forward pass       \u001b]8;id=933108;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=906604;file:///orfeo/cephfs/home/dssc/francescortu/VisualComp/easyroutine/easyroutine/logger.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         finished - started to aggregate different batch                          \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For demo purposes, we'll also create a simple fake dataset\n",
    "fake_dataset = [\n",
    "    {\n",
    "        \"input_ids\": torch.randint(0, 100, (1, 20)),\n",
    "        \"attention_mask\": torch.ones(1, 20),\n",
    "    },\n",
    "    {\n",
    "        \"input_ids\": torch.randint(0, 100, (1, 20)),\n",
    "        \"attention_mask\": torch.ones(1, 20),\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract activations for all layers - we need residual stream outputs\n",
    "# and the final layer norm for best results\n",
    "cache = model.extract_cache(\n",
    "    [inputs],\n",
    "    target_token_positions=[\"last\"],\n",
    "    extraction_config=ExtractionConfig(\n",
    "        extract_resid_out=True,\n",
    "        extract_last_layernorm=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what's in our cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache(`resid_out_0, resid_out_1, logits, mapping_index, example_dict`)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the keys in the cache to understand what we have\n",
    "print(\"Cache keys:\")\n",
    "[key for key in cache.keys() if not key.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic LogitLens Analysis\n",
    "\n",
    "Now we'll apply the LogitLens to see what the model 'predicts' at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Logit Lens of resid_out_{i}: 100%|██████████| 2/2 [00:33<00:00, 16.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Number of layers in the model\n",
    "num_layers = model.model_config.num_hidden_layers\n",
    "print(f\"The model has {num_layers} layers\")\n",
    "\n",
    "# Apply logit lens to all layers\n",
    "logit_lens_results = {}\n",
    "for layer in range(num_layers):\n",
    "    out = logit_lens.compute(\n",
    "        activations=cache,\n",
    "        target_key=f\"resid_out_{layer}\",\n",
    "        apply_norm=True,  # Apply layer normalization\n",
    "        apply_softmax=True  # Convert to probabilities\n",
    "    )\n",
    "    logit_lens_results[layer] = out[f\"logit_lens_resid_out_{layer}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the shape of the output for one layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 32000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the shape of the LogitLens output\n",
    "layer = 0\n",
    "print(f\"Shape of logit lens output for layer {layer}:\")\n",
    "logit_lens_results[layer].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Results\n",
    "\n",
    "Let's extract the top-k predicted tokens at each layer and see how they evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tokens(layer_results, k=5):\n",
    "    \"\"\"Get top-k token predictions for the last token in the sequence.\"\"\"\n",
    "    # Get probabilities for the last token position\n",
    "    token_probs = layer_results[0, -1]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_k = torch.topk(token_probs, k)\n",
    "    \n",
    "    # Convert token ids to strings\n",
    "    tokens = [tokenizer.decode(idx.item()) for idx in top_k.indices]\n",
    "    probs = [prob.item() for prob in top_k.values]\n",
    "    \n",
    "    return tokens, probs\n",
    "\n",
    "# Get top-5 predictions for each layer\n",
    "all_layer_predictions = {}\n",
    "for layer in range(num_layers):\n",
    "    tokens, probs = get_top_k_tokens(logit_lens_results[layer], k=5)\n",
    "    all_layer_predictions[layer] = list(zip(tokens, probs))\n",
    "\n",
    "# Create a DataFrame to display the evolution of predictions through layers\n",
    "predictions_df = pd.DataFrame(\n",
    "    {f\"Layer {layer}\": [f\"{t} ({p:.3f})\" for t, p in preds] \n",
    "     for layer, preds in all_layer_predictions.items()}\n",
    ")\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Token Directions\n",
    "\n",
    "We can also compute logit differences between specific tokens to see how the model's 'preference' changes across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some interesting token pairs to compare\n",
    "# If this were a real example using \"The capital of France is\",\n",
    "# we might compare \"Paris\" vs \"London\"\n",
    "\n",
    "# Get token IDs for comparison\n",
    "target_tokens = [\" Paris\", \" London\"]\n",
    "token_ids = [tokenizer.encode(t)[0] for t in target_tokens]\n",
    "\n",
    "# Compute logit differences across all layers\n",
    "logit_diffs = []\n",
    "for layer in range(num_layers):\n",
    "    result = logit_lens.compute(\n",
    "        activations=cache,\n",
    "        target_key=f\"resid_out_{layer}\",\n",
    "        token_directions=[(token_ids[0], token_ids[1])],\n",
    "        metric=\"logit_diff\",\n",
    "        apply_norm=True\n",
    "    )\n",
    "    # Extract the logit difference\n",
    "    logit_diff = result[f\"logit_diff_resid_out_{layer}\"].item()\n",
    "    logit_diffs.append(logit_diff)\n",
    "\n",
    "# Plot the evolution of logit differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_layers), logit_diffs, marker='o')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.title(f'Logit Difference: {target_tokens[0]} vs {target_tokens[1]} Across Layers')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Logit Difference')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Analysis: Visualizing Intermediate Representations\n",
    "\n",
    "Let's create a heatmap to visualize how the probabilities for top tokens evolve across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top-10 tokens from the final layer\n",
    "final_layer = num_layers - 1\n",
    "final_probs = logit_lens_results[final_layer][0, -1]\n",
    "top_indices = torch.topk(final_probs, 10).indices\n",
    "top_tokens = [tokenizer.decode(idx.item()) for idx in top_indices]\n",
    "\n",
    "# Create a probability matrix for these tokens across all layers\n",
    "probs_matrix = np.zeros((num_layers, len(top_tokens)))\n",
    "for layer in range(num_layers):\n",
    "    layer_probs = logit_lens_results[layer][0, -1]\n",
    "    for i, token_idx in enumerate(top_indices):\n",
    "        probs_matrix[layer, i] = layer_probs[token_idx].item()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(probs_matrix, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Probability')\n",
    "plt.xlabel('Top Tokens')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Token Probability Evolution Across Layers')\n",
    "plt.xticks(range(len(top_tokens)), top_tokens, rotation=45, ha='right')\n",
    "plt.yticks(range(num_layers))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this tutorial, we've seen how to:\n",
    "\n",
    "1. Create a LogitLens instance from a model\n",
    "2. Extract activations from the model\n",
    "3. Apply LogitLens to analyze intermediate representations\n",
    "4. Compare specific token directions across layers\n",
    "5. Visualize how predictions evolve through the network\n",
    "\n",
    "LogitLens is a powerful tool for interpreting what happens inside transformer models as information flows through the layers. It can help identify where certain concepts emerge in the model and how representations develop throughout the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
