{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>This libary contains code that I use frequently. I find useful to organize and document the code for future project.</p>"},{"location":"api/console/","title":"Console","text":"<p>The <code>console</code> module provides tools for interacting with the console.</p>"},{"location":"api/console/progress/","title":"Progress","text":""},{"location":"api/console/progress/#easyroutine.console.progress.LoggingProgress","title":"<code>LoggingProgress</code>","text":"<p>A progress tracker designed for batch environments (sbatch, etc.) that outputs clean, consistent progress updates to stdout/stderr.</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>class LoggingProgress:\n    \"\"\"\n    A progress tracker designed for batch environments (sbatch, etc.)\n    that outputs clean, consistent progress updates to stdout/stderr.\n    \"\"\"\n\n    def __init__(self, log_interval: int = 5, update_frequency: int = 0):\n        \"\"\"\n        Initialize the logging progress tracker.\n\n        Args:\n            log_interval: How often to log progress updates (in seconds)\n            update_frequency: Alternative to log_interval - update every N items (0 = use log_interval only)\n        \"\"\"\n        self.tasks = {}\n        self.log_interval = log_interval\n        self.update_frequency = update_frequency\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    def add_task(self, description: str, total: int = None, **kwargs):\n        \"\"\"Add a task to track.\"\"\"\n        task_id = len(self.tasks)\n        self.tasks[task_id] = {\n            \"description\": description,\n            \"total\": total,\n            \"completed\": 0,\n            \"start_time\": time.time(),\n            \"last_log_time\": 0,  # 0 ensures first update is always logged\n            \"last_item_logged\": 0,\n        }\n        if description:\n            print(f\"\\n[PROGRESS] Starting: {description} (Total: {total or 'unknown'})\")\n        return task_id\n\n    def update(self, task_id, advance=1, **kwargs):\n        \"\"\"Update task progress.\"\"\"\n        if task_id not in self.tasks:\n            return\n\n        task = self.tasks[task_id]\n        task[\"completed\"] += advance\n        current_time = time.time()\n\n        # Determine if we should log based on either time interval or item count\n        should_log = False\n\n        # Check time interval\n        if current_time - task[\"last_log_time\"] &gt;= self.log_interval:\n            should_log = True\n\n        # Check item count interval (if specified)\n        if self.update_frequency &gt; 0:\n            items_since_log = task[\"completed\"] - task[\"last_item_logged\"]\n            if items_since_log &gt;= self.update_frequency:\n                should_log = True\n\n        if should_log:\n            elapsed = current_time - task[\"start_time\"]\n            if task[\"total\"]:\n                percentage = (task[\"completed\"] / task[\"total\"]) * 100\n                remaining = (\n                    (elapsed / task[\"completed\"]) * (task[\"total\"] - task[\"completed\"])\n                    if task[\"completed\"] &gt; 0\n                    else 0\n                )\n                print(\n                    f\"[PROGRESS] {task['description']}: {task['completed']}/{task['total']} \"\n                    f\"({percentage:.1f}%) - Elapsed: {format_time(elapsed)}, \"\n                    f\"Remaining: {format_time(remaining)}\"\n                )\n            else:\n                print(\n                    f\"[PROGRESS] {task['description']}: {task['completed']} items - \"\n                    f\"Elapsed: {format_time(elapsed)}\"\n                )\n            task[\"last_log_time\"] = current_time\n            task[\"last_item_logged\"] = task[\"completed\"]\n\n    def track(\n        self, iterable: Iterable[T], total: Optional[int] = None, description: str = \"\"\n    ) -&gt; Iterable[T]:\n        \"\"\"Track progress through an iterable.\"\"\"\n        if total is None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                pass\n\n        task_id = self.add_task(description, total=total)\n        count = 0\n\n        for item in iterable:\n            count += 1\n            yield item\n            self.update(task_id)\n\n        # Final update to show 100% completion\n        if total:\n            elapsed = time.time() - self.tasks[task_id][\"start_time\"]\n            print(\n                f\"[PROGRESS] Complete: {description} - {count}/{total} items in {format_time(elapsed)}\"\n            )\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.LoggingProgress.__init__","title":"<code>__init__(log_interval=5, update_frequency=0)</code>","text":"<p>Initialize the logging progress tracker.</p> <p>Parameters:</p> Name Type Description Default <code>log_interval</code> <code>int</code> <p>How often to log progress updates (in seconds)</p> <code>5</code> <code>update_frequency</code> <code>int</code> <p>Alternative to log_interval - update every N items (0 = use log_interval only)</p> <code>0</code> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def __init__(self, log_interval: int = 5, update_frequency: int = 0):\n    \"\"\"\n    Initialize the logging progress tracker.\n\n    Args:\n        log_interval: How often to log progress updates (in seconds)\n        update_frequency: Alternative to log_interval - update every N items (0 = use log_interval only)\n    \"\"\"\n    self.tasks = {}\n    self.log_interval = log_interval\n    self.update_frequency = update_frequency\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.LoggingProgress.add_task","title":"<code>add_task(description, total=None, **kwargs)</code>","text":"<p>Add a task to track.</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def add_task(self, description: str, total: int = None, **kwargs):\n    \"\"\"Add a task to track.\"\"\"\n    task_id = len(self.tasks)\n    self.tasks[task_id] = {\n        \"description\": description,\n        \"total\": total,\n        \"completed\": 0,\n        \"start_time\": time.time(),\n        \"last_log_time\": 0,  # 0 ensures first update is always logged\n        \"last_item_logged\": 0,\n    }\n    if description:\n        print(f\"\\n[PROGRESS] Starting: {description} (Total: {total or 'unknown'})\")\n    return task_id\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.LoggingProgress.track","title":"<code>track(iterable, total=None, description='')</code>","text":"<p>Track progress through an iterable.</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def track(\n    self, iterable: Iterable[T], total: Optional[int] = None, description: str = \"\"\n) -&gt; Iterable[T]:\n    \"\"\"Track progress through an iterable.\"\"\"\n    if total is None:\n        try:\n            total = len(iterable)\n        except (TypeError, AttributeError):\n            pass\n\n    task_id = self.add_task(description, total=total)\n    count = 0\n\n    for item in iterable:\n        count += 1\n        yield item\n        self.update(task_id)\n\n    # Final update to show 100% completion\n    if total:\n        elapsed = time.time() - self.tasks[task_id][\"start_time\"]\n        print(\n            f\"[PROGRESS] Complete: {description} - {count}/{total} items in {format_time(elapsed)}\"\n        )\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.LoggingProgress.update","title":"<code>update(task_id, advance=1, **kwargs)</code>","text":"<p>Update task progress.</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def update(self, task_id, advance=1, **kwargs):\n    \"\"\"Update task progress.\"\"\"\n    if task_id not in self.tasks:\n        return\n\n    task = self.tasks[task_id]\n    task[\"completed\"] += advance\n    current_time = time.time()\n\n    # Determine if we should log based on either time interval or item count\n    should_log = False\n\n    # Check time interval\n    if current_time - task[\"last_log_time\"] &gt;= self.log_interval:\n        should_log = True\n\n    # Check item count interval (if specified)\n    if self.update_frequency &gt; 0:\n        items_since_log = task[\"completed\"] - task[\"last_item_logged\"]\n        if items_since_log &gt;= self.update_frequency:\n            should_log = True\n\n    if should_log:\n        elapsed = current_time - task[\"start_time\"]\n        if task[\"total\"]:\n            percentage = (task[\"completed\"] / task[\"total\"]) * 100\n            remaining = (\n                (elapsed / task[\"completed\"]) * (task[\"total\"] - task[\"completed\"])\n                if task[\"completed\"] &gt; 0\n                else 0\n            )\n            print(\n                f\"[PROGRESS] {task['description']}: {task['completed']}/{task['total']} \"\n                f\"({percentage:.1f}%) - Elapsed: {format_time(elapsed)}, \"\n                f\"Remaining: {format_time(remaining)}\"\n            )\n        else:\n            print(\n                f\"[PROGRESS] {task['description']}: {task['completed']} items - \"\n                f\"Elapsed: {format_time(elapsed)}\"\n            )\n        task[\"last_log_time\"] = current_time\n        task[\"last_item_logged\"] = task[\"completed\"]\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.format_time","title":"<code>format_time(seconds)</code>","text":"<p>Format seconds into a readable time string.</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def format_time(seconds: float) -&gt; str:\n    \"\"\"Format seconds into a readable time string.\"\"\"\n    if seconds &lt; 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds &lt; 3600:\n        minutes = seconds / 60\n        return f\"{minutes:.1f}m\"\n    else:\n        hours = seconds / 3600\n        return f\"{hours:.1f}h\"\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.get_progress_bar","title":"<code>get_progress_bar(disable=False, force_batch_mode=False, log_interval=1, update_frequency=0)</code>","text":"<p>Returns a progress tracker appropriate for the current environment.</p> <p>In interactive environments (including interactive Slurm sessions), this will use a rich progress bar. In non-interactive batch jobs (like sbatch), it will use simpler text-based output.</p> <p>Parameters:</p> Name Type Description Default <code>disable</code> <code>bool</code> <p>If True, returns a No-Op progress object that does nothing.</p> <code>False</code> <code>force_batch_mode</code> <code>bool</code> <p>If True, use the text-based progress tracker              even in interactive environments.</p> <code>False</code> <code>log_interval</code> <code>int</code> <p>How often (in seconds) to log progress in batch mode.</p> <code>1</code> <code>update_frequency</code> <code>int</code> <p>In batch mode, update progress after this many items             (0 means use only time-based updates)</p> <code>0</code> <p>Returns:</p> Type Description <p>A progress tracker compatible with the current environment.</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def get_progress_bar(\n    disable: bool = False,\n    force_batch_mode: bool = False,\n    log_interval: int = 1,\n    update_frequency: int = 0,\n):\n    \"\"\"\n    Returns a progress tracker appropriate for the current environment.\n\n    In interactive environments (including interactive Slurm sessions),\n    this will use a rich progress bar. In non-interactive batch jobs\n    (like sbatch), it will use simpler text-based output.\n\n    Args:\n        disable: If True, returns a No-Op progress object that does nothing.\n        force_batch_mode: If True, use the text-based progress tracker\n                         even in interactive environments.\n        log_interval: How often (in seconds) to log progress in batch mode.\n        update_frequency: In batch mode, update progress after this many items\n                        (0 means use only time-based updates)\n\n    Returns:\n        A progress tracker compatible with the current environment.\n    \"\"\"\n    if disable:\n        return _NoOpProgress()\n\n    # Check if we're in a non-interactive batch environment (e.g., sbatch)\n    is_batch = force_batch_mode or is_non_interactive_batch()\n\n    # For batch jobs, use the simplified logging-based progress\n    if is_batch:\n        # In sbatch, use more frequent updates by default, including per-item updates\n        if \"SLURM_JOB_ID\" in os.environ and update_frequency == 0:\n            # By default in sbatch jobs, use both time-based and every 5 items\n            update_frequency = 1\n\n        return LoggingProgress(\n            log_interval=log_interval, update_frequency=update_frequency\n        )\n\n    # Use rich progress for interactive environments\n    return Progress(\n        TextColumn(\"[progress.description]{task.description}:\"),\n        TextColumn(\"[progress.percentage]{task.percentage:&gt;3.0f}%\"),\n        BarColumn(),\n        MofNCompleteColumn(),\n        TextColumn(\"\u2022\"),\n        TimeElapsedColumn(),\n        TextColumn(\"\u2022\"),\n        TimeRemainingColumn(),\n    )\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.is_non_interactive_batch","title":"<code>is_non_interactive_batch()</code>","text":"<p>Detect if running in a non-interactive batch job (like sbatch) where fancy progress bars won't display properly.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if in a non-interactive batch job, False otherwise</p> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def is_non_interactive_batch() -&gt; bool:\n    \"\"\"\n    Detect if running in a non-interactive batch job (like sbatch) where\n    fancy progress bars won't display properly.\n\n    Returns:\n        bool: True if in a non-interactive batch job, False otherwise\n    \"\"\"\n    # Check if running in Jupyter/IPython - these environments can display rich output\n    # even though sys.stdout.isatty() returns False\n    try:\n        get_ipython()\n        # We're in IPython/Jupyter, which supports rich output\n        return False\n    except NameError:\n        # Not in IPython/Jupyter, continue with other checks\n        pass\n\n    # Definite indicators of batch execution\n    batch_env_vars = [\"SLURM_JOB_ID\", \"PBS_JOBID\", \"LSB_JOBID\", \"SGE_TASK_ID\"]\n    for var in batch_env_vars:\n        if var in os.environ:\n            # Running in a batch system, now check if it's non-interactive\n            if not sys.stdout.isatty():\n                return True\n\n    # If specific batch execution indicators weren't found,\n    # use more general checks for non-interactive environment\n\n    # Check if TERM is set to \"dumb\" (common in batch environments)\n    if os.environ.get(\"TERM\", \"\") == \"dumb\":\n        return True\n\n    # Check for output redirection\n    if not sys.stdout.isatty():\n        # Special case: when using \"srun --pty\" on Slurm, we might be in\n        # a pseudo-terminal that can handle rich output\n        if \"SLURM_PTY_PORT\" in os.environ:\n            return False\n        return True\n\n    return False\n</code></pre>"},{"location":"api/console/progress/#easyroutine.console.progress.progress","title":"<code>progress(iterable, description='', desc=None, total=None, disable=False, force_batch_mode=False, log_interval=1, update_frequency=0)</code>","text":"<p>A tqdm-style progress bar that can be wrapped around an iterable.</p> <p>This function automatically adapts to the environment: - In interactive sessions (including interactive Slurm jobs), it shows a rich progress bar - In non-interactive batch jobs (like sbatch), it uses simple text-based progress tracking</p> <p>e.g. <code>for i in progress(range(10)):</code></p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <p>The iterable to wrap with a progress bar.</p> required <code>description</code> <code>str</code> <p>Description to display.</p> <code>''</code> <code>total</code> <code>int</code> <p>The total number of items. If None, it's inferred from len(iterable).</p> <code>None</code> <code>disable</code> <code>bool</code> <p>If True, the progress bar is disabled completely.</p> <code>False</code> <code>force_batch_mode</code> <code>bool</code> <p>If True, use text-based progress tracking even in interactive environments.</p> <code>False</code> <code>log_interval</code> <code>int</code> <p>In batch mode, how often (in seconds) to log progress updates.</p> <code>1</code> <code>update_frequency</code> <code>int</code> <p>In batch mode, update progress after processing this many items.                    Set to 0 to use only time-based updates.</p> <code>0</code> Source code in <code>easyroutine/console/progress.py</code> <pre><code>def progress(\n    iterable,\n    description: str = \"\",\n    desc: Optional[str] = None,\n    total: Optional[int] = None,\n    disable: bool = False,\n    force_batch_mode: bool = False,\n    log_interval: int = 1,\n    update_frequency: int = 0,\n):\n    \"\"\"\n    A tqdm-style progress bar that can be wrapped around an iterable.\n\n    This function automatically adapts to the environment:\n    - In interactive sessions (including interactive Slurm jobs), it shows a rich progress bar\n    - In non-interactive batch jobs (like sbatch), it uses simple text-based progress tracking\n\n    e.g. `for i in progress(range(10)):`\n\n    Args:\n        iterable: The iterable to wrap with a progress bar.\n        description (str): Description to display.\n        total (int, optional): The total number of items. If None, it's inferred from len(iterable).\n        disable (bool): If True, the progress bar is disabled completely.\n        force_batch_mode (bool): If True, use text-based progress tracking even in interactive environments.\n        log_interval (int): In batch mode, how often (in seconds) to log progress updates.\n        update_frequency (int): In batch mode, update progress after processing this many items.\n                               Set to 0 to use only time-based updates.\n    \"\"\"\n    if total is None:\n        try:\n            total = len(iterable)\n        except (TypeError, AttributeError):\n            pass\n    if desc is not None:\n        description = desc\n\n    with get_progress_bar(\n        disable=disable,\n        force_batch_mode=force_batch_mode,\n        log_interval=log_interval,\n        update_frequency=update_frequency,\n    ) as progress:\n        yield from progress.track(iterable, total=total, description=description)\n</code></pre>"},{"location":"api/inference/","title":"Inference","text":"<p>The <code>inference</code> module provides a standardized interface for interacting with various language models.</p>"},{"location":"api/inference/base_model_interface/","title":"BaseModelInterface","text":""},{"location":"api/inference/base_model_interface/#easyroutine.inference.base_model_interface.BaseInferenceModel","title":"<code>BaseInferenceModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for inference models. This class should be extended by specific model implementations.</p> Source code in <code>easyroutine/inference/base_model_interface.py</code> <pre><code>class BaseInferenceModel(ABC):\n    \"\"\"\n    Base class for inference models.\n    This class should be extended by specific model implementations.\n    \"\"\"\n\n    def __init__(self, config: BaseInferenceModelConfig):\n        self.config = config\n\n    @classmethod\n    def init_model(cls, model_name: str, n_gpus: int = 1, dtype: str = 'bfloat16') -&gt; 'BaseInferenceModel':\n        \"\"\"\n        Initialize the model with the given configuration.\n\n        Arguments:\n            model_name (str): Name of the model to initialize.\n            n_gpus (int): Number of GPUs to use.\n            dtype (str): Data type for the model.\n        Returns:\n\n            InferenceModel: An instance of the model.\n        \"\"\"\n        config = BaseInferenceModelConfig(model_name=model_name, n_gpus=n_gpus, dtype=dtype)\n        return cls(config)\n\n    def append_with_chat_template(self, message:str, role:Literal['user', 'assistant', 'system'] = 'user', chat_history:List[dict[str,str]] = []) -&gt; List[dict[str, str]]:\n        \"\"\"\n        Apply chat template to the message.\n        \"\"\"\n        # assert the chat_history\n        if len(chat_history) &gt; 0:\n            assert all('role' in msg and 'content' in msg for msg in chat_history), \"Chat history must contain 'role' and 'content' keys.\"\n        # Append the new message to the chat history\n        return chat_history + [{'role': role, 'content': message}]\n\n    @abstractmethod\n    def convert_chat_messages_to_custom_format(self, chat_messages: List[dict[str, str]]) -&gt; Union[List[dict[str, str]], str]:\n        \"\"\"\n        Convert chat messages to a custom format required by the model.\n\n        Arguments:\n            chat_messages (List[dict[str, str]]): List of chat messages to convert.\n\n        Returns:\n            Union[List[dict[str, str]], str]: Converted chat messages in the required format.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def chat(self, chat_messages: list, **kwargs) -&gt; list:\n        \"\"\"\n        Generate a response based on the provided chat messages.\n\n        Arguments:\n            chat_messages (list): List of chat messages to process.\n            **kwargs: Additional parameters for the model.\n\n        Returns:\n            str: The generated response from the model.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/inference/base_model_interface/#easyroutine.inference.base_model_interface.BaseInferenceModel.append_with_chat_template","title":"<code>append_with_chat_template(message, role='user', chat_history=[])</code>","text":"<p>Apply chat template to the message.</p> Source code in <code>easyroutine/inference/base_model_interface.py</code> <pre><code>def append_with_chat_template(self, message:str, role:Literal['user', 'assistant', 'system'] = 'user', chat_history:List[dict[str,str]] = []) -&gt; List[dict[str, str]]:\n    \"\"\"\n    Apply chat template to the message.\n    \"\"\"\n    # assert the chat_history\n    if len(chat_history) &gt; 0:\n        assert all('role' in msg and 'content' in msg for msg in chat_history), \"Chat history must contain 'role' and 'content' keys.\"\n    # Append the new message to the chat history\n    return chat_history + [{'role': role, 'content': message}]\n</code></pre>"},{"location":"api/inference/base_model_interface/#easyroutine.inference.base_model_interface.BaseInferenceModel.chat","title":"<code>chat(chat_messages, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response based on the provided chat messages.</p> <p>Parameters:</p> Name Type Description Default <code>chat_messages</code> <code>list</code> <p>List of chat messages to process.</p> required <code>**kwargs</code> <p>Additional parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>list</code> <p>The generated response from the model.</p> Source code in <code>easyroutine/inference/base_model_interface.py</code> <pre><code>@abstractmethod\ndef chat(self, chat_messages: list, **kwargs) -&gt; list:\n    \"\"\"\n    Generate a response based on the provided chat messages.\n\n    Arguments:\n        chat_messages (list): List of chat messages to process.\n        **kwargs: Additional parameters for the model.\n\n    Returns:\n        str: The generated response from the model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/inference/base_model_interface/#easyroutine.inference.base_model_interface.BaseInferenceModel.convert_chat_messages_to_custom_format","title":"<code>convert_chat_messages_to_custom_format(chat_messages)</code>  <code>abstractmethod</code>","text":"<p>Convert chat messages to a custom format required by the model.</p> <p>Parameters:</p> Name Type Description Default <code>chat_messages</code> <code>List[dict[str, str]]</code> <p>List of chat messages to convert.</p> required <p>Returns:</p> Type Description <code>Union[List[dict[str, str]], str]</code> <p>Union[List[dict[str, str]], str]: Converted chat messages in the required format.</p> Source code in <code>easyroutine/inference/base_model_interface.py</code> <pre><code>@abstractmethod\ndef convert_chat_messages_to_custom_format(self, chat_messages: List[dict[str, str]]) -&gt; Union[List[dict[str, str]], str]:\n    \"\"\"\n    Convert chat messages to a custom format required by the model.\n\n    Arguments:\n        chat_messages (List[dict[str, str]]): List of chat messages to convert.\n\n    Returns:\n        Union[List[dict[str, str]], str]: Converted chat messages in the required format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/inference/base_model_interface/#easyroutine.inference.base_model_interface.BaseInferenceModel.init_model","title":"<code>init_model(model_name, n_gpus=1, dtype='bfloat16')</code>  <code>classmethod</code>","text":"<p>Initialize the model with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to initialize.</p> required <code>n_gpus</code> <code>int</code> <p>Number of GPUs to use.</p> <code>1</code> <code>dtype</code> <code>str</code> <p>Data type for the model.</p> <code>'bfloat16'</code> <p>Returns:</p> <pre><code>InferenceModel: An instance of the model.\n</code></pre> Source code in <code>easyroutine/inference/base_model_interface.py</code> <pre><code>@classmethod\ndef init_model(cls, model_name: str, n_gpus: int = 1, dtype: str = 'bfloat16') -&gt; 'BaseInferenceModel':\n    \"\"\"\n    Initialize the model with the given configuration.\n\n    Arguments:\n        model_name (str): Name of the model to initialize.\n        n_gpus (int): Number of GPUs to use.\n        dtype (str): Data type for the model.\n    Returns:\n\n        InferenceModel: An instance of the model.\n    \"\"\"\n    config = BaseInferenceModelConfig(model_name=model_name, n_gpus=n_gpus, dtype=dtype)\n    return cls(config)\n</code></pre>"},{"location":"api/inference/base_model_interface/#easyroutine.inference.base_model_interface.BaseInferenceModelConfig","title":"<code>BaseInferenceModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the model interface.</p> Source code in <code>easyroutine/inference/base_model_interface.py</code> <pre><code>@dataclass\nclass BaseInferenceModelConfig:\n    \"\"\"\n    Configuration for the model interface.\n    \"\"\"\n    model_name: str\n    n_gpus: int = 1\n    dtype: str = 'bfloat16'\n    temperature: float = 0\n    top_p: float = 0.95\n    max_new_tokens: int = 5000\n</code></pre>"},{"location":"api/inference/litellm_model_interface/","title":"LiteLLMModelInterface","text":""},{"location":"api/inference/litellm_model_interface/#easyroutine.inference.litellm_model_interface.LiteLLMInferenceModel","title":"<code>LiteLLMInferenceModel</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> Source code in <code>easyroutine/inference/litellm_model_interface.py</code> <pre><code>class LiteLLMInferenceModel(BaseInferenceModel):\n\n    def __init__(self, config: LiteLLMInferenceModelConfig):\n        self.config = config\n        self.set_os_env()\n\n    def set_os_env(self):\n        import os\n        os.environ['OPENAI_API_KEY'] = self.config.openai_api_key\n        os.environ['ANTHROPIC_API_KEY'] = self.config.anthropic_api_key\n        os.environ['XAI_API_KEY'] = self.config.xai_api_key\n\n    def convert_chat_messages_to_custom_format(self, chat_messages: List[dict[str, str]]) -&gt; List[dict[str, str]]:\n        \"\"\"\n        For now, VLLM is compatible with the chat template format we use.\n        \"\"\"\n        return chat_messages\n\n    def chat(self, chat_messages: List[dict[str, str]], use_tqdm=False, **kwargs) -&gt; list:\n        \"\"\"\n        Generate a response based on the provided chat messages.\n\n        Arguments:\n            chat_messages (List[dict[str, str]]): List of chat messages to process.\n            **kwargs: Additional parameters for the model.\n\n        Returns:\n            str: The generated response from the model.\n        \"\"\"\n        chat_messages = self.convert_chat_messages_to_custom_format(chat_messages)\n\n\n        response = completion(\n            model = self.config.model_name,\n            messages = chat_messages,\n            temperature = self.config.temperature,\n            top_p = self.config.top_p,\n            max_tokens = self.config.max_new_tokens,\n        )\n        return response['choices']\n\n    def batch_chat(self, chat_messages: List[List[dict[str, str]]], use_tqdm=False, **kwargs) -&gt; List[list]:\n        \"\"\"\n        Generate responses for a batch of chat messages.\n\n        Arguments:\n            chat_messages (List[List[dict[str, str]]]): List of chat messages to process.\n            **kwargs: Additional parameters for the model.\n\n        Returns:\n            List[list]: List of generated responses from the model.\n        \"\"\"\n        chat_messages = [self.convert_chat_messages_to_custom_format(msg) for msg in chat_messages]\n\n        responses = batch_completion(\n            model = self.config.model_name,\n            messages = chat_messages,\n            temperature = self.config.temperature,\n            top_p = self.config.top_p,\n            max_tokens = self.config.max_new_tokens,\n        )\n        return responses\n</code></pre>"},{"location":"api/inference/litellm_model_interface/#easyroutine.inference.litellm_model_interface.LiteLLMInferenceModel.batch_chat","title":"<code>batch_chat(chat_messages, use_tqdm=False, **kwargs)</code>","text":"<p>Generate responses for a batch of chat messages.</p> <p>Parameters:</p> Name Type Description Default <code>chat_messages</code> <code>List[List[dict[str, str]]]</code> <p>List of chat messages to process.</p> required <code>**kwargs</code> <p>Additional parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[list]</code> <p>List[list]: List of generated responses from the model.</p> Source code in <code>easyroutine/inference/litellm_model_interface.py</code> <pre><code>def batch_chat(self, chat_messages: List[List[dict[str, str]]], use_tqdm=False, **kwargs) -&gt; List[list]:\n    \"\"\"\n    Generate responses for a batch of chat messages.\n\n    Arguments:\n        chat_messages (List[List[dict[str, str]]]): List of chat messages to process.\n        **kwargs: Additional parameters for the model.\n\n    Returns:\n        List[list]: List of generated responses from the model.\n    \"\"\"\n    chat_messages = [self.convert_chat_messages_to_custom_format(msg) for msg in chat_messages]\n\n    responses = batch_completion(\n        model = self.config.model_name,\n        messages = chat_messages,\n        temperature = self.config.temperature,\n        top_p = self.config.top_p,\n        max_tokens = self.config.max_new_tokens,\n    )\n    return responses\n</code></pre>"},{"location":"api/inference/litellm_model_interface/#easyroutine.inference.litellm_model_interface.LiteLLMInferenceModel.chat","title":"<code>chat(chat_messages, use_tqdm=False, **kwargs)</code>","text":"<p>Generate a response based on the provided chat messages.</p> <p>Parameters:</p> Name Type Description Default <code>chat_messages</code> <code>List[dict[str, str]]</code> <p>List of chat messages to process.</p> required <code>**kwargs</code> <p>Additional parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>list</code> <p>The generated response from the model.</p> Source code in <code>easyroutine/inference/litellm_model_interface.py</code> <pre><code>def chat(self, chat_messages: List[dict[str, str]], use_tqdm=False, **kwargs) -&gt; list:\n    \"\"\"\n    Generate a response based on the provided chat messages.\n\n    Arguments:\n        chat_messages (List[dict[str, str]]): List of chat messages to process.\n        **kwargs: Additional parameters for the model.\n\n    Returns:\n        str: The generated response from the model.\n    \"\"\"\n    chat_messages = self.convert_chat_messages_to_custom_format(chat_messages)\n\n\n    response = completion(\n        model = self.config.model_name,\n        messages = chat_messages,\n        temperature = self.config.temperature,\n        top_p = self.config.top_p,\n        max_tokens = self.config.max_new_tokens,\n    )\n    return response['choices']\n</code></pre>"},{"location":"api/inference/litellm_model_interface/#easyroutine.inference.litellm_model_interface.LiteLLMInferenceModel.convert_chat_messages_to_custom_format","title":"<code>convert_chat_messages_to_custom_format(chat_messages)</code>","text":"<p>For now, VLLM is compatible with the chat template format we use.</p> Source code in <code>easyroutine/inference/litellm_model_interface.py</code> <pre><code>def convert_chat_messages_to_custom_format(self, chat_messages: List[dict[str, str]]) -&gt; List[dict[str, str]]:\n    \"\"\"\n    For now, VLLM is compatible with the chat template format we use.\n    \"\"\"\n    return chat_messages\n</code></pre>"},{"location":"api/inference/litellm_model_interface/#easyroutine.inference.litellm_model_interface.LiteLLMInferenceModelConfig","title":"<code>LiteLLMInferenceModelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseInferenceModelConfig</code></p> <p>just a placeholder for now, as we don't have any specific config for VLLM.</p> Source code in <code>easyroutine/inference/litellm_model_interface.py</code> <pre><code>@dataclass\nclass LiteLLMInferenceModelConfig(BaseInferenceModelConfig):\n    \"\"\"just a placeholder for now, as we don't have any specific config for VLLM.\"\"\"\n    model_name: str\n\n    n_gpus: int = 0\n    dtype: str = 'bfloat16'\n    temperature: float = 0\n    top_p: float = 0.95\n    max_new_tokens: int = 5000\n\n    openai_api_key: str = ''\n    anthropic_api_key: str = ''\n    xai_api_key: str = ''\n</code></pre>"},{"location":"api/inference/vllm_model_interface/","title":"VLLMModelInterface","text":""},{"location":"api/inference/vllm_model_interface/#easyroutine.inference.vllm_model_interface.VLLMInferenceModel","title":"<code>VLLMInferenceModel</code>","text":"<p>               Bases: <code>BaseInferenceModel</code></p> <p>VLLM inference model interface. This class extends the BaseInferenceModel to provide specific functionality for VLLM.</p> Source code in <code>easyroutine/inference/vllm_model_interface.py</code> <pre><code>class VLLMInferenceModel(BaseInferenceModel):\n    \"\"\"\n    VLLM inference model interface.\n    This class extends the BaseInferenceModel to provide specific functionality for VLLM.\n    \"\"\"\n\n    def __init__(self, config: BaseInferenceModelConfig):\n        super().__init__(config)\n        self.model = LLM(model=config.model_name, tensor_parallel_size=config.n_gpus, dtype=config.dtype)\n\n\n    def convert_chat_messages_to_custom_format(self, chat_messages: List[dict[str, str]]) -&gt; List[dict[str, str]]:\n        \"\"\"\n        For now, VLLM is compatible with the chat template format we use.\n        \"\"\"\n        return chat_messages\n\n    def chat(self, chat_messages: List[dict[str, str]], use_tqdm=False, **kwargs) -&gt; list:\n        \"\"\"\n        Generate a response based on the provided chat messages.\n\n        Arguments:\n            chat_messages (List[dict[str, str]]): List of chat messages to process.\n            **kwargs: Additional parameters for the model.\n\n        Returns:\n            str: The generated response from the model.\n        \"\"\"\n        chat_messages = self.convert_chat_messages_to_custom_format(chat_messages)\n\n        sampling_params = SamplingParams(\n            temperature=self.config.temperature,\n            top_p=self.config.top_p,\n            max_tokens=self.config.max_new_tokens\n        )\n\n\n        # Generate response using VLLM\n        response = self.model.chat(chat_messages, sampling_params=sampling_params, use_tqdm=use_tqdm) # type: ignore\n\n        return response\n</code></pre>"},{"location":"api/inference/vllm_model_interface/#easyroutine.inference.vllm_model_interface.VLLMInferenceModel.chat","title":"<code>chat(chat_messages, use_tqdm=False, **kwargs)</code>","text":"<p>Generate a response based on the provided chat messages.</p> <p>Parameters:</p> Name Type Description Default <code>chat_messages</code> <code>List[dict[str, str]]</code> <p>List of chat messages to process.</p> required <code>**kwargs</code> <p>Additional parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>list</code> <p>The generated response from the model.</p> Source code in <code>easyroutine/inference/vllm_model_interface.py</code> <pre><code>def chat(self, chat_messages: List[dict[str, str]], use_tqdm=False, **kwargs) -&gt; list:\n    \"\"\"\n    Generate a response based on the provided chat messages.\n\n    Arguments:\n        chat_messages (List[dict[str, str]]): List of chat messages to process.\n        **kwargs: Additional parameters for the model.\n\n    Returns:\n        str: The generated response from the model.\n    \"\"\"\n    chat_messages = self.convert_chat_messages_to_custom_format(chat_messages)\n\n    sampling_params = SamplingParams(\n        temperature=self.config.temperature,\n        top_p=self.config.top_p,\n        max_tokens=self.config.max_new_tokens\n    )\n\n\n    # Generate response using VLLM\n    response = self.model.chat(chat_messages, sampling_params=sampling_params, use_tqdm=use_tqdm) # type: ignore\n\n    return response\n</code></pre>"},{"location":"api/inference/vllm_model_interface/#easyroutine.inference.vllm_model_interface.VLLMInferenceModel.convert_chat_messages_to_custom_format","title":"<code>convert_chat_messages_to_custom_format(chat_messages)</code>","text":"<p>For now, VLLM is compatible with the chat template format we use.</p> Source code in <code>easyroutine/inference/vllm_model_interface.py</code> <pre><code>def convert_chat_messages_to_custom_format(self, chat_messages: List[dict[str, str]]) -&gt; List[dict[str, str]]:\n    \"\"\"\n    For now, VLLM is compatible with the chat template format we use.\n    \"\"\"\n    return chat_messages\n</code></pre>"},{"location":"api/inference/vllm_model_interface/#easyroutine.inference.vllm_model_interface.VLLMInferenceModelConfig","title":"<code>VLLMInferenceModelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseInferenceModelConfig</code></p> <p>just a placeholder for now, as we don't have any specific config for VLLM.</p> Source code in <code>easyroutine/inference/vllm_model_interface.py</code> <pre><code>@dataclass\nclass VLLMInferenceModelConfig(BaseInferenceModelConfig):\n    \"\"\"just a placeholder for now, as we don't have any specific config for VLLM.\"\"\"\n</code></pre>"},{"location":"api/interpretability/","title":"API Introduction","text":"<p><code>easyroutine.interpretability</code> is the module that implement code for extract the hidden rappresentations of HuggingFace LLMs and intervening on the forward pass.</p>"},{"location":"api/interpretability/#simple-tutorial","title":"Simple Tutorial","text":"<pre><code># First we need to import the HookedModel and the config classes\nfrom easyroutine.interpretability import HookedModel, ExtractionConfig\n\n# Then we can create the hooked model\nhooked_model = HookedModel.from_pretrained(model_name=\"mistral-community/pixtral-12b\", device_map = \"auto\")\n\n# Now let's define a simple dataset\ndataset = [\n    \"This is a test\",\n    \"This is another test\"\n]\n\ntokenizer = hooked_model.get_tokenizer()\n\ndataset = tokenizer(dataset, padding=True, truncation=True, return_tensors=\"pt\") \n\ncache = hooked_model.extract_cache(\n    dataset,\n    target_token_positions = [\"last\"],\n    extraction_config = ExtractionConfig(\n        extract_resid_out = True\n    )\n)\n\n</code></pre>"},{"location":"api/interpretability/activation_cache/","title":"Activation cache","text":""},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache","title":"<code>ActivationCache</code>","text":"<p>A dictionary-like cache for storing and aggregating model activation values. Supports custom aggregation strategies registered for keys (by prefix match) and falls back to a default aggregation that can dynamically switch types if needed.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>class ActivationCache:\n    \"\"\"\n    A dictionary-like cache for storing and aggregating model activation values.\n    Supports custom aggregation strategies registered for keys (by prefix match)\n    and falls back to a default aggregation that can dynamically switch types if needed.\n    \"\"\"\n\n    def __init__(self):\n        self.cache = {}\n        self.valid_keys = (\n            re.compile(r\"resid_out_\\d+\"),\n            re.compile(r\"resid_in_\\d+\"),\n            re.compile(r\"resid_mid_\\d+\"),\n            re.compile(r\"attn_in_\\d+\"),\n            re.compile(r\"attn_out_\\d+\"),\n            re.compile(r\"avg_attn_pattern_L\\dH\\d+\"),\n            re.compile(r\"pattern_L\\dH+\\d+\"),\n            re.compile(r\"head_values_L\\dH+\\d+\"),\n            re.compile(r\"head_keys_L\\dH+\\d+\"),\n            re.compile(r\"head_queries_L\\dH+\\d+\"),\n            re.compile(r\"values_L\\d+\"),\n            re.compile(r\"keys_L\\d+\"),\n            re.compile(r\"queries_L\\d+\"),\n            re.compile(r\"input_ids\"),\n            re.compile(r\"mapping_index\"),\n            re.compile(r\"mlp_out_\\d+\"),\n            re.compile(r\"last_layernorm\"),\n            re.compile(r\"token_dict\")\n        )\n        self.aggregation_strategies = {}\n        # Register default aggregators for some keys\n        self.register_aggregation(\"mapping_index\", just_old)\n        self.register_aggregation(\"offset\", sublist)\n        self.register_aggregation(\"last_layernorm\", aggregate_last_layernorm)\n        self.register_aggregation(\"token_dict\", sublist)\n        self.deferred_cache = False\n\n    def __repr__(self) -&gt; str:\n        # Skip 'metadata' from the printed keys\n        items = [key for key in self.cache.keys() if key != \"metadata\"]\n        return f\"ActivationCache(`{', '.join(items)}`)\"\n\n    def __str__(self) -&gt; str:\n        # Skip 'metadata' from the printed items\n        items = [\n            f\"{key}: {value}\" for key, value in self.cache.items() if key != \"metadata\"\n        ]\n        return f\"ActivationCache({', '.join(items)})\"\n\n    def __setitem__(self, key: str, value):\n        if not any(pattern.match(key) for pattern in self.valid_keys):\n            logger.debug(\n                f\"Invalid key: {key}. Valid keys are: {self.valid_keys}. Could be a user-defined key.\"\n            )\n        self.cache[key] = value\n\n    def __getitem__(self, key: str):\n        return self.cache[key]\n\n    def __delitem__(self, key: str):\n        del self.cache[key]\n\n    def __add__(self, other) -&gt; \"ActivationCache\":\n        if not isinstance(other, (dict, ActivationCache)):\n            raise TypeError(\"Can only add ActivationCache or dict objects.\")\n        new_cache = ActivationCache()\n        new_cache.cache = {\n            **self.cache,\n            **(other.cache if isinstance(other, ActivationCache) else other),\n        }\n        return new_cache\n\n    def __contains__(self, key):\n        return key in self.cache\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state.pop(\"logger\", None)\n        state.pop(\"aggregation_strategies\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.aggregation_strategies = {}\n        self.register_aggregation(\"mapping_index\", just_old)\n        self.register_aggregation(\"input_ids\", just_me)\n        self.register_aggregation(\"offset\", sublist)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"\n        Returns True if the cache is empty, False otherwise.\n        \"\"\"\n        return len(self.cache) == 0\n\n    def get(self, key: str, default=None):\n        return self.cache.get(key, default)\n\n    def items(self):\n        return self.cache.items()\n\n    def keys(self):\n        return self.cache.keys()\n\n    def values(self):\n        return self.cache.values()\n\n    def update(self, other):\n        if isinstance(other, dict):\n            self.cache.update(other)\n        elif isinstance(other, type(self)):\n            self.cache.update(other.cache)\n        else:\n            raise TypeError(\"Can only update with dict or ActivationCache objects.\")\n\n    def to(self, device: Union[str, torch.device]):\n        for key, value in self.cache.items():\n            if hasattr(value, \"to\"):\n                self.cache[key] = value.to(device)\n            elif isinstance(value, dict):\n                for k, v in value.items():\n                    if hasattr(v, \"to\"):\n                        value[k] = v.to(device)\n                self.cache[key] = value\n\n    def cpu(self):\n        self.to(\"cpu\")\n\n    def cuda(self):\n        self.to(\"cuda\")\n\n    def register_aggregation(self, key_pattern, function):\n        \"\"\"\n        Registers a custom aggregation function for keys that start with key_pattern.\n        \"\"\"\n        logger.debug(\n            f\"Registering aggregation strategy for keys starting with '{key_pattern}'\"\n        )\n        self.aggregation_strategies[key_pattern] = function\n\n    def remove_aggregation(self, key_pattern):\n        if key_pattern in self.aggregation_strategies:\n            del self.aggregation_strategies[key_pattern]\n\n    def _get_aggregation_strategy(self, key: str):\n        \"\"\"\n        Returns the aggregation function for the given key.\n        If no custom function is registered, the default aggregation is used.\n        \"\"\"\n        for pattern, strategy in self.aggregation_strategies.items():\n            if key.startswith(pattern):\n                return strategy\n        return self.default_aggregation\n\n    def default_aggregation(self, old, new):\n        \"\"\"\n        Default aggregation strategy.\n        - If old is None, simply return new.\n        - For torch.Tensor values, first try torch.cat, then torch.stack, and finally fallback to list aggregation.\n        - For lists and tuples, aggregates by appending (or converting tuples to lists).\n        - For ValueWithInfo, aggregates the inner values.\n        - Otherwise, tries the '+' operator and falls back to a list if necessary.\n        \"\"\"\n        if old is None:\n            return new\n\n        # Aggregation for torch.Tensor\n        if isinstance(old, torch.Tensor) and isinstance(new, torch.Tensor):\n            try:\n                return torch.cat([old, new], dim=0)\n            except Exception as e:\n                logger.warning(\n                    f\"torch.cat failed for tensor shapes {old.shape} and {new.shape}: {e}; trying torch.stack.\"\n                )\n                try:\n                    return torch.stack([old, new], dim=0)\n                except Exception as e:\n                    logger.warning(\n                        f\"torch.stack also failed: {e}; switching to list aggregation.\"\n                    )\n                    return [old, new]\n\n        # Aggregation for lists\n        if isinstance(old, list):\n            return old + (new if isinstance(new, list) else [new])\n\n        # Aggregation for tuples: convert to list\n        if isinstance(old, tuple):\n            if isinstance(new, tuple):\n                return [old, new]\n            elif isinstance(new, list):\n                return [old] + new\n            else:\n                return [old, new]\n\n        # Aggregation for ValueWithInfo: aggregate the underlying values.\n        if isinstance(old, ValueWithInfo) and isinstance(new, ValueWithInfo):\n            aggregated_value = self.default_aggregation(old.value(), new.value())\n            return ValueWithInfo(aggregated_value, old.info())\n\n        # Fallback: try using the + operator.\n        try:\n            return old + new\n        except Exception as e:\n            logger.debug(\n                f\"Aggregation failed for values {old} and {new}: {e}; using list fallback.\"\n            )\n            return [old, new]\n\n    def cat(self, external_cache):\n        \"\"\"\n        Merges the current cache with an external cache using the registered\n        aggregation strategies (or the default if none is registered).\n\n        If the cache is empty, each key is initialized using the aggregator with old=None.\n        Otherwise, keys must match exactly between the two caches.\n        \"\"\"\n        if not isinstance(external_cache, type(self)):\n            raise TypeError(\"external_cache must be an instance of ActivationCache\")\n\n        # If in deferred mode, store the external cache for later aggregation.\n        if isinstance(self.deferred_cache, list):\n            self.deferred_cache.append(external_cache)\n            return\n\n        # Case 1: If self.cache is empty, initialize each key using the aggregator.\n        if not self.cache:\n            for key, new_value in external_cache.cache.items():\n                aggregator = self._get_aggregation_strategy(key)\n                self.cache[key] = aggregator(None, new_value)\n            return\n\n        # Case 2: Ensure both caches have the same keys.\n        self_keys = set(self.cache.keys())\n        external_keys = set(external_cache.cache.keys())\n        if self_keys != external_keys:\n            raise ValueError(\n                f\"Key mismatch: self has {self_keys - external_keys}, external has {external_keys - self_keys}\"\n            )\n\n        # Case 3: Aggregate matching keys.\n        for key in self.cache:\n            aggregator = self._get_aggregation_strategy(key)\n            try:\n                self.cache[key] = aggregator(self.cache[key], external_cache.cache[key])\n            except Exception as e:\n                logger.error(f\"Error aggregating key '{key}': {e}\")\n                self.cache[key] = [self.cache[key], external_cache.cache[key]]\n\n    @contextlib.contextmanager\n    def deferred_mode(self):\n        \"\"\"\n        Context manager for deferred aggregation. Instead of merging\n        immediately when calling `cat`, external caches are stored and then\n        aggregated once the context is exited.\n        \"\"\"\n        original_deferred = self.deferred_cache\n        self.deferred_cache = []\n        try:\n            yield self\n            for ext_cache in self.deferred_cache:\n                self.cat(ext_cache)\n        finally:\n            self.deferred_cache = original_deferred\n\n    def add_with_info(self, key: str, value, info: str):\n        \"\"\"\n        Wraps a value (e.g. a tensor) with additional info and stores it in the cache.\n        \"\"\"\n        wrapped = ValueWithInfo(value, info)\n        self[key] = wrapped\n\n    def add_metadata(\n        self, target_token_positions, model_name: str, extraction_config, interventions\n    ):\n        \"\"\"\n        Adds metadata to the cache for future reference.\n        \"\"\"\n        self.cache[\"metadata\"] = {\n            \"target_token_positions\": target_token_positions,\n            \"model_name\": model_name,\n            \"extraction_config\": extraction_config,\n            \"interventions\": interventions,\n        }\n\n    def map_to_dict(self, key: str) -&gt; dict:\n        \"\"\"\n        Maps the cache values to a dictionary based on mapping_index.\n        \"\"\"\n        if self.cache[key] is None:\n            logger.error(f\"Key {key} not found in cache.\")\n\n        elif not isinstance(self.cache[key], torch.Tensor):\n            logger.error(f\"Value for key {key} is not a tensor.\")\n\n        mapping_index = self.cache[\"mapping_index\"]\n\n        return_dict = {}\n        for key_map, value_map in mapping_index.items():\n            return_dict[key_map] = self.cache[key][:, value_map].squeeze()\n\n        return return_dict\n\n    def memory_size(self, key: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Returns the memory size of the cache in bytes.\n        \"\"\"\n        if key is not None:\n            if key not in self.cache:\n                return \"Not present: 0 B\"\n            value = self.cache[key]\n            if isinstance(value, torch.Tensor):\n                total_size = value.element_size() * value.nelement()\n            elif isinstance(value, dict):\n                total_size = 0\n                for v in value.values():\n                    if isinstance(v, torch.Tensor):\n                        total_size += v.element_size() * v.nelement()\n            elif isinstance(value, list):\n                total_size = 0\n                for v in value:\n                    if isinstance(v, torch.Tensor):\n                        total_size += v.element_size() * v.nelement()\n                    else:\n                        logger.warning(f\"Unknown type in list for key {key}: {type(v)}\")\n            else:\n                logger.warning(f\"Unknown type for key {key}: {type(value)}\")\n                return \"0 B\"\n        else:\n            total_size = 0\n            for key, value in self.cache.items():\n                if isinstance(value, torch.Tensor):\n                    total_size += value.element_size() * value.nelement()\n                elif isinstance(value, dict):\n                    for v in value.values():\n                        if isinstance(v, torch.Tensor):\n                            total_size += v.element_size() * v.nelement()\n                elif isinstance(value, list):\n                    for v in value:\n                        if isinstance(v, torch.Tensor):\n                            total_size += v.element_size() * v.nelement()\n                        else:\n                            logger.warning(\n                                f\"Unknown type in list for key {key}: {type(v)}\"\n                            )\n                else:\n                    logger.warning(f\"Unknown type for key {key}: {type(value)}\")\n\n        # depending of the size, return the size in KB, MB or GB\n        # prioritize the size in GB, so 0.1 GB is 100 MB\n        if total_size &gt; 1e9:\n            total_size /= 1e9\n            return f\"{total_size:.2f} GB\"\n        elif total_size &gt; 1e6:\n            total_size /= 1e6\n            return f\"{total_size:.2f} MB\"\n        elif total_size &gt; 1e3:\n            total_size /= 1e3\n            return f\"{total_size:.2f} KB\"\n        else:\n            total_size /= 1e3\n            return f\"{total_size:.2f} B\"\n\n    def memory_tree(self, print_tree: bool = False, grouped_tree: bool = False) -&gt; dict:\n        \"\"\"\n        Print a tree of the memory size of the cache.\n\n        Args:\n            print_tree (bool): If True, print the tree to the console.\n            grouped_tree (bool): If True, group similar keys (e.g., resid_out_0, resid_out_1 -&gt; resid_out).\n\n        Returns:\n            dict: A dictionary of the memory sizes.\n        \"\"\"\n        tree = {}\n        for key, value in self.cache.items():\n            if isinstance(value, torch.Tensor):\n                tree[key] = self.memory_size(key)\n            elif isinstance(value, dict):\n                tree[key] = {}\n                for k, v in value.items():\n                    if isinstance(v, torch.Tensor):\n                        tree[key][k] = self.memory_size(k)\n                    else:\n                        tree[key][k] = \"Unknown type\"\n            elif isinstance(value, list):\n                tree[key] = []\n                for v in value:\n                    if isinstance(v, torch.Tensor):\n                        tree[key].append(self.memory_size(key))\n                    else:\n                        tree[key].append(\"Unknown type\")\n            else:\n                tree[key] = \"Unknown type\"\n\n        if grouped_tree:\n            grouped = {}\n            # Define regex patterns for grouping\n            patterns = {\n                r\"resid_out_\\d+\": \"resid_out\",\n                r\"resid_in_\\d+\": \"resid_in\",\n                r\"resid_mid_\\d+\": \"resid_mid\",\n                r\"attn_in_\\d+\": \"attn_in\",\n                r\"attn_out_\\d+\": \"attn_out\",\n                r\"avg_attn_pattern_L\\d+H\\d+\": lambda m: f\"avg_attn_pattern_L{m.group(1)}\",\n                r\"pattern_L(\\d+)H\\d+\": lambda m: f\"pattern_L{m.group(1)}\",\n                r\"head_out_\\d+\": \"head_out\",\n                r\"mlp_out_\\d+\": \"mlp_out\",\n                r\"head_values_L(\\d+)H\\d+\": lambda m: f\"head_values_L{m.group(1)}\",\n                r\"head_keys_L(\\d+)H\\d+\": lambda m: f\"head_keys_L{m.group(1)}\",\n                r\"head_queries_L(\\d+)H\\d+\": lambda m: f\"head_queries_L{m.group(1)}\",\n                r\"values_L(\\d+)\": lambda m: f\"values_L{m.group(1)}\",\n                r\"keys_L(\\d+)\": lambda m: f\"keys_L{m.group(1)}\",\n                r\"queries_L(\\d+)\": lambda m: f\"queries_L{m.group(1)}\"\n\n            }\n\n            for key, size in tree.items():\n                # Skip metadata and other special keys\n                if key == \"metadata\" or isinstance(size, (dict, list)):\n                    grouped[key] = size\n                    continue\n\n                # Try to match the key with patterns\n                matched = False\n                for pattern, replacement in patterns.items():\n                    match = re.match(pattern, key)\n                    if match:\n                        group_key = replacement\n                        if callable(replacement):\n                            group_key = replacement(match)\n\n                        # Convert size string to float value for aggregation\n                        size_value, unit = size.split()\n                        size_value = float(size_value)\n\n                        # Initialize group if not exists\n                        if group_key not in grouped:\n                            grouped[group_key] = {\"size\": 0.0, \"unit\": unit, \"count\": 0}\n\n                        # Normalize units\n                        if unit == \"KB\" and grouped[group_key][\"unit\"] == \"MB\":\n                            size_value /= 1000\n                        elif unit == \"B\" and grouped[group_key][\"unit\"] == \"KB\":\n                            size_value /= 1000\n                        elif unit == \"B\" and grouped[group_key][\"unit\"] == \"MB\":\n                            size_value /= 1000000\n                        elif unit == \"MB\" and grouped[group_key][\"unit\"] == \"KB\":\n                            grouped[group_key][\"size\"] /= 1000\n                            grouped[group_key][\"unit\"] = \"MB\"\n                        elif unit == \"MB\" and grouped[group_key][\"unit\"] == \"B\":\n                            grouped[group_key][\"size\"] /= 1000000\n                            grouped[group_key][\"unit\"] = \"MB\"\n                        elif unit == \"GB\" and grouped[group_key][\"unit\"] != \"GB\":\n                            # Convert all to GB\n                            if grouped[group_key][\"unit\"] == \"MB\":\n                                grouped[group_key][\"size\"] /= 1000\n                            elif grouped[group_key][\"unit\"] == \"KB\":\n                                grouped[group_key][\"size\"] /= 1000000\n                            elif grouped[group_key][\"unit\"] == \"B\":\n                                grouped[group_key][\"size\"] /= 1000000000\n                            grouped[group_key][\"unit\"] = \"GB\"\n                        elif grouped[group_key][\"unit\"] == \"GB\" and unit != \"GB\":\n                            # Convert size_value to GB\n                            if unit == \"MB\":\n                                size_value /= 1000\n                            elif unit == \"KB\":\n                                size_value /= 1000000\n                            elif unit == \"B\":\n                                size_value /= 1000000000\n\n                        # Aggregate sizes\n                        grouped[group_key][\"size\"] += size_value\n                        grouped[group_key][\"count\"] += 1\n                        matched = True\n                        break\n\n                # If no pattern matched, keep the original key\n                if not matched:\n                    grouped[key] = size\n\n            # Format the grouped sizes back to strings\n            for key, info in grouped.items():\n                if isinstance(info, dict) and \"size\" in info:\n                    grouped[key] = (\n                        f\"{info['size']:.2f} {info['unit']} ({info['count']} items)\"\n                    )\n\n            tree = grouped\n\n        if print_tree:\n            print(\"-\" * 4 + \" Activation Cache Memory Tree\" + \" -\" * 4)\n            print(\"Total size: \", self.memory_size())\n            for key, value in tree.items():\n                print(f\"   - {key}: {value}\")\n\n        return tree\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.add_metadata","title":"<code>add_metadata(target_token_positions, model_name, extraction_config, interventions)</code>","text":"<p>Adds metadata to the cache for future reference.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def add_metadata(\n    self, target_token_positions, model_name: str, extraction_config, interventions\n):\n    \"\"\"\n    Adds metadata to the cache for future reference.\n    \"\"\"\n    self.cache[\"metadata\"] = {\n        \"target_token_positions\": target_token_positions,\n        \"model_name\": model_name,\n        \"extraction_config\": extraction_config,\n        \"interventions\": interventions,\n    }\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.add_with_info","title":"<code>add_with_info(key, value, info)</code>","text":"<p>Wraps a value (e.g. a tensor) with additional info and stores it in the cache.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def add_with_info(self, key: str, value, info: str):\n    \"\"\"\n    Wraps a value (e.g. a tensor) with additional info and stores it in the cache.\n    \"\"\"\n    wrapped = ValueWithInfo(value, info)\n    self[key] = wrapped\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.cat","title":"<code>cat(external_cache)</code>","text":"<p>Merges the current cache with an external cache using the registered aggregation strategies (or the default if none is registered).</p> <p>If the cache is empty, each key is initialized using the aggregator with old=None. Otherwise, keys must match exactly between the two caches.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def cat(self, external_cache):\n    \"\"\"\n    Merges the current cache with an external cache using the registered\n    aggregation strategies (or the default if none is registered).\n\n    If the cache is empty, each key is initialized using the aggregator with old=None.\n    Otherwise, keys must match exactly between the two caches.\n    \"\"\"\n    if not isinstance(external_cache, type(self)):\n        raise TypeError(\"external_cache must be an instance of ActivationCache\")\n\n    # If in deferred mode, store the external cache for later aggregation.\n    if isinstance(self.deferred_cache, list):\n        self.deferred_cache.append(external_cache)\n        return\n\n    # Case 1: If self.cache is empty, initialize each key using the aggregator.\n    if not self.cache:\n        for key, new_value in external_cache.cache.items():\n            aggregator = self._get_aggregation_strategy(key)\n            self.cache[key] = aggregator(None, new_value)\n        return\n\n    # Case 2: Ensure both caches have the same keys.\n    self_keys = set(self.cache.keys())\n    external_keys = set(external_cache.cache.keys())\n    if self_keys != external_keys:\n        raise ValueError(\n            f\"Key mismatch: self has {self_keys - external_keys}, external has {external_keys - self_keys}\"\n        )\n\n    # Case 3: Aggregate matching keys.\n    for key in self.cache:\n        aggregator = self._get_aggregation_strategy(key)\n        try:\n            self.cache[key] = aggregator(self.cache[key], external_cache.cache[key])\n        except Exception as e:\n            logger.error(f\"Error aggregating key '{key}': {e}\")\n            self.cache[key] = [self.cache[key], external_cache.cache[key]]\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.default_aggregation","title":"<code>default_aggregation(old, new)</code>","text":"<p>Default aggregation strategy. - If old is None, simply return new. - For torch.Tensor values, first try torch.cat, then torch.stack, and finally fallback to list aggregation. - For lists and tuples, aggregates by appending (or converting tuples to lists). - For ValueWithInfo, aggregates the inner values. - Otherwise, tries the '+' operator and falls back to a list if necessary.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def default_aggregation(self, old, new):\n    \"\"\"\n    Default aggregation strategy.\n    - If old is None, simply return new.\n    - For torch.Tensor values, first try torch.cat, then torch.stack, and finally fallback to list aggregation.\n    - For lists and tuples, aggregates by appending (or converting tuples to lists).\n    - For ValueWithInfo, aggregates the inner values.\n    - Otherwise, tries the '+' operator and falls back to a list if necessary.\n    \"\"\"\n    if old is None:\n        return new\n\n    # Aggregation for torch.Tensor\n    if isinstance(old, torch.Tensor) and isinstance(new, torch.Tensor):\n        try:\n            return torch.cat([old, new], dim=0)\n        except Exception as e:\n            logger.warning(\n                f\"torch.cat failed for tensor shapes {old.shape} and {new.shape}: {e}; trying torch.stack.\"\n            )\n            try:\n                return torch.stack([old, new], dim=0)\n            except Exception as e:\n                logger.warning(\n                    f\"torch.stack also failed: {e}; switching to list aggregation.\"\n                )\n                return [old, new]\n\n    # Aggregation for lists\n    if isinstance(old, list):\n        return old + (new if isinstance(new, list) else [new])\n\n    # Aggregation for tuples: convert to list\n    if isinstance(old, tuple):\n        if isinstance(new, tuple):\n            return [old, new]\n        elif isinstance(new, list):\n            return [old] + new\n        else:\n            return [old, new]\n\n    # Aggregation for ValueWithInfo: aggregate the underlying values.\n    if isinstance(old, ValueWithInfo) and isinstance(new, ValueWithInfo):\n        aggregated_value = self.default_aggregation(old.value(), new.value())\n        return ValueWithInfo(aggregated_value, old.info())\n\n    # Fallback: try using the + operator.\n    try:\n        return old + new\n    except Exception as e:\n        logger.debug(\n            f\"Aggregation failed for values {old} and {new}: {e}; using list fallback.\"\n        )\n        return [old, new]\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.deferred_mode","title":"<code>deferred_mode()</code>","text":"<p>Context manager for deferred aggregation. Instead of merging immediately when calling <code>cat</code>, external caches are stored and then aggregated once the context is exited.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>@contextlib.contextmanager\ndef deferred_mode(self):\n    \"\"\"\n    Context manager for deferred aggregation. Instead of merging\n    immediately when calling `cat`, external caches are stored and then\n    aggregated once the context is exited.\n    \"\"\"\n    original_deferred = self.deferred_cache\n    self.deferred_cache = []\n    try:\n        yield self\n        for ext_cache in self.deferred_cache:\n            self.cat(ext_cache)\n    finally:\n        self.deferred_cache = original_deferred\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.is_empty","title":"<code>is_empty()</code>","text":"<p>Returns True if the cache is empty, False otherwise.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"\n    Returns True if the cache is empty, False otherwise.\n    \"\"\"\n    return len(self.cache) == 0\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.map_to_dict","title":"<code>map_to_dict(key)</code>","text":"<p>Maps the cache values to a dictionary based on mapping_index.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def map_to_dict(self, key: str) -&gt; dict:\n    \"\"\"\n    Maps the cache values to a dictionary based on mapping_index.\n    \"\"\"\n    if self.cache[key] is None:\n        logger.error(f\"Key {key} not found in cache.\")\n\n    elif not isinstance(self.cache[key], torch.Tensor):\n        logger.error(f\"Value for key {key} is not a tensor.\")\n\n    mapping_index = self.cache[\"mapping_index\"]\n\n    return_dict = {}\n    for key_map, value_map in mapping_index.items():\n        return_dict[key_map] = self.cache[key][:, value_map].squeeze()\n\n    return return_dict\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.memory_size","title":"<code>memory_size(key=None)</code>","text":"<p>Returns the memory size of the cache in bytes.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def memory_size(self, key: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Returns the memory size of the cache in bytes.\n    \"\"\"\n    if key is not None:\n        if key not in self.cache:\n            return \"Not present: 0 B\"\n        value = self.cache[key]\n        if isinstance(value, torch.Tensor):\n            total_size = value.element_size() * value.nelement()\n        elif isinstance(value, dict):\n            total_size = 0\n            for v in value.values():\n                if isinstance(v, torch.Tensor):\n                    total_size += v.element_size() * v.nelement()\n        elif isinstance(value, list):\n            total_size = 0\n            for v in value:\n                if isinstance(v, torch.Tensor):\n                    total_size += v.element_size() * v.nelement()\n                else:\n                    logger.warning(f\"Unknown type in list for key {key}: {type(v)}\")\n        else:\n            logger.warning(f\"Unknown type for key {key}: {type(value)}\")\n            return \"0 B\"\n    else:\n        total_size = 0\n        for key, value in self.cache.items():\n            if isinstance(value, torch.Tensor):\n                total_size += value.element_size() * value.nelement()\n            elif isinstance(value, dict):\n                for v in value.values():\n                    if isinstance(v, torch.Tensor):\n                        total_size += v.element_size() * v.nelement()\n            elif isinstance(value, list):\n                for v in value:\n                    if isinstance(v, torch.Tensor):\n                        total_size += v.element_size() * v.nelement()\n                    else:\n                        logger.warning(\n                            f\"Unknown type in list for key {key}: {type(v)}\"\n                        )\n            else:\n                logger.warning(f\"Unknown type for key {key}: {type(value)}\")\n\n    # depending of the size, return the size in KB, MB or GB\n    # prioritize the size in GB, so 0.1 GB is 100 MB\n    if total_size &gt; 1e9:\n        total_size /= 1e9\n        return f\"{total_size:.2f} GB\"\n    elif total_size &gt; 1e6:\n        total_size /= 1e6\n        return f\"{total_size:.2f} MB\"\n    elif total_size &gt; 1e3:\n        total_size /= 1e3\n        return f\"{total_size:.2f} KB\"\n    else:\n        total_size /= 1e3\n        return f\"{total_size:.2f} B\"\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.memory_tree","title":"<code>memory_tree(print_tree=False, grouped_tree=False)</code>","text":"<p>Print a tree of the memory size of the cache.</p> <p>Parameters:</p> Name Type Description Default <code>print_tree</code> <code>bool</code> <p>If True, print the tree to the console.</p> <code>False</code> <code>grouped_tree</code> <code>bool</code> <p>If True, group similar keys (e.g., resid_out_0, resid_out_1 -&gt; resid_out).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of the memory sizes.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def memory_tree(self, print_tree: bool = False, grouped_tree: bool = False) -&gt; dict:\n    \"\"\"\n    Print a tree of the memory size of the cache.\n\n    Args:\n        print_tree (bool): If True, print the tree to the console.\n        grouped_tree (bool): If True, group similar keys (e.g., resid_out_0, resid_out_1 -&gt; resid_out).\n\n    Returns:\n        dict: A dictionary of the memory sizes.\n    \"\"\"\n    tree = {}\n    for key, value in self.cache.items():\n        if isinstance(value, torch.Tensor):\n            tree[key] = self.memory_size(key)\n        elif isinstance(value, dict):\n            tree[key] = {}\n            for k, v in value.items():\n                if isinstance(v, torch.Tensor):\n                    tree[key][k] = self.memory_size(k)\n                else:\n                    tree[key][k] = \"Unknown type\"\n        elif isinstance(value, list):\n            tree[key] = []\n            for v in value:\n                if isinstance(v, torch.Tensor):\n                    tree[key].append(self.memory_size(key))\n                else:\n                    tree[key].append(\"Unknown type\")\n        else:\n            tree[key] = \"Unknown type\"\n\n    if grouped_tree:\n        grouped = {}\n        # Define regex patterns for grouping\n        patterns = {\n            r\"resid_out_\\d+\": \"resid_out\",\n            r\"resid_in_\\d+\": \"resid_in\",\n            r\"resid_mid_\\d+\": \"resid_mid\",\n            r\"attn_in_\\d+\": \"attn_in\",\n            r\"attn_out_\\d+\": \"attn_out\",\n            r\"avg_attn_pattern_L\\d+H\\d+\": lambda m: f\"avg_attn_pattern_L{m.group(1)}\",\n            r\"pattern_L(\\d+)H\\d+\": lambda m: f\"pattern_L{m.group(1)}\",\n            r\"head_out_\\d+\": \"head_out\",\n            r\"mlp_out_\\d+\": \"mlp_out\",\n            r\"head_values_L(\\d+)H\\d+\": lambda m: f\"head_values_L{m.group(1)}\",\n            r\"head_keys_L(\\d+)H\\d+\": lambda m: f\"head_keys_L{m.group(1)}\",\n            r\"head_queries_L(\\d+)H\\d+\": lambda m: f\"head_queries_L{m.group(1)}\",\n            r\"values_L(\\d+)\": lambda m: f\"values_L{m.group(1)}\",\n            r\"keys_L(\\d+)\": lambda m: f\"keys_L{m.group(1)}\",\n            r\"queries_L(\\d+)\": lambda m: f\"queries_L{m.group(1)}\"\n\n        }\n\n        for key, size in tree.items():\n            # Skip metadata and other special keys\n            if key == \"metadata\" or isinstance(size, (dict, list)):\n                grouped[key] = size\n                continue\n\n            # Try to match the key with patterns\n            matched = False\n            for pattern, replacement in patterns.items():\n                match = re.match(pattern, key)\n                if match:\n                    group_key = replacement\n                    if callable(replacement):\n                        group_key = replacement(match)\n\n                    # Convert size string to float value for aggregation\n                    size_value, unit = size.split()\n                    size_value = float(size_value)\n\n                    # Initialize group if not exists\n                    if group_key not in grouped:\n                        grouped[group_key] = {\"size\": 0.0, \"unit\": unit, \"count\": 0}\n\n                    # Normalize units\n                    if unit == \"KB\" and grouped[group_key][\"unit\"] == \"MB\":\n                        size_value /= 1000\n                    elif unit == \"B\" and grouped[group_key][\"unit\"] == \"KB\":\n                        size_value /= 1000\n                    elif unit == \"B\" and grouped[group_key][\"unit\"] == \"MB\":\n                        size_value /= 1000000\n                    elif unit == \"MB\" and grouped[group_key][\"unit\"] == \"KB\":\n                        grouped[group_key][\"size\"] /= 1000\n                        grouped[group_key][\"unit\"] = \"MB\"\n                    elif unit == \"MB\" and grouped[group_key][\"unit\"] == \"B\":\n                        grouped[group_key][\"size\"] /= 1000000\n                        grouped[group_key][\"unit\"] = \"MB\"\n                    elif unit == \"GB\" and grouped[group_key][\"unit\"] != \"GB\":\n                        # Convert all to GB\n                        if grouped[group_key][\"unit\"] == \"MB\":\n                            grouped[group_key][\"size\"] /= 1000\n                        elif grouped[group_key][\"unit\"] == \"KB\":\n                            grouped[group_key][\"size\"] /= 1000000\n                        elif grouped[group_key][\"unit\"] == \"B\":\n                            grouped[group_key][\"size\"] /= 1000000000\n                        grouped[group_key][\"unit\"] = \"GB\"\n                    elif grouped[group_key][\"unit\"] == \"GB\" and unit != \"GB\":\n                        # Convert size_value to GB\n                        if unit == \"MB\":\n                            size_value /= 1000\n                        elif unit == \"KB\":\n                            size_value /= 1000000\n                        elif unit == \"B\":\n                            size_value /= 1000000000\n\n                    # Aggregate sizes\n                    grouped[group_key][\"size\"] += size_value\n                    grouped[group_key][\"count\"] += 1\n                    matched = True\n                    break\n\n            # If no pattern matched, keep the original key\n            if not matched:\n                grouped[key] = size\n\n        # Format the grouped sizes back to strings\n        for key, info in grouped.items():\n            if isinstance(info, dict) and \"size\" in info:\n                grouped[key] = (\n                    f\"{info['size']:.2f} {info['unit']} ({info['count']} items)\"\n                )\n\n        tree = grouped\n\n    if print_tree:\n        print(\"-\" * 4 + \" Activation Cache Memory Tree\" + \" -\" * 4)\n        print(\"Total size: \", self.memory_size())\n        for key, value in tree.items():\n            print(f\"   - {key}: {value}\")\n\n    return tree\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.register_aggregation","title":"<code>register_aggregation(key_pattern, function)</code>","text":"<p>Registers a custom aggregation function for keys that start with key_pattern.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def register_aggregation(self, key_pattern, function):\n    \"\"\"\n    Registers a custom aggregation function for keys that start with key_pattern.\n    \"\"\"\n    logger.debug(\n        f\"Registering aggregation strategy for keys starting with '{key_pattern}'\"\n    )\n    self.aggregation_strategies[key_pattern] = function\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ValueWithInfo","title":"<code>ValueWithInfo</code>","text":"<p>A thin wrapper around a value that also stores extra info.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>class ValueWithInfo:\n    \"\"\"\n    A thin wrapper around a value that also stores extra info.\n    \"\"\"\n\n    __slots__ = (\"_value\", \"_info\")\n\n    def __init__(self, value, info):\n        self._value = value\n        self._info = info\n\n    def info(self):\n        return self._info\n\n    def value(self):\n        return self._value\n\n    def __getattr__(self, name):\n        return getattr(self._value, name)\n\n    def __repr__(self):\n        return f\"ValueWithInfo(value={self._value!r}, info={self._info!r})\"\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.aggregate_last_layernorm","title":"<code>aggregate_last_layernorm(old, new)</code>","text":"<p>Aggregates <code>last_layernorm</code> values by concatenating along the first dimension.</p> <p>Parameters:</p> Name Type Description Default <code>old</code> <code>dict</code> <p>Previous stored dictionary, where each value is a tensor of shape [m, N].</p> required <code>new</code> <code>dict</code> <p>New incoming dictionary, where each value is a tensor of shape [1, N].</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Aggregated dictionary where each value is a tensor of shape [m+1, N].</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def aggregate_last_layernorm(old, new):\n    \"\"\"\n    Aggregates `last_layernorm` values by concatenating along the first dimension.\n\n    Args:\n        old (dict): Previous stored dictionary, where each value is a tensor of shape [m, N].\n        new (dict): New incoming dictionary, where each value is a tensor of shape [1, N].\n\n    Returns:\n        dict: Aggregated dictionary where each value is a tensor of shape [m+1, N].\n    \"\"\"\n    if old is None:\n        return new  # If there's no existing data, just return the new one.\n\n    if not isinstance(old, dict) or not isinstance(new, dict):\n        raise TypeError(\"Both old and new values must be dictionaries.\")\n\n    aggregated = {}\n    for key in new:\n        if key not in old:\n            aggregated[key] = new[key]  # If key is new, just add it.\n        else:\n            if not isinstance(old[key], torch.Tensor) or not isinstance(\n                new[key], torch.Tensor\n            ):\n                raise TypeError(f\"Values for key {key} must be tensors.\")\n\n            if old[key].shape[-1] != new[key].shape[-1]:\n                raise ValueError(\n                    f\"Tensor shape mismatch for key {key}: \"\n                    f\"{old[key].shape} vs {new[key].shape}\"\n                )\n\n            # Concatenate along the first dimension\n            aggregated[key] = torch.cat([old[key], new[key]], dim=0)\n\n    return aggregated\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.just_me","title":"<code>just_me(old, new)</code>","text":"<p>If no old value, start a list; otherwise, add new to the list.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def just_me(old, new):\n    \"\"\"If no old value, start a list; otherwise, add new to the list.\"\"\"\n    return [new] if old is None else old + new\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.just_old","title":"<code>just_old(old, new)</code>","text":"<p>Always return the new value (or if old is None, return new).</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def just_old(old, new):\n    \"\"\"Always return the new value (or if old is None, return new).\"\"\"\n    return new if old is None else new\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.sublist","title":"<code>sublist(old, new)</code>","text":"<p>Aggregates by flattening. If old is already a list, extend it; otherwise, put old into a list and add new (flattening if needed).</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def sublist(old, new):\n    \"\"\"\n    Aggregates by flattening. If old is already a list, extend it;\n    otherwise, put old into a list and add new (flattening if needed).\n    \"\"\"\n    all_values = []\n    if old is not None:\n        if isinstance(old, list):\n            all_values.extend(old)\n        else:\n            all_values.append(old)\n    if isinstance(new, list):\n        all_values.extend(new)\n    else:\n        all_values.append(new)\n    return all_values\n</code></pre>"},{"location":"api/interpretability/hooked_model/","title":"Hooked model","text":""},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.ExtractionConfig","title":"<code>ExtractionConfig</code>  <code>dataclass</code>","text":"<p>Configuration of the extraction of the activations of the model. It store what activations you want to extract from the model.</p> <p>Parameters:</p> Name Type Description Default <code>extract_resid_in</code> <code>bool</code> <p>if True, extract the input of the residual stream</p> <code>False</code> <code>extract_resid_mid</code> <code>bool</code> <p>if True, extract the output of the intermediate stream</p> <code>False</code> <code>extract_resid_out</code> <code>bool</code> <p>if True, extract the output of the residual stream</p> <code>False</code> <code>extract_resid_in_post_layernorm</code> <code>bool</code> <p>if True, extract the input of the residual stream after the layernorm</p> <code>False</code> <code>extract_attn_pattern</code> <code>bool</code> <p>if True, extract the attention pattern of the attn</p> <code>False</code> <code>extract_head_values_projected</code> <code>bool</code> <p>if True, extract the values vectors projected of the model</p> <code>False</code> <code>extract_head_keys_projected</code> <code>bool</code> <p>if True, extract the key vectors projected of the model</p> <code>False</code> <code>extract_head_queries_projected</code> <code>bool</code> <p>if True, extract the query vectors projected of the model</p> <code>False</code> <code>extract_head_keys</code> <code>bool</code> <p>if True, extract the keys of the attention</p> <code>False</code> <code>extract_head_values</code> <code>bool</code> <p>if True, extract the values of the attention</p> <code>False</code> <code>extract_head_queries</code> <code>bool</code> <p>if True, extract the queries of the attention</p> <code>False</code> <code>extract_values</code> <code>bool</code> <p>if True, extract the values. This do not reshape the values to the attention heads as extract_head_values does</p> <code>False</code> <code>extract_keys</code> <code>bool</code> <p>if True, extract the keys. This do not reshape the keys to the attention heads as extract_head_keys does</p> <code>False</code> <code>extract_queries</code> <code>bool</code> <p>if True, extract the queries. This do not reshape the queries to the attention heads as extract_head_queries does</p> <code>False</code> <code>extract_last_layernorm</code> <code>bool</code> <p>if True, extract the last layernorm of the model</p> <code>False</code> <code>extract_head_out</code> <code>bool</code> <p>if True, extract the output of the heads [DEPRECATED]</p> <code>False</code> <code>extract_attn_out</code> <code>bool</code> <p>if True, extract the output of the attention of the attn_heads passed</p> <code>False</code> <code>extract_attn_in</code> <code>bool</code> <p>if True, extract the input of the attention of the attn_heads passed</p> <code>False</code> <code>extract_mlp_out</code> <code>bool</code> <p>if True, extract the output of the mlp of the attn</p> <code>False</code> <code>save_input_ids</code> <code>bool</code> <p>if True, save the input_ids in the cache</p> <code>False</code> <code>avg</code> <code>bool</code> <p>if True, extract the average of the activations over the target positions</p> <code>False</code> <code>avg_over_example</code> <code>bool</code> <p>if True, extract the average of the activations over the examples (it required a external cache to save the running avg)</p> <code>False</code> <code>attn_heads</code> <code>Union[list[dict], Literal['all']]</code> <p>list of dictionaries with the layer and head to extract the attention pattern or 'all' to</p> <code>'all'</code> <code>attn_pattern_avg</code> <code>Literal['mean', 'sum', 'baseline_ratio', 'none']</code> <p>the type of average to perform over the attention pattern. See hook.py attention_pattern_head for more details</p> <code>'none'</code> <code>attn_pattern_row_positions</code> <code>Optional[Union[List[int], List[Tuple], List[str], List[Union[int, Tuple, str]]]</code> <p>the row positions of the attention pattern to extract. See hook.py attention_pattern_head for more details</p> <code>None</code> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@dataclass\nclass ExtractionConfig:\n    \"\"\"\n    Configuration of the extraction of the activations of the model. It store what activations you want to extract from the model.\n\n    Arguments:\n        extract_resid_in (bool): if True, extract the input of the residual stream\n        extract_resid_mid (bool): if True, extract the output of the intermediate stream\n        extract_resid_out (bool): if True, extract the output of the residual stream\n        extract_resid_in_post_layernorm(bool): if True, extract the input of the residual stream after the layernorm\n        extract_attn_pattern (bool): if True, extract the attention pattern of the attn\n        extract_head_values_projected (bool): if True, extract the values vectors projected of the model\n        extract_head_keys_projected (bool): if True, extract the key vectors projected of the model\n        extract_head_queries_projected (bool): if True, extract the query vectors projected of the model\n        extract_head_keys (bool): if True, extract the keys of the attention\n        extract_head_values (bool): if True, extract the values of the attention\n        extract_head_queries (bool): if True, extract the queries of the attention\n        extract_values (bool): if True, extract the values. This do not reshape the values to the attention heads as extract_head_values does\n        extract_keys (bool): if True, extract the keys. This do not reshape the keys to the attention heads as extract_head_keys does\n        extract_queries (bool): if True, extract the queries. This do not reshape the queries to the attention heads as extract_head_queries does\n        extract_last_layernorm (bool): if True, extract the last layernorm of the model\n        extract_head_out (bool): if True, extract the output of the heads [DEPRECATED]\n        extract_attn_out (bool): if True, extract the output of the attention of the attn_heads passed\n        extract_attn_in (bool): if True, extract the input of the attention of the attn_heads passed\n        extract_mlp_out (bool): if True, extract the output of the mlp of the attn\n        save_input_ids (bool): if True, save the input_ids in the cache\n        avg (bool): if True, extract the average of the activations over the target positions\n        avg_over_example (bool): if True, extract the average of the activations over the examples (it required a external cache to save the running avg)\n        attn_heads (Union[list[dict], Literal[\"all\"]]): list of dictionaries with the layer and head to extract the attention pattern or 'all' to\n        attn_pattern_avg (Literal[\"mean\", \"sum\", \"baseline_ratio\", \"none\"]): the type of average to perform over the attention pattern. See hook.py attention_pattern_head for more details\n        attn_pattern_row_positions (Optional[Union[List[int], List[Tuple], List[str], List[Union[int, Tuple, str]]]): the row positions of the attention pattern to extract. See hook.py attention_pattern_head for more details\n    \"\"\"\n\n    extract_embed: bool = False\n    extract_resid_in: bool = False\n    extract_resid_mid: bool = False\n    extract_resid_out: bool = False\n    extract_resid_in_post_layernorm: bool = False\n    extract_attn_pattern: bool = False\n    extract_head_values_projected: bool = False\n    extract_head_keys_projected: bool = False\n    extract_head_queries_projected: bool = False\n    extract_head_keys: bool = False\n    extract_head_values: bool = False\n    extract_head_queries: bool = False\n    extract_values: bool = False\n    extract_keys: bool = False\n    extract_queries: bool = False\n    extract_head_out: bool = False\n    extract_attn_out: bool = False\n    extract_attn_in: bool = False\n    extract_mlp_out: bool = False\n    extract_last_layernorm: bool = False\n    save_input_ids: bool = False\n    avg: bool = False\n    avg_over_example: bool = False\n    attn_heads: Union[list[dict], Literal[\"all\"]] = \"all\"\n    attn_pattern_avg: Literal[\"mean\", \"sum\", \"baseline_ratio\", \"none\"] = \"none\"\n    attn_pattern_row_positions: Optional[\n        Union[List[int], List[Tuple], List[str], List[Union[int, Tuple, str]]]\n    ] = None\n    save_logits: bool = True\n    keep_gradient: bool = False  # New flag\n\n    def is_not_empty(self):\n        \"\"\"\n        Return True if at least one extraction option is enabled in the config, False otherwise.\n        \"\"\"\n        return any(\n            [\n                self.extract_resid_in,\n                self.extract_resid_mid,\n                self.extract_resid_out,\n                self.extract_attn_pattern,\n                self.extract_head_values_projected,\n                self.extract_head_keys_projected,\n                self.extract_head_queries_projected,\n                self.extract_head_keys,\n                self.extract_head_values,\n                self.extract_head_queries,\n                self.extract_head_out,\n                self.extract_attn_out,\n                self.extract_attn_in,\n                self.extract_mlp_out,\n                self.save_input_ids,\n                self.avg,\n                self.avg_over_example,\n            ]\n        )\n\n    def to_dict(self):\n        \"\"\"\n        Return the configuration as a dictionary.\n        \"\"\"\n        return self.__dict__\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.ExtractionConfig.is_not_empty","title":"<code>is_not_empty()</code>","text":"<p>Return True if at least one extraction option is enabled in the config, False otherwise.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def is_not_empty(self):\n    \"\"\"\n    Return True if at least one extraction option is enabled in the config, False otherwise.\n    \"\"\"\n    return any(\n        [\n            self.extract_resid_in,\n            self.extract_resid_mid,\n            self.extract_resid_out,\n            self.extract_attn_pattern,\n            self.extract_head_values_projected,\n            self.extract_head_keys_projected,\n            self.extract_head_queries_projected,\n            self.extract_head_keys,\n            self.extract_head_values,\n            self.extract_head_queries,\n            self.extract_head_out,\n            self.extract_attn_out,\n            self.extract_attn_in,\n            self.extract_mlp_out,\n            self.save_input_ids,\n            self.avg,\n            self.avg_over_example,\n        ]\n    )\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.ExtractionConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Return the configuration as a dictionary.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Return the configuration as a dictionary.\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel","title":"<code>HookedModel</code>","text":"<p>Wrapper around a HuggingFace model for extracting activations and supporting mechanistic interpretability methods.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>class HookedModel:\n    \"\"\"\n    Wrapper around a HuggingFace model for extracting activations and supporting mechanistic interpretability methods.\n    \"\"\"\n\n    def __init__(self, config: HookedModelConfig, log_file_path: Optional[str] = None):\n        self.config = config\n        self.hf_model, self.hf_language_model, self.model_config = (\n            ModelFactory.load_model(\n                model_name=config.model_name,\n                device_map=config.device_map,\n                torch_dtype=config.torch_dtype,\n                attn_implementation=\"eager\"\n                if config.attn_implementation == \"custom_eager\"\n                else config.attn_implementation,\n            )\n        )\n        self.hf_model.eval()\n        self.base_model = None\n        self.module_wrapper_manager = ModuleWrapperManager(model=self.hf_model)\n        self.intervention_manager = InterventionManager(model_config=self.model_config)\n\n        tokenizer, processor = TokenizerFactory.load_tokenizer(\n            model_name=config.model_name,\n            torch_dtype=config.torch_dtype,\n            device_map=config.device_map,\n        )\n        self.hf_tokenizer = tokenizer\n        self.input_handler = InputHandler(model_name=config.model_name)\n        if processor is True:\n            self.processor = tokenizer\n            self.text_tokenizer = self.processor.tokenizer  # type: ignore\n        else:\n            self.processor = None\n            self.text_tokenizer = tokenizer\n\n        self.first_device = next(self.hf_model.parameters()).device\n        device_num = torch.cuda.device_count()\n        logger.info(\n            f\"HookedModel: Model loaded in {device_num} devices. First device: {self.first_device}\"\n        )\n        self.act_type_to_hook_name = {\n            \"resid_in\": self.model_config.residual_stream_input_hook_name,\n            \"resid_out\": self.model_config.residual_stream_hook_name,\n            \"resid_mid\": self.model_config.intermediate_stream_hook_name,\n            \"attn_out\": self.model_config.attn_out_hook_name,\n            \"attn_in\": self.model_config.attn_in_hook_name,\n            \"values\": self.model_config.head_value_hook_name,\n            # Add other act_types if needed\n        }\n        self.additional_hooks = []\n        self.additional_interventions = []\n        self.assert_all_modules_exist()\n\n        self.image_placeholder = yaml_config[\"tokenizer_placeholder\"][config.model_name]\n\n        if self.config.attn_implementation == \"custom_eager\":\n            logger.info(\n                \"\"\" HookedModel:\n                            The model is using the custom eager attention implementation that support attention matrix hooks because I get config.attn_impelemntation == 'custom_eager'. If you don't want this, you can call HookedModel.restore_original_modules. \n                            However, we reccomend using this implementation since the base one do not contains attention matrix hook resulting in unexpected behaviours. \n                            \"\"\",\n            )\n            self.set_custom_modules()\n\n    def __repr__(self):\n        return f\"\"\"HookedModel(model_name={self.config.model_name}):\n        {self.hf_model.__repr__()}\n    \"\"\"\n\n    @classmethod\n    def from_pretrained(cls, model_name: str, **kwargs):\n        return cls(HookedModelConfig(model_name=model_name, **kwargs))\n\n    def assert_module_exists(self, component: str):\n        # Remove '.input' or '.output' from the component\n        component = component.replace(\".input\", \"\").replace(\".output\", \"\")\n\n        # Check if '{}' is in the component, indicating layer indexing\n        if \"{}\" in component:\n            for i in range(0, self.model_config.num_hidden_layers):\n                attr_name = component.format(i)\n\n                try:\n                    get_attribute_by_name(self.hf_model, attr_name)\n                except AttributeError:\n                    try:\n                        if attr_name in self.module_wrapper_manager:\n                            self.set_custom_modules()\n                            get_attribute_by_name(self.hf_model, attr_name)\n                            self.restore_original_modules()\n                    except AttributeError:\n                        raise ValueError(\n                            f\"Component '{attr_name}' does not exist in the model. Please check the model configuration.\"\n                        )\n        else:\n            try:\n                get_attribute_by_name(self.hf_model, component)\n            except AttributeError:\n                raise ValueError(\n                    f\"Component '{component}' does not exist in the model. Please check the model configuration.\"\n                )\n\n    def assert_all_modules_exist(self):\n        # get the list of all attributes of model_config\n        all_attributes = [attr_name for attr_name in self.model_config.__dict__.keys()]\n        # save just the attributes that have \"hook\" in the name\n        hook_attributes = [\n            attr_name for attr_name in all_attributes if \"hook\" in attr_name\n        ]\n        for hook_attribute in hook_attributes:\n            self.assert_module_exists(getattr(self.model_config, hook_attribute))\n\n    def set_custom_modules(self):\n        \"\"\"\n        Substitute custom modules (e.g., attention) into the model for advanced interpretability.\n        \"\"\"\n        logger.info(\"HookedModel: Setting custom modules.\")\n        self.module_wrapper_manager.substitute_attention_module(self.hf_model)\n\n    def restore_original_modules(self):\n        \"\"\"\n        Restore the original modules of the model, removing any custom substitutions.\n        \"\"\"\n        logger.info(\"HookedModel: Restoring original modules.\")\n        self.module_wrapper_manager.restore_original_attention_module(self.hf_model)\n\n    def is_multimodal(self) -&gt; bool:\n        \"\"\"\n        Return True if the model supports multimodal inputs (e.g., images), False otherwise.\n        \"\"\"\n        if self.processor is not None:\n            return True\n        return False\n\n    def use_full_model(self):\n        \"\"\"\n        Switch to the full model (including multimodal components if available).\n        \"\"\"\n        if self.processor is not None:\n            logger.debug(\"HookedModel: Using full model capabilities\")\n            if self.base_model is not None:\n                self.hf_model = self.base_model\n                self.model_config.restore_full_model()\n                self.base_model = None\n        else:\n            if self.base_model is not None:\n                self.hf_model = self.base_model\n            logger.debug(\"HookedModel: Using full text only model capabilities\")\n\n    def use_language_model_only(self):\n        \"\"\"\n        Switch to using only the language model component (text-only mode).\n        \"\"\"\n        if self.hf_language_model is None:\n            logger.warning(\n                \"HookedModel: The model does not have a separate language model that can be used\",\n            )\n        else:\n            # check if we are already using the language model\n            if self.hf_model == self.hf_language_model:\n                return\n            self.base_model = self.hf_model\n            self.hf_model = self.hf_language_model\n            self.model_config.use_language_model()\n            logger.debug(\"HookedModel: Using only language model capabilities\")\n\n    def get_tokenizer(self):\n        \"\"\"\n        Return the tokenizer associated with the model.\n        \"\"\"\n        return self.hf_tokenizer\n\n    def get_text_tokenizer(self):\n        r\"\"\"\n        If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer\n\n        Args:\n            None\n\n        Returns:\n            tokenizer: the tokenizer of the model\n        \"\"\"\n        if self.processor is not None:\n            if not hasattr(self.processor, \"tokenizer\"):\n                raise ValueError(\"The processor does not have a tokenizer\")\n            return self.processor.tokenizer  # type: ignore\n        return self.hf_tokenizer\n\n    def get_processor(self):\n        r\"\"\"\n        Return the processor of the model (None if the model does not have a processor, i.e. text only model)\n\n        Args:\n            None\n\n        Returns:\n            processor: the processor of the model\n        \"\"\"\n        if self.processor is None:\n            raise ValueError(\"The model does not have a processor\")\n        return self.processor\n\n    def get_lm_head(self):\n        \"\"\"\n        Return the language modeling head (output projection layer) of the model.\n        \"\"\"\n        return get_attribute_by_name(self.hf_model, self.model_config.unembed_matrix)\n\n    def get_last_layernorm(self):\n        \"\"\"\n        Return the last layer normalization module of the model.\n        \"\"\"\n        return get_attribute_by_name(self.hf_model, self.model_config.last_layernorm)\n\n    def get_image_placeholder(self) -&gt; str:\n        \"\"\"\n        Return the image placeholder string used by the tokenizer for multimodal models.\n        \"\"\"\n        return self.image_placeholder\n\n    def eval(self):\n        \"\"\"\n        Set the model to evaluation mode.\n        \"\"\"\n        self.hf_model.eval()\n\n    def device(self):\n        \"\"\"\n        Return the device (e.g., 'cuda', 'cpu') where the model is located.\n        \"\"\"\n        return self.first_device\n\n    def register_forward_hook(self, component: str, hook_function: Callable):\n        r\"\"\"\n        Register a forward hook on a model component.\n\n        Args:\n            component (str): Name of the model component.\n            hook_function (Callable): Function to call during forward pass.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; def hook_function(module, input, output):\n            &gt;&gt;&gt;     # your code here\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt; model.register_forward_hook(\"model.layers[0].self_attn\", hook_function)\n        \"\"\"\n        self.additional_hooks.append(\n            {\n                \"component\": component,\n                \"intervention\": hook_function,\n            }\n        )\n\n    def to_string_tokens(\n        self,\n        tokens: Union[list, torch.Tensor],\n    ):\n        r\"\"\"\n        Transform a list or a tensor of tokens in a list of string tokens.\n\n        Args:\n            tokens (Union[list, torch.Tensor]): the tokens to transform in string tokens\n\n        Returns:\n            string_tokens (list): the list of string tokens\n\n        Examples:\n            &gt;&gt;&gt; tokens = [101, 1234, 1235, 102]\n            &gt;&gt;&gt; model.to_string_tokens(tokens)\n            ['[CLS]', 'hello', 'world', '[SEP]']\n        \"\"\"\n        if isinstance(tokens, torch.Tensor):\n            if tokens.dim() == 1:\n                tokens = tokens.tolist()\n            else:\n                tokens = tokens.squeeze().tolist()\n        string_tokens = []\n        for tok in tokens:\n            string_tokens.append(self.hf_tokenizer.decode(tok))  # type: ignore\n        return string_tokens\n\n    def register_interventions(self, interventions: List[Intervention]):\n        \"\"\"\n        Register a list of interventions to be applied during forward passes.\n        \"\"\"\n        self.additional_interventions = interventions\n        logger.debug(f\"HookedModel: Registered {len(interventions)} interventions\")\n\n    def clean_interventions(self):\n        \"\"\"\n        Remove all registered interventions.\n        \"\"\"\n        self.additional_interventions = []\n        logger.debug(\n            f\"HookedModel: Removed {len(self.additional_interventions)} interventions\"\n        )\n\n    def create_hooks(\n        self,\n        inputs,\n        cache: ActivationCache,\n        token_indexes: List,\n        token_dict: Dict,\n        # string_tokens: List[str],\n        extraction_config: ExtractionConfig = ExtractionConfig(),\n        interventions: Optional[List[Intervention]] = None,\n        batch_idx: Optional[int] = None,\n        external_cache: Optional[ActivationCache] = None,\n    ):\n        r\"\"\"\n        Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.\n\n        Arguments:\n            inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n            cache (ActivationCache): dictionary where the activations of the model will be saved\n            token_indexes (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n            token_dict (Dict): dictionary with the token indexes\n            extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model (default = ExtractionConfig())\n            interventions (Optional[List[Intervention]]): list of interventions to perform during forward pass\n            batch_idx (Optional[int]): index of the batch in the dataloader\n            external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n\n        Returns:\n            hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n        \"\"\"\n        hooks = []\n\n        # compute layer and head indexes\n        if (\n            isinstance(extraction_config.attn_heads, str)\n            and extraction_config.attn_heads == \"all\"\n        ):\n            layer_indexes = [i for i in range(0, self.model_config.num_hidden_layers)]\n            head_indexes = [\"all\"] * len(layer_indexes)\n        elif isinstance(extraction_config.attn_heads, list):\n            layer_head_indexes = [\n                (el[\"layer\"], el[\"head\"]) for el in extraction_config.attn_heads\n            ]\n            layer_indexes = [el[0] for el in layer_head_indexes]\n            head_indexes = [el[1] for el in layer_head_indexes]\n        else:\n            raise ValueError(\n                \"attn_heads must be 'all' or a list of dictionaries as [{'layer': 0, 'head': 0}]\"\n            )\n        # register the intervention hooks as first thing to do\n        if self.additional_interventions is not None:\n            hooks += self.intervention_manager.create_intervention_hooks(\n                interventions=self.additional_interventions, token_dict=token_dict\n            )\n\n        if extraction_config.extract_resid_out:\n            # assert that the component exists in the model\n            hooks += [\n                {\n                    \"component\": self.model_config.residual_stream_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_out_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n        if extraction_config.extract_resid_in:\n            # assert that the component exists in the model\n            hooks += [\n                {\n                    \"component\": self.model_config.residual_stream_input_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_in_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_resid_in_post_layernorm:\n            hooks += [\n                {\n                    \"component\": self.model_config.residual_stream_input_post_layernorm_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_in_post_layernorm_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.save_input_ids:\n            hooks += [\n                {\n                    \"component\": self.model_config.embed_tokens,\n                    \"intervention\": partial(\n                        embed_hook,\n                        token_indexes=token_indexes,\n                        cache=cache,\n                        cache_key=\"input_ids\",\n                    ),\n                }\n            ]\n\n        if extraction_config.extract_embed:  # New block\n            hooks += [\n                {\n                    \"component\": self.model_config.embed_tokens,  # Use the embedding module name directly\n                    \"intervention\": partial(\n                        input_embedding_hook,\n                        cache=cache,\n                        cache_key=\"input_embeddings\",\n                        token_indexes=token_indexes,\n                        keep_gradient=extraction_config.keep_gradient,\n                        avg=extraction_config.avg,\n                    ),\n                }\n            ]\n\n        if extraction_config.extract_head_queries:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_query_hook_name.format(i),\n                    \"intervention\": partial(\n                        query_key_value_hook,\n                        cache=cache,\n                        cache_key=\"head_queries_\",\n                        token_indexes=token_indexes,\n                        head_dim=self.model_config.head_dim,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_key_value_groups=self.model_config.num_key_value_groups,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_values:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_value_hook_name.format(i),\n                    \"intervention\": partial(\n                        query_key_value_hook,\n                        cache=cache,\n                        cache_key=\"head_values_\",\n                        token_indexes=token_indexes,\n                        head_dim=self.model_config.head_dim,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_key_value_groups=self.model_config.num_key_value_groups,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_keys:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_key_hook_name.format(i),\n                    \"intervention\": partial(\n                        query_key_value_hook,\n                        cache=cache,\n                        cache_key=\"head_keys_\",\n                        token_indexes=token_indexes,\n                        head_dim=self.model_config.head_dim,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_key_value_groups=self.model_config.num_key_value_groups,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_values:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_value_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"values_L{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_keys:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_key_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"keys_L{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_queries:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_query_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"queries_L{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_head_out:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_o_proj_input_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        head_out_hook,\n                        cache=cache,\n                        cache_key=\"head_out_\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_heads=self.model_config.num_attention_heads,\n                        head_dim=self.model_config.head_dim,\n                        o_proj_weight=get_attribute_from_name(\n                            self.hf_model,\n                            self.model_config.attn_out_proj_weight.format(i),\n                        ),\n                        o_proj_bias=get_attribute_from_name(\n                            self.hf_model,\n                            self.model_config.attn_out_proj_bias.format(i),\n                        ),\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_attn_in:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_in_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"attn_in_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_attn_out:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_out_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"attn_out_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        # if extraction_config.extract_avg:\n        #     # Define a hook that saves the activations of the residual stream\n        #     raise NotImplementedError(\n        #         \"The hook for the average is not working with token_index as a list\"\n        #     )\n\n        #     # hooks.extend(\n        #     #     [\n        #     #         {\n        #     #             \"component\": self.model_config.residual_stream_hook_name.format(\n        #     #                 i\n        #     #             ),\n        #     #             \"intervention\": partial(\n        #     #                 avg_hook,\n        #     #                 cache=cache,\n        #     #                 cache_key=\"resid_avg_{}\".format(i),\n        #     #                 last_image_idx=last_image_idxs, #type\n        #     #                 end_image_idx=end_image_idxs,\n        #     #             ),\n        #     #         }\n        #     #         for i in range(0, self.model_config.num_hidden_layers)\n        #     #     ]\n        #     # )\n        if extraction_config.extract_resid_mid:\n            hooks += [\n                {\n                    \"component\": self.model_config.intermediate_stream_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_mid_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n            # if we want to extract the output of the heads\n        if extraction_config.extract_mlp_out:\n            hooks += [\n                {\n                    \"component\": self.model_config.mlp_out_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"mlp_out_{i}\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_last_layernorm:\n            hooks += [\n                {\n                    \"component\": self.model_config.last_layernorm_hook_name,\n                    \"intervention\": partial(\n                        layernom_hook,\n                        cache=cache,\n                        cache_key=\"last_layernorm\",\n                        token_indexes=token_indexes,\n                        avg=extraction_config.avg,\n                    ),\n                }\n            ]\n        # ABLATION AND PATCHING\n        if interventions is not None:\n            hooks += self.intervention_manager.create_intervention_hooks(\n                interventions=interventions, token_dict=token_dict\n            )\n        if extraction_config.extract_head_values_projected:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_value_hook_name.format(i),\n                    \"intervention\": partial(\n                        projected_value_vectors_head,\n                        cache=cache,\n                        token_indexes=token_indexes,\n                        layer=i,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                        num_key_value_heads=self.model_config.num_key_value_heads,\n                        hidden_size=self.model_config.hidden_size,\n                        d_head=self.model_config.head_dim,\n                        out_proj_weight=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                        ),\n                        out_proj_bias=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                        ),\n                        head=head,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_keys_projected:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_key_hook_name.format(i),\n                    \"intervention\": partial(\n                        projected_key_vectors_head,\n                        cache=cache,\n                        token_indexes=token_indexes,\n                        layer=i,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                        num_key_value_heads=self.model_config.num_key_value_heads,\n                        hidden_size=self.model_config.hidden_size,\n                        d_head=self.model_config.head_dim,\n                        out_proj_weight=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                        ),\n                        out_proj_bias=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                        ),\n                        head=head,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_queries_projected:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_query_hook_name.format(i),\n                    \"intervention\": partial(\n                        projected_query_vectors_head,\n                        cache=cache,\n                        token_indexes=token_indexes,\n                        layer=i,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                        num_key_value_heads=self.model_config.num_key_value_heads,\n                        hidden_size=self.model_config.hidden_size,\n                        d_head=self.model_config.head_dim,\n                        out_proj_weight=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                        ),\n                        out_proj_bias=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                        ),\n                        head=head,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_attn_pattern:\n            if extraction_config.avg_over_example:\n                if external_cache is None:\n                    logger.warning(\n                        \"\"\"The external_cache is None. The average could not be computed since missing an external cache where store the iterations.\n                        \"\"\"\n                    )\n                elif batch_idx is None:\n                    logger.warning(\n                        \"\"\"The batch_idx is None. The average could not be computed since missing the batch index.\n\n                        \"\"\"\n                    )\n                else:\n                    # move the cache to the same device of the model\n                    external_cache.to(self.first_device)\n                    hooks += [\n                        {\n                            \"component\": self.model_config.attn_matrix_hook_name.format(\n                                i\n                            ),\n                            \"intervention\": partial(\n                                avg_attention_pattern_head,\n                                token_indexes=token_indexes,\n                                layer=i,\n                                attn_pattern_current_avg=external_cache,\n                                batch_idx=batch_idx,\n                                cache=cache,\n                                # avg=extraction_config.avg,\n                                extract_avg_value=extraction_config.extract_head_values_projected,\n                            ),\n                        }\n                        for i in range(0, self.model_config.num_hidden_layers)\n                    ]\n            else:\n                hooks += [\n                    {\n                        \"component\": self.model_config.attn_matrix_hook_name.format(i),\n                        \"intervention\": partial(\n                            attention_pattern_head,\n                            token_indexes=token_indexes,\n                            cache=cache,\n                            layer=i,\n                            head=head,\n                            attn_pattern_avg=extraction_config.attn_pattern_avg,\n                            attn_pattern_row_partition=None\n                            if extraction_config.attn_pattern_row_positions is None\n                            else tuple(token_dict[\"attn_pattern_row_positions\"]),\n                        ),\n                    }\n                    for i, head in zip(layer_indexes, head_indexes)\n                ]\n\n            # if additional hooks are not empty, add them to the hooks list\n        if self.additional_hooks:\n            for hook in self.additional_hooks:\n                hook[\"intervention\"] = partial(\n                    hook[\"intervention\"],\n                    cache=cache,\n                    token_indexes=token_indexes,\n                    token_dict=token_dict,\n                    **hook[\"intervention\"],\n                )\n                hooks.append(hook)\n        return hooks\n\n    @conditional_no_grad()\n    # @torch.no_grad()\n    def forward(\n        self,\n        inputs,\n        target_token_positions: Union[\n            List[Union[str, int, Tuple[int, int]]],\n            List[str],\n            List[int],\n            List[Tuple[int, int]],\n        ] = [\"all\"],\n        pivot_positions: Optional[List[int]] = None,\n        extraction_config: ExtractionConfig = ExtractionConfig(),\n        interventions: Optional[List[Intervention]] = None,\n        external_cache: Optional[ActivationCache] = None,\n        # attn_heads: Union[list[dict], Literal[\"all\"]] = \"all\",\n        batch_idx: Optional[int] = None,\n        move_to_cpu: bool = False,\n        vocabulary_index: Optional[int] = None,\n        **kwargs,\n    ) -&gt; ActivationCache:\n        r\"\"\"\n        Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.\n\n        Args:\n            inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n            target_token_positions (Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]): tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\", -1, (2,10)]). See TokenIndex.get_token_index for more details\n            pivot_positions (Optional[list[int]]): list of split positions of the tokens\n            extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model\n            ablation_queries (Optional[pd.DataFrame | None]): dataframe with the ablation queries to perform during forward pass\n            patching_queries (Optional[pd.DataFrame | None]): dataframe with the patching queries to perform during forward pass\n            external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n            attn_heads (Union[list[dict], Literal[\"all\"]]): list of dictionaries with the layer and head to extract the attention pattern or 'all' to\n            batch_idx (Optional[int]): index of the batch in the dataloader\n            move_to_cpu (bool): if True, move the activations to the cpu\n\n        Returns:\n            cache (ActivationCache): dictionary with the activations of the model\n\n        Examples:\n            &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n            &gt;&gt;&gt; model.forward(inputs, target_token_positions=[\"last\"], extract_resid_out=True)\n            {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'input_ids': tensor([[101, 1234, 1235, 102]]), 'mapping_index': {'last': [0]}}\n        \"\"\"\n\n        if target_token_positions is None and extraction_config.is_not_empty():\n            raise ValueError(\n                \"target_token_positions must be passed if we want to extract the activations of the model\"\n            )\n\n        cache = ActivationCache()\n        string_tokens = self.to_string_tokens(\n            self.input_handler.get_input_ids(inputs).squeeze()\n        )\n        token_index_finder = TokenIndex(\n            self.config.model_name, pivot_positions=pivot_positions\n        )\n        token_indexes, token_dict = token_index_finder.get_token_index(\n            tokens=target_token_positions,\n            string_tokens=string_tokens,\n            return_type=\"all\",\n        )\n        if extraction_config.attn_pattern_row_positions is not None:\n            token_row_indexes, _ = token_index_finder.get_token_index(\n                tokens=extraction_config.attn_pattern_row_positions,\n                string_tokens=string_tokens,\n                return_type=\"all\",\n            )\n            token_dict[\"attn_pattern_row_positions\"] = token_row_indexes\n\n        assert isinstance(token_indexes, list), \"Token index must be a list\"\n        assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n\n        hooks = self.create_hooks(  # TODO: add **kwargs\n            inputs=inputs,\n            token_dict=token_dict,\n            token_indexes=token_indexes,\n            cache=cache,\n            extraction_config=extraction_config,\n            interventions=interventions,\n            batch_idx=batch_idx,\n            external_cache=external_cache,\n        )\n\n        hook_handlers = self.set_hooks(hooks)\n        inputs = self.input_handler.prepare_inputs(\n            inputs, self.first_device, self.config.torch_dtype\n        )\n        # forward pass\n        output = self.hf_model(\n            **inputs,\n            # output_original_output=True,\n            # output_attentions=extract_attn_pattern,\n        )\n\n        # save the logit of the target_token_positions\n        flatten_target_token_positions = [\n            item for sublist in token_indexes for item in sublist\n        ]\n        if extraction_config.save_logits:\n            cache[\"logits\"] = output.logits[:, flatten_target_token_positions, :]\n        # since attention_patterns are returned in the output, we need to adapt to the cache structure\n        if move_to_cpu:\n            cache.cpu()\n            if external_cache is not None:\n                external_cache.cpu()\n\n        stored_token_dict = {}\n        mapping_index = {}\n        current_index = 0\n\n        for token in target_token_positions:\n            mapping_index[token] = []\n            if isinstance(token_dict, int):\n                mapping_index[token].append(current_index)\n                stored_token_dict[token] = token_dict\n                current_index += 1\n            elif isinstance(token_dict, dict):\n                stored_token_dict[token] = token_dict[token]\n                for idx in range(len(token_dict[token])):\n                    mapping_index[token].append(current_index)\n                    current_index += 1\n            elif isinstance(token_dict, list):\n                stored_token_dict[token] = token_dict\n                for idx in range(len(token_dict)):\n                    mapping_index[token].append(current_index)\n                    current_index += 1\n            else:\n                raise ValueError(\"Token dict must be an int, a dict or a list\")\n        # update the mapping index in the cache if avg\n        if extraction_config.avg:\n            for i, token in enumerate(target_token_positions):\n                mapping_index[token] = [i]\n            mapping_index[\"info\"] = \"avg\"\n        cache[\"mapping_index\"] = mapping_index\n        cache[\"token_dict\"] = stored_token_dict\n        self.remove_hooks(hook_handlers)\n\n        if extraction_config.keep_gradient:\n            assert vocabulary_index is not None, (\n                \"dict_token_index must be provided if extract_input_embeddings_for_grad is True\"\n            )\n            self._compute_input_gradients(cache, output.logits, vocabulary_index)\n\n        return cache\n\n    def __call__(self, *args, **kwds) -&gt; ActivationCache:\n        r\"\"\"\n        Call the forward method of the model\n        \"\"\"\n        return self.forward(*args, **kwds)\n\n    def predict(self, k=10, strip: bool = True, **kwargs):\n        out = self.forward(**kwargs)\n        logits = out[\"logits\"][:, -1, :]\n        probs = torch.softmax(logits, dim=-1)\n        probs = probs.squeeze()\n        topk = torch.topk(probs, k)\n        # return a dictionary with the topk tokens and their probabilities\n        string_tokens = self.to_string_tokens(topk.indices)\n        token_probs = {}\n        for token, prob in zip(string_tokens, topk.values):\n            if strip:\n                token = token.strip()\n            if token not in token_probs:\n                token_probs[token] = prob.item()\n        return token_probs\n        # return {\n        #     token: prob.item() for token, prob in zip(string_tokens, topk.values)\n        # }\n\n    def get_module_from_string(self, component: str):\n        r\"\"\"\n        Return a module from the model given the string of the module.\n\n        Args:\n            component (str): the string of the module\n\n        Returns:\n            module (torch.nn.Module): the module of the model\n\n        Examples:\n            &gt;&gt;&gt; model.get_module_from_string(\"model.layers[0].self_attn\")\n            BertAttention(...)\n        \"\"\"\n        return self.hf_model.retrieve_modules_from_names(component)\n\n    def set_hooks(self, hooks: List[Dict[str, Any]]):\n        r\"\"\"\n        Set the hooks in the model\n\n        Args:\n            hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n\n        Returns:\n            hook_handlers (list): list of hook handlers\n        \"\"\"\n\n        if len(hooks) == 0:\n            return []\n\n        hook_handlers = []\n        for hook in hooks:\n            component = hook[\"component\"]\n            hook_function = hook[\"intervention\"]\n\n            # get the last module string (.input or .output) and remove it from the component string\n            last_module = component.split(\".\")[-1]\n            # now remove the last module from the component string\n            component = component[: -len(last_module) - 1]\n            # check if the component exists in the model\n            try:\n                self.assert_module_exists(component)\n            except ValueError as e:\n                logger.error(\n                    f\"Error: {e}. Probably the module {component} do not exists in the model. If the module is the attention_matrix_hook, try callig HookedModel.set_custom_hooks() or setting attn_implementation == 'custom_eager'.  Now we will skip the hook for the component {component}\"\n                )\n                continue\n            if last_module == \"input\":\n                hook_handlers.append(\n                    get_module_by_path(\n                        self.hf_model, component\n                    ).register_forward_pre_hook(\n                        partial(hook_function, output=None), with_kwargs=True\n                    )\n                )\n            elif last_module == \"output\":\n                hook_handlers.append(\n                    get_module_by_path(self.hf_model, component).register_forward_hook(\n                        hook_function, with_kwargs=True\n                    )\n                )\n            else:\n                logger.warning(\n                    f\"Warning: the last module of the component {component} is not 'input' or 'output'. We will skip this hook\"\n                )\n\n        return hook_handlers\n\n    def remove_hooks(self, hook_handlers):\n        \"\"\"\n        Remove all hooks from the model using the provided handlers.\n        \"\"\"\n        for hook_handler in hook_handlers:\n            hook_handler.remove()\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs,\n        generation_config: Optional[GenerationConfig] = None,\n        target_token_positions: Optional[List[str]] = None,\n        return_text: bool = False,\n        **kwargs,\n    ) -&gt; ActivationCache:\n        r\"\"\"\n        __WARNING__: This method could be buggy in the return dict of the output. Pay attention!\n\n        Generate new tokens using the model and the inputs passed as argument\n        Args:\n            inputs (dict): dictionary with the inputs of the model {\"input_ids\": ..., \"attention_mask\": ..., \"pixel_values\": ...}\n            generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration\n            **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)\n        Returns:\n            output (ActivationCache): dictionary with the output of the model\n\n        Examples:\n            &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n            &gt;&gt;&gt; model.generate(inputs)\n            {'sequences': tensor([[101, 1234, 1235, 102]])}\n        \"\"\"\n        # Initialize cache for logits\n        # raise NotImplementedError(\"This method is not working. It needs to be fixed\")\n        cache = ActivationCache()\n        hook_handlers = None\n        if (\n            target_token_positions is not None\n            or self.additional_interventions is not None\n        ):\n            string_tokens = self.to_string_tokens(\n                self.input_handler.get_input_ids(inputs).squeeze()\n            )\n            token_indexes, token_dict = TokenIndex(\n                self.config.model_name, pivot_positions=None\n            ).get_token_index(tokens=[], string_tokens=string_tokens, return_type=\"all\")\n            assert isinstance(token_indexes, list), \"Token index must be a list\"\n            assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n            hooks = self.create_hooks(\n                inputs=inputs,\n                token_dict=token_dict,\n                token_indexes=token_indexes,\n                cache=cache,\n                **kwargs,\n            )\n            hook_handlers = self.set_hooks(hooks)\n\n        inputs = self.input_handler.prepare_inputs(inputs, self.first_device)\n        # print(inputs.keys())\n        output = self.hf_model.generate(\n            **inputs,  # type: ignore\n            generation_config=generation_config,\n            # output_scores=False,  # type: ignore\n        )\n        if hook_handlers:\n            self.remove_hooks(hook_handlers)\n        if return_text:\n            return self.hf_tokenizer.decode(output[0], skip_special_tokens=True)  # type: ignore\n        if not cache.is_empty():\n            # if the cache is not empty, we will return the cache\n            output = {\"generation_output\": output, \"cache\": cache}\n        return output  # type: ignore\n\n    def extract_cache(\n        self,\n        dataloader,\n        target_token_positions: Union[\n            List[Union[str, int, Tuple[int, int]]],\n            List[str],\n            List[int],\n            List[Tuple[int, int]],\n        ],\n        extraction_config: ExtractionConfig = ExtractionConfig(),\n        interventions: Optional[List[Intervention]] = None,\n        batch_saver: Callable = lambda x: None,\n        move_to_cpu_after_forward: bool = True,\n        # save_other_batch_elements: bool = False,\n        **kwargs,\n    ):\n        r\"\"\"\n        Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.\n\n        Arguments:\n            - dataloader (iterable): dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)\n            - extracted_token_position (Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\", -1, (2,10)]). See TokenIndex.get_token_index for more details\n            - batch_saver (Callable): function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)\n            - move_to_cpu_after_forward (bool): if True, move the activations to the cpu right after the any forward pass of the model\n            - dict_token_index (Optional[torch.Tensor]): If provided, specifies the index in the vocabulary for which to compute gradients of logits with respect to input embeddings. Requires extraction_config.extract_input_embeddings_for_grad to be True.\n            - **kwargs: additional arguments to control hooks generation, basically accept any argument handled by the `.forward` method (i.e. ablation_queries, patching_queries, extract_resid_in)\n\n        Returns:\n            final_cache: dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n        Examples:\n            &gt;&gt;&gt; dataloader = [{\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]]), \"labels\": torch.tensor([1])}, ...]\n            &gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[\"last\"], batch_saver=lambda x: {\"labels\": x[\"labels\"]})\n            {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'labels': tensor([1]), 'mapping_index': {'last': [0]}}\n        \"\"\"\n\n        logger.info(\"HookedModel: Extracting cache\")\n\n        # get the function to save in the cache the additional element from the batch sime\n\n        logger.info(\"HookedModel: Forward pass started\")\n        all_cache = ActivationCache()  # a list of dictoionaries, each dictionary contains the activations of the model for a batch (so a dict of tensors)\n        attn_pattern = (\n            ActivationCache()\n        )  # Initialize the dictionary to hold running averages\n\n        # if register_agregation is in the kwargs, we will register the aggregation of the attention pattern\n        if \"register_aggregation\" in kwargs:\n            all_cache.register_aggregation(\n                kwargs[\"register_aggregation\"][0], kwargs[\"register_aggregation\"][1]\n            )\n            attn_pattern.register_aggregation(\n                kwargs[\"register_aggregation\"][0], kwargs[\"register_aggregation\"][1]\n            )\n\n        # example_dict = {}\n        n_batches = 0  # Initialize batch counter\n\n        for batch in progress(\n            dataloader, desc=\"Extracting cache\", total=len(dataloader)\n        ):\n            # log_memory_usage(\"Extract cache - Before batch\")\n            # tokens, others = batch\n            # inputs = {k: v.to(self.first_device) for k, v in tokens.items()}\n\n            # get input_ids, attention_mask, and if available, pixel_values from batch (that is a dictionary)\n            # then move them to the first device\n\n            inputs = self.input_handler.prepare_inputs(\n                batch, self.first_device\n            )  # require_grads is False, gradients handled by hook if needed\n            others = {k: v for k, v in batch.items() if k not in inputs}\n\n            cache = self.forward(\n                inputs,\n                target_token_positions=target_token_positions,\n                pivot_positions=batch.get(\"pivot_positions\", None),\n                external_cache=attn_pattern,\n                batch_idx=n_batches,\n                extraction_config=extraction_config,\n                interventions=interventions,\n                vocabulary_index=batch.get(\"vocabulary_index\", None),\n                **kwargs,\n            )\n\n            # Compute input gradients if requested\n\n            # possible memory leak from here -___---------------&gt;\n            additional_dict = batch_saver(\n                others\n            )  # TODO: Maybe keep the batch_saver in a different cache\n            if additional_dict is not None:\n                # cache = {**cache, **additional_dict}if a\n                cache.update(additional_dict)\n\n            if move_to_cpu_after_forward:\n                cache.cpu()\n\n            n_batches += 1  # Increment batch counter# Process and remove \"pattern_\" keys from cache\n            all_cache.cat(cache)\n\n            del cache\n\n            # Use the new cleanup_tensors method from InputHandler to free memory\n            self.input_handler.cleanup_tensors(inputs, others)\n\n            torch.cuda.empty_cache()\n\n        logger.debug(\"Forward pass finished - started to aggregate different batch\")\n        all_cache.update(attn_pattern)\n        # all_cache[\"example_dict\"] = example_dict\n        # logger.info(\"HookedModel: Aggregation finished\")\n\n        torch.cuda.empty_cache()\n\n        # add a metadata field to the cache\n        all_cache.add_metadata(\n            target_token_positions=target_token_positions,\n            model_name=self.config.model_name,\n            extraction_config=extraction_config.to_dict(),\n            interventions=interventions,\n        )\n\n        return all_cache\n\n    def compute_patching(\n        self,\n        target_token_positions: List[Union[str, int, Tuple[int, int]]],\n        # counterfactual_dataset,\n        base_dataloader,\n        target_dataloader,\n        patching_query=[\n            {\n                \"patching_elem\": \"@end-image\",\n                \"layers_to_patch\": [1, 2, 3, 4],\n                \"activation_type\": \"resid_in_{}\",\n            }\n        ],\n        base_dictonary_idxs: Optional[List[List[int]]] = None,\n        target_dictonary_idxs: Optional[List[List[int]]] = None,\n        return_logit_diff: bool = False,\n        batch_saver: Callable = lambda x: None,\n        **kwargs,\n    ) -&gt; ActivationCache:\n        r\"\"\"\n        Method for activation patching. This substitutes the activations of the model\n        with the activations of the counterfactual dataset.\n\n        It performs three forward passes:\n        1. Forward pass on the base dataset to extract the activations of the model (cat).\n        2. Forward pass on the target dataset to extract clean logits (dog)\n        [to compare against the patched logits].\n        3. Forward pass on the target dataset to patch (cat) into (dog)\n        and extract the patched logits.\n\n        Arguments:\n            - target_token_positions (Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]): List of tokens to extract the activations from. See TokenIndex.get_token_index for more details\n            - base_dataloader (torch.utils.data.DataLoader): Dataloader with the base dataset. (dataset where we sample the activations from)\n            - target_dataloader (torch.utils.data.DataLoader): Dataloader with the target dataset. (dataset where we patch the activations)\n            - patching_query (list[dict]): List of dictionaries with the patching queries. Each dictionary must have the keys \"patching_elem\", \"layers_to_patch\" and \"activation_type\". The \"patching_elem\" is the token to patch, the \"layers_to_patch\" is the list of layers to patch and the \"activation_type\" is the type of the activation to patch. The activation type must be one of the following: \"resid_in_{}\", \"resid_out_{}\", \"resid_mid_{}\", \"attn_in_{}\", \"attn_out_{}\", \"values_{}\". The \"{}\" will be replaced with the layer index.\n            - base_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n            - target_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n            - return_logit_diff (bool): If True, it will return the logit difference between the clean logits and the patched logits.\n\n\n        Returns:\n            final_cache (ActivationCache): dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n        Examples:\n            &gt;&gt;&gt; model.compute_patching(\n            &gt;&gt;&gt;     target_token_positions=[\"end-image\", \" last\"],\n            &gt;&gt;&gt;     base_dataloader=base_dataloader,\n            &gt;&gt;&gt;     target_dataloader=target_dataloader,\n            &gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,\n            &gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,\n            &gt;&gt;&gt;     patching_query=[\n            &gt;&gt;&gt;         {\n            &gt;&gt;&gt;             \"patching_elem\": \"@end-image\",\n            &gt;&gt;&gt;             \"layers_to_patch\": [1, 2, 3, 4],\n            &gt;&gt;&gt;             \"activation_type\": \"resid_in_{}\",\n            &gt;&gt;&gt;         }\n            &gt;&gt;&gt;     ],\n            &gt;&gt;&gt;     return_logit_diff=False,\n            &gt;&gt;&gt;     batch_saver=lambda x: None,\n            &gt;&gt;&gt; )\n            &gt;&gt;&gt; print(final_cache)\n            {\n                \"resid_out_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n                \"resid_mid_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n                ....\n                \"logit_diff_variation\": tensor of shape [batch] with the logit difference variation\n                \"logit_diff_in_clean\": tensor of shape [batch] with the logit difference in the clean logits\n                \"logit_diff_in_patched\": tensor of shape [batch] with the logit difference in the patched logits\n            }\n        \"\"\"\n        logger.debug(\"HookedModel: Computing patching\")\n\n        logger.debug(\"HookedModel: Forward pass started\")\n        logger.info(\n            f\"HookedModel: Patching elements: {[q['patching_elem'] for q in patching_query]} at {[query['activation_type'][:-3] for query in patching_query]}\"\n        )\n\n        # if target_token_positions is not a list, convert it to a list\n        if not isinstance(target_token_positions, list):\n            target_token_positions = [target_token_positions]\n\n        # get a random number in the range of the dataset to save a random batch\n        all_cache = ActivationCache()\n        # for each batch in the dataset\n        for index, (base_batch, target_batch) in progress(\n            enumerate(zip(base_dataloader, target_dataloader)),\n            desc=\"Computing patching on the dataset:\",\n            total=len(base_dataloader),\n        ):\n            torch.cuda.empty_cache()\n            inputs = self.input_handler.prepare_inputs(base_batch, self.first_device)\n\n            # set the right arguments for extract the patching activations\n            activ_type = [query[\"activation_type\"][:-3] for query in patching_query]\n\n            args = {\n                \"extract_resid_out\": True,\n                \"extract_resid_in\": False,\n                \"extract_resid_mid\": False,\n                \"extract_attn_in\": False,\n                \"extract_attn_out\": False,\n                \"extract_head_values\": False,\n                \"extract_head_out\": False,\n                \"extract_avg_attn_pattern\": False,\n                \"extract_avg_values_vectors_projected\": False,\n                \"extract_head_values_projected\": False,\n                \"extract_avg\": False,\n                \"ablation_queries\": None,\n                \"patching_queries\": None,\n                \"external_cache\": None,\n                \"attn_heads\": \"all\",\n                \"batch_idx\": None,\n                \"move_to_cpu\": False,\n            }\n\n            if \"resid_in\" in activ_type:\n                args[\"extract_resid_in\"] = True\n            if \"resid_out\" in activ_type:\n                args[\"extract_resid_out\"] = True\n            if \"resid_mid\" in activ_type:\n                args[\"extract_intermediate_states\"] = True\n            if \"attn_in\" in activ_type:\n                args[\"extract_attn_in\"] = True\n            if \"attn_out\" in activ_type:\n                args[\"extract_attn_out\"] = True\n            if \"values\" in activ_type:\n                args[\"extract_head_values\"] = True\n            # other cases\n\n            # first forward pass to extract the base activations\n            base_cache = self.forward(\n                inputs=inputs,\n                target_token_positions=target_token_positions,\n                pivot_positions=base_batch.get(\"pivot_positions\", None),\n                external_cache=args[\"external_cache\"],\n                batch_idx=args[\"batch_idx\"],\n                extraction_config=ExtractionConfig(**args),\n                interventions=args[\"interventions\"],\n                move_to_cpu=args[\"move_to_cpu\"],\n            )\n\n            # extract the target activations\n            target_inputs = self.input_handler.prepare_inputs(\n                target_batch, self.first_device\n            )\n\n            requested_position_to_extract = []\n            interventions = []\n            for query in patching_query:\n                if (\n                    query[\"patching_elem\"].split(\"@\")[1]\n                    not in requested_position_to_extract\n                ):\n                    requested_position_to_extract.append(\n                        query[\"patching_elem\"].split(\"@\")[1]\n                    )\n                interventions.extend(\n                    [\n                        Intervention(\n                            type=\"full\",\n                            activation=query[\"activation_type\"].format(layer),\n                            token_positions=[query[\"patching_elem\"].split(\"@\")[1]],\n                            patching_values=base_cache[\n                                query[\"activation_type\"].format(layer)\n                            ]\n                            .detach()\n                            .clone(),\n                        )\n                        for layer in query[\"layers_to_patch\"]\n                    ]\n                )\n\n                # query[\"patching_activations\"] = base_cache\n                #     )\n                # query[\"base_activation_index\"] = base_cache[\"mapping_index\"][\n                #     query[\"patching_elem\"].split(\"@\")[1]\n                # ]\n\n            # second forward pass to extract the clean logits\n            target_clean_cache = self.forward(\n                target_inputs,\n                target_token_positions=requested_position_to_extract,\n                pivot_positions=target_batch.get(\"pivot_positions\", None),\n                # move_to_cpu=True,\n            )\n\n            # merge requested_position_to_extract with extracted_token_positio\n            # third forward pass to patch the activations\n            target_patched_cache = self.forward(\n                target_inputs,\n                target_token_positions=list(\n                    set(target_token_positions + requested_position_to_extract)\n                ),\n                pivot_positions=target_batch.get(\"pivot_positions\", None),\n                patching_queries=patching_query,\n                **kwargs,\n            )\n\n            if return_logit_diff:\n                if base_dictonary_idxs is None or target_dictonary_idxs is None:\n                    raise ValueError(\n                        \"To compute the logit difference, you need to pass the base_dictonary_idxs and the target_dictonary_idxs\"\n                    )\n                logger.info(\"HookedModel: Computing logit difference\")\n                # get the target tokens (\" cat\" and \" dog\")\n                base_targets = base_dictonary_idxs[index]\n                target_targets = target_dictonary_idxs[index]\n\n                # compute the logit difference\n                result_diff = logit_diff(\n                    base_label_tokens=[s for s in base_targets],\n                    target_label_tokens=[c for c in target_targets],\n                    target_clean_logits=target_clean_cache[\"logits\"],\n                    target_patched_logits=target_patched_cache[\"logits\"],\n                )\n                target_patched_cache[\"logit_diff_variation\"] = result_diff[\n                    \"diff_variation\"\n                ]\n                target_patched_cache[\"logit_diff_in_clean\"] = result_diff[\n                    \"diff_in_clean\"\n                ]\n                target_patched_cache[\"logit_diff_in_patched\"] = result_diff[\n                    \"diff_in_patched\"\n                ]\n\n            # compute the KL divergence\n            result_kl = kl_divergence_diff(\n                base_logits=base_cache[\"logits\"],\n                target_clean_logits=target_clean_cache[\"logits\"],\n                target_patched_logits=target_patched_cache[\"logits\"],\n            )\n            for key, value in result_kl.items():\n                target_patched_cache[key] = value\n\n            target_patched_cache[\"base_logits\"] = base_cache[\"logits\"]\n            target_patched_cache[\"target_clean_logits\"] = target_clean_cache[\"logits\"]\n            # rename logits to target_patched_logits\n            target_patched_cache[\"target_patched_logits\"] = target_patched_cache[\n                \"logits\"\n            ]\n            del target_patched_cache[\"logits\"]\n\n            target_patched_cache.cpu()\n\n            # all_cache.append(target_patched_cache)\n            all_cache.cat(target_patched_cache)\n\n        logger.debug(\n            \"HookedModel: Forward pass finished - started to aggregate different batch\"\n        )\n        # final_cache = aggregate_cache_efficient(all_cache)\n\n        logger.debug(\"HookedModel: Aggregation finished\")\n        return all_cache\n\n    def _compute_input_gradients(self, cache, logits, vocabulary_index):\n        \"\"\"\n        Private method to compute gradients of logits with respect to input embeddings.\n\n        Args:\n            cache (ActivationCache): Cache containing logits and input_embeddings\n            logits (torch.Tensor): Model output logits\n            vocabulary_index (int): Index in the vocabulary for which to compute gradients\n\n        Returns:\n            bool: True if gradients were successfully computed, False otherwise\n        \"\"\"\n\n        supported_keys = [\"input_embeddings\"]\n\n        if any(key not in cache for key in supported_keys):\n            logger.warning(\n                f\"Cannot compute gradients: {supported_keys} not found in cache. \"\n                \"Ensure extraction_config.extract_embed is True.\"\n            )\n            return False\n\n        input_embeds = cache[\"input_embeddings\"]\n\n        if not input_embeds.requires_grad:\n            logger.warning(\n                \"Cannot compute gradients: input embeddings do not require gradients.\"\n            )\n            return False\n\n        # Select the specific logit for the target token\n        target_logits = logits[0, -1, vocabulary_index]\n\n        # Zero out existing gradients if any\n        if input_embeds.grad is not None:\n            input_embeds.grad.zero_()\n\n        # try:\n        # Backward pass - use retain_graph=False to free memory after each backward pass\n        target_logits.backward(retain_graph=False)\n\n        # Store the computed gradients before they're cleared\n        for key in supported_keys:\n            if key in cache and input_embeds.grad is not None:\n                cache[key + \"_gradients\"] = input_embeds.grad.detach().clone()\n\n        # Process token slicing\n        tupled_indexes = tuple(cache[\"token_dict\"].values())\n        flatten_indexes = [item for sublist in tupled_indexes for item in sublist]\n        for key in supported_keys:\n            cache[key] = cache[key][..., flatten_indexes, :].detach()\n            if key + \"_gradients\" in cache:\n                cache[key + \"_gradients\"] = cache[key + \"_gradients\"][\n                    ..., flatten_indexes, :\n                ].detach()\n\n        # Explicitly free memory\n        torch.cuda.empty_cache()\n        return True\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.__call__","title":"<code>__call__(*args, **kwds)</code>","text":"<p>Call the forward method of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def __call__(self, *args, **kwds) -&gt; ActivationCache:\n    r\"\"\"\n    Call the forward method of the model\n    \"\"\"\n    return self.forward(*args, **kwds)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.clean_interventions","title":"<code>clean_interventions()</code>","text":"<p>Remove all registered interventions.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def clean_interventions(self):\n    \"\"\"\n    Remove all registered interventions.\n    \"\"\"\n    self.additional_interventions = []\n    logger.debug(\n        f\"HookedModel: Removed {len(self.additional_interventions)} interventions\"\n    )\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.compute_patching","title":"<code>compute_patching(target_token_positions, base_dataloader, target_dataloader, patching_query=[{'patching_elem': '@end-image', 'layers_to_patch': [1, 2, 3, 4], 'activation_type': 'resid_in_{}'}], base_dictonary_idxs=None, target_dictonary_idxs=None, return_logit_diff=False, batch_saver=lambda x: None, **kwargs)</code>","text":"<p>Method for activation patching. This substitutes the activations of the model with the activations of the counterfactual dataset.</p> <p>It performs three forward passes: 1. Forward pass on the base dataset to extract the activations of the model (cat). 2. Forward pass on the target dataset to extract clean logits (dog) [to compare against the patched logits]. 3. Forward pass on the target dataset to patch (cat) into (dog) and extract the patched logits.</p> <p>Parameters:</p> Name Type Description Default <code>- target_token_positions</code> <code>Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]</code> <p>List of tokens to extract the activations from. See TokenIndex.get_token_index for more details</p> required <code>- base_dataloader</code> <code>DataLoader</code> <p>Dataloader with the base dataset. (dataset where we sample the activations from)</p> required <code>- target_dataloader</code> <code>DataLoader</code> <p>Dataloader with the target dataset. (dataset where we patch the activations)</p> required <code>- patching_query</code> <code>list[dict]</code> <p>List of dictionaries with the patching queries. Each dictionary must have the keys \"patching_elem\", \"layers_to_patch\" and \"activation_type\". The \"patching_elem\" is the token to patch, the \"layers_to_patch\" is the list of layers to patch and the \"activation_type\" is the type of the activation to patch. The activation type must be one of the following: \"resid_in_{}\", \"resid_out_{}\", \"resid_mid_{}\", \"attn_in_{}\", \"attn_out_{}\", \"values_{}\". The \"{}\" will be replaced with the layer index.</p> required <code>- base_dictonary_idxs</code> <code>list[list[int]]</code> <p>List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.</p> required <code>- target_dictonary_idxs</code> <code>list[list[int]]</code> <p>List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.</p> required <code>- return_logit_diff</code> <code>bool</code> <p>If True, it will return the logit difference between the clean logits and the patched logits.</p> required <p>Returns:</p> Name Type Description <code>final_cache</code> <code>ActivationCache</code> <p>dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model.compute_patching(\n&gt;&gt;&gt;     target_token_positions=[\"end-image\", \" last\"],\n&gt;&gt;&gt;     base_dataloader=base_dataloader,\n&gt;&gt;&gt;     target_dataloader=target_dataloader,\n&gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,\n&gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,\n&gt;&gt;&gt;     patching_query=[\n&gt;&gt;&gt;         {\n&gt;&gt;&gt;             \"patching_elem\": \"@end-image\",\n&gt;&gt;&gt;             \"layers_to_patch\": [1, 2, 3, 4],\n&gt;&gt;&gt;             \"activation_type\": \"resid_in_{}\",\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;     ],\n&gt;&gt;&gt;     return_logit_diff=False,\n&gt;&gt;&gt;     batch_saver=lambda x: None,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(final_cache)\n{\n    \"resid_out_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n    \"resid_mid_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n    ....\n    \"logit_diff_variation\": tensor of shape [batch] with the logit difference variation\n    \"logit_diff_in_clean\": tensor of shape [batch] with the logit difference in the clean logits\n    \"logit_diff_in_patched\": tensor of shape [batch] with the logit difference in the patched logits\n}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def compute_patching(\n    self,\n    target_token_positions: List[Union[str, int, Tuple[int, int]]],\n    # counterfactual_dataset,\n    base_dataloader,\n    target_dataloader,\n    patching_query=[\n        {\n            \"patching_elem\": \"@end-image\",\n            \"layers_to_patch\": [1, 2, 3, 4],\n            \"activation_type\": \"resid_in_{}\",\n        }\n    ],\n    base_dictonary_idxs: Optional[List[List[int]]] = None,\n    target_dictonary_idxs: Optional[List[List[int]]] = None,\n    return_logit_diff: bool = False,\n    batch_saver: Callable = lambda x: None,\n    **kwargs,\n) -&gt; ActivationCache:\n    r\"\"\"\n    Method for activation patching. This substitutes the activations of the model\n    with the activations of the counterfactual dataset.\n\n    It performs three forward passes:\n    1. Forward pass on the base dataset to extract the activations of the model (cat).\n    2. Forward pass on the target dataset to extract clean logits (dog)\n    [to compare against the patched logits].\n    3. Forward pass on the target dataset to patch (cat) into (dog)\n    and extract the patched logits.\n\n    Arguments:\n        - target_token_positions (Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]): List of tokens to extract the activations from. See TokenIndex.get_token_index for more details\n        - base_dataloader (torch.utils.data.DataLoader): Dataloader with the base dataset. (dataset where we sample the activations from)\n        - target_dataloader (torch.utils.data.DataLoader): Dataloader with the target dataset. (dataset where we patch the activations)\n        - patching_query (list[dict]): List of dictionaries with the patching queries. Each dictionary must have the keys \"patching_elem\", \"layers_to_patch\" and \"activation_type\". The \"patching_elem\" is the token to patch, the \"layers_to_patch\" is the list of layers to patch and the \"activation_type\" is the type of the activation to patch. The activation type must be one of the following: \"resid_in_{}\", \"resid_out_{}\", \"resid_mid_{}\", \"attn_in_{}\", \"attn_out_{}\", \"values_{}\". The \"{}\" will be replaced with the layer index.\n        - base_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n        - target_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n        - return_logit_diff (bool): If True, it will return the logit difference between the clean logits and the patched logits.\n\n\n    Returns:\n        final_cache (ActivationCache): dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n    Examples:\n        &gt;&gt;&gt; model.compute_patching(\n        &gt;&gt;&gt;     target_token_positions=[\"end-image\", \" last\"],\n        &gt;&gt;&gt;     base_dataloader=base_dataloader,\n        &gt;&gt;&gt;     target_dataloader=target_dataloader,\n        &gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,\n        &gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,\n        &gt;&gt;&gt;     patching_query=[\n        &gt;&gt;&gt;         {\n        &gt;&gt;&gt;             \"patching_elem\": \"@end-image\",\n        &gt;&gt;&gt;             \"layers_to_patch\": [1, 2, 3, 4],\n        &gt;&gt;&gt;             \"activation_type\": \"resid_in_{}\",\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;     ],\n        &gt;&gt;&gt;     return_logit_diff=False,\n        &gt;&gt;&gt;     batch_saver=lambda x: None,\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(final_cache)\n        {\n            \"resid_out_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n            \"resid_mid_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n            ....\n            \"logit_diff_variation\": tensor of shape [batch] with the logit difference variation\n            \"logit_diff_in_clean\": tensor of shape [batch] with the logit difference in the clean logits\n            \"logit_diff_in_patched\": tensor of shape [batch] with the logit difference in the patched logits\n        }\n    \"\"\"\n    logger.debug(\"HookedModel: Computing patching\")\n\n    logger.debug(\"HookedModel: Forward pass started\")\n    logger.info(\n        f\"HookedModel: Patching elements: {[q['patching_elem'] for q in patching_query]} at {[query['activation_type'][:-3] for query in patching_query]}\"\n    )\n\n    # if target_token_positions is not a list, convert it to a list\n    if not isinstance(target_token_positions, list):\n        target_token_positions = [target_token_positions]\n\n    # get a random number in the range of the dataset to save a random batch\n    all_cache = ActivationCache()\n    # for each batch in the dataset\n    for index, (base_batch, target_batch) in progress(\n        enumerate(zip(base_dataloader, target_dataloader)),\n        desc=\"Computing patching on the dataset:\",\n        total=len(base_dataloader),\n    ):\n        torch.cuda.empty_cache()\n        inputs = self.input_handler.prepare_inputs(base_batch, self.first_device)\n\n        # set the right arguments for extract the patching activations\n        activ_type = [query[\"activation_type\"][:-3] for query in patching_query]\n\n        args = {\n            \"extract_resid_out\": True,\n            \"extract_resid_in\": False,\n            \"extract_resid_mid\": False,\n            \"extract_attn_in\": False,\n            \"extract_attn_out\": False,\n            \"extract_head_values\": False,\n            \"extract_head_out\": False,\n            \"extract_avg_attn_pattern\": False,\n            \"extract_avg_values_vectors_projected\": False,\n            \"extract_head_values_projected\": False,\n            \"extract_avg\": False,\n            \"ablation_queries\": None,\n            \"patching_queries\": None,\n            \"external_cache\": None,\n            \"attn_heads\": \"all\",\n            \"batch_idx\": None,\n            \"move_to_cpu\": False,\n        }\n\n        if \"resid_in\" in activ_type:\n            args[\"extract_resid_in\"] = True\n        if \"resid_out\" in activ_type:\n            args[\"extract_resid_out\"] = True\n        if \"resid_mid\" in activ_type:\n            args[\"extract_intermediate_states\"] = True\n        if \"attn_in\" in activ_type:\n            args[\"extract_attn_in\"] = True\n        if \"attn_out\" in activ_type:\n            args[\"extract_attn_out\"] = True\n        if \"values\" in activ_type:\n            args[\"extract_head_values\"] = True\n        # other cases\n\n        # first forward pass to extract the base activations\n        base_cache = self.forward(\n            inputs=inputs,\n            target_token_positions=target_token_positions,\n            pivot_positions=base_batch.get(\"pivot_positions\", None),\n            external_cache=args[\"external_cache\"],\n            batch_idx=args[\"batch_idx\"],\n            extraction_config=ExtractionConfig(**args),\n            interventions=args[\"interventions\"],\n            move_to_cpu=args[\"move_to_cpu\"],\n        )\n\n        # extract the target activations\n        target_inputs = self.input_handler.prepare_inputs(\n            target_batch, self.first_device\n        )\n\n        requested_position_to_extract = []\n        interventions = []\n        for query in patching_query:\n            if (\n                query[\"patching_elem\"].split(\"@\")[1]\n                not in requested_position_to_extract\n            ):\n                requested_position_to_extract.append(\n                    query[\"patching_elem\"].split(\"@\")[1]\n                )\n            interventions.extend(\n                [\n                    Intervention(\n                        type=\"full\",\n                        activation=query[\"activation_type\"].format(layer),\n                        token_positions=[query[\"patching_elem\"].split(\"@\")[1]],\n                        patching_values=base_cache[\n                            query[\"activation_type\"].format(layer)\n                        ]\n                        .detach()\n                        .clone(),\n                    )\n                    for layer in query[\"layers_to_patch\"]\n                ]\n            )\n\n            # query[\"patching_activations\"] = base_cache\n            #     )\n            # query[\"base_activation_index\"] = base_cache[\"mapping_index\"][\n            #     query[\"patching_elem\"].split(\"@\")[1]\n            # ]\n\n        # second forward pass to extract the clean logits\n        target_clean_cache = self.forward(\n            target_inputs,\n            target_token_positions=requested_position_to_extract,\n            pivot_positions=target_batch.get(\"pivot_positions\", None),\n            # move_to_cpu=True,\n        )\n\n        # merge requested_position_to_extract with extracted_token_positio\n        # third forward pass to patch the activations\n        target_patched_cache = self.forward(\n            target_inputs,\n            target_token_positions=list(\n                set(target_token_positions + requested_position_to_extract)\n            ),\n            pivot_positions=target_batch.get(\"pivot_positions\", None),\n            patching_queries=patching_query,\n            **kwargs,\n        )\n\n        if return_logit_diff:\n            if base_dictonary_idxs is None or target_dictonary_idxs is None:\n                raise ValueError(\n                    \"To compute the logit difference, you need to pass the base_dictonary_idxs and the target_dictonary_idxs\"\n                )\n            logger.info(\"HookedModel: Computing logit difference\")\n            # get the target tokens (\" cat\" and \" dog\")\n            base_targets = base_dictonary_idxs[index]\n            target_targets = target_dictonary_idxs[index]\n\n            # compute the logit difference\n            result_diff = logit_diff(\n                base_label_tokens=[s for s in base_targets],\n                target_label_tokens=[c for c in target_targets],\n                target_clean_logits=target_clean_cache[\"logits\"],\n                target_patched_logits=target_patched_cache[\"logits\"],\n            )\n            target_patched_cache[\"logit_diff_variation\"] = result_diff[\n                \"diff_variation\"\n            ]\n            target_patched_cache[\"logit_diff_in_clean\"] = result_diff[\n                \"diff_in_clean\"\n            ]\n            target_patched_cache[\"logit_diff_in_patched\"] = result_diff[\n                \"diff_in_patched\"\n            ]\n\n        # compute the KL divergence\n        result_kl = kl_divergence_diff(\n            base_logits=base_cache[\"logits\"],\n            target_clean_logits=target_clean_cache[\"logits\"],\n            target_patched_logits=target_patched_cache[\"logits\"],\n        )\n        for key, value in result_kl.items():\n            target_patched_cache[key] = value\n\n        target_patched_cache[\"base_logits\"] = base_cache[\"logits\"]\n        target_patched_cache[\"target_clean_logits\"] = target_clean_cache[\"logits\"]\n        # rename logits to target_patched_logits\n        target_patched_cache[\"target_patched_logits\"] = target_patched_cache[\n            \"logits\"\n        ]\n        del target_patched_cache[\"logits\"]\n\n        target_patched_cache.cpu()\n\n        # all_cache.append(target_patched_cache)\n        all_cache.cat(target_patched_cache)\n\n    logger.debug(\n        \"HookedModel: Forward pass finished - started to aggregate different batch\"\n    )\n    # final_cache = aggregate_cache_efficient(all_cache)\n\n    logger.debug(\"HookedModel: Aggregation finished\")\n    return all_cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.create_hooks","title":"<code>create_hooks(inputs, cache, token_indexes, token_dict, extraction_config=ExtractionConfig(), interventions=None, batch_idx=None, external_cache=None)</code>","text":"<p>Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</p> required <code>cache</code> <code>ActivationCache</code> <p>dictionary where the activations of the model will be saved</p> required <code>token_indexes</code> <code>list[str]</code> <p>list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])</p> required <code>token_dict</code> <code>Dict</code> <p>dictionary with the token indexes</p> required <code>extraction_config</code> <code>ExtractionConfig</code> <p>configuration of the extraction of the activations of the model (default = ExtractionConfig())</p> <code>ExtractionConfig()</code> <code>interventions</code> <code>Optional[List[Intervention]]</code> <p>list of interventions to perform during forward pass</p> <code>None</code> <code>batch_idx</code> <code>Optional[int]</code> <p>index of the batch in the dataloader</p> <code>None</code> <code>external_cache</code> <code>Optional[ActivationCache]</code> <p>external cache to use in the forward pass</p> <code>None</code> <p>Returns:</p> Name Type Description <code>hooks</code> <code>list[dict]</code> <p>list of dictionaries with the component and the intervention to perform in the forward pass of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def create_hooks(\n    self,\n    inputs,\n    cache: ActivationCache,\n    token_indexes: List,\n    token_dict: Dict,\n    # string_tokens: List[str],\n    extraction_config: ExtractionConfig = ExtractionConfig(),\n    interventions: Optional[List[Intervention]] = None,\n    batch_idx: Optional[int] = None,\n    external_cache: Optional[ActivationCache] = None,\n):\n    r\"\"\"\n    Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.\n\n    Arguments:\n        inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n        cache (ActivationCache): dictionary where the activations of the model will be saved\n        token_indexes (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n        token_dict (Dict): dictionary with the token indexes\n        extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model (default = ExtractionConfig())\n        interventions (Optional[List[Intervention]]): list of interventions to perform during forward pass\n        batch_idx (Optional[int]): index of the batch in the dataloader\n        external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n\n    Returns:\n        hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n    \"\"\"\n    hooks = []\n\n    # compute layer and head indexes\n    if (\n        isinstance(extraction_config.attn_heads, str)\n        and extraction_config.attn_heads == \"all\"\n    ):\n        layer_indexes = [i for i in range(0, self.model_config.num_hidden_layers)]\n        head_indexes = [\"all\"] * len(layer_indexes)\n    elif isinstance(extraction_config.attn_heads, list):\n        layer_head_indexes = [\n            (el[\"layer\"], el[\"head\"]) for el in extraction_config.attn_heads\n        ]\n        layer_indexes = [el[0] for el in layer_head_indexes]\n        head_indexes = [el[1] for el in layer_head_indexes]\n    else:\n        raise ValueError(\n            \"attn_heads must be 'all' or a list of dictionaries as [{'layer': 0, 'head': 0}]\"\n        )\n    # register the intervention hooks as first thing to do\n    if self.additional_interventions is not None:\n        hooks += self.intervention_manager.create_intervention_hooks(\n            interventions=self.additional_interventions, token_dict=token_dict\n        )\n\n    if extraction_config.extract_resid_out:\n        # assert that the component exists in the model\n        hooks += [\n            {\n                \"component\": self.model_config.residual_stream_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_out_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n    if extraction_config.extract_resid_in:\n        # assert that the component exists in the model\n        hooks += [\n            {\n                \"component\": self.model_config.residual_stream_input_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_in_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_resid_in_post_layernorm:\n        hooks += [\n            {\n                \"component\": self.model_config.residual_stream_input_post_layernorm_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_in_post_layernorm_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.save_input_ids:\n        hooks += [\n            {\n                \"component\": self.model_config.embed_tokens,\n                \"intervention\": partial(\n                    embed_hook,\n                    token_indexes=token_indexes,\n                    cache=cache,\n                    cache_key=\"input_ids\",\n                ),\n            }\n        ]\n\n    if extraction_config.extract_embed:  # New block\n        hooks += [\n            {\n                \"component\": self.model_config.embed_tokens,  # Use the embedding module name directly\n                \"intervention\": partial(\n                    input_embedding_hook,\n                    cache=cache,\n                    cache_key=\"input_embeddings\",\n                    token_indexes=token_indexes,\n                    keep_gradient=extraction_config.keep_gradient,\n                    avg=extraction_config.avg,\n                ),\n            }\n        ]\n\n    if extraction_config.extract_head_queries:\n        hooks += [\n            {\n                \"component\": self.model_config.head_query_hook_name.format(i),\n                \"intervention\": partial(\n                    query_key_value_hook,\n                    cache=cache,\n                    cache_key=\"head_queries_\",\n                    token_indexes=token_indexes,\n                    head_dim=self.model_config.head_dim,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_values:\n        hooks += [\n            {\n                \"component\": self.model_config.head_value_hook_name.format(i),\n                \"intervention\": partial(\n                    query_key_value_hook,\n                    cache=cache,\n                    cache_key=\"head_values_\",\n                    token_indexes=token_indexes,\n                    head_dim=self.model_config.head_dim,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_keys:\n        hooks += [\n            {\n                \"component\": self.model_config.head_key_hook_name.format(i),\n                \"intervention\": partial(\n                    query_key_value_hook,\n                    cache=cache,\n                    cache_key=\"head_keys_\",\n                    token_indexes=token_indexes,\n                    head_dim=self.model_config.head_dim,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_values:\n        hooks += [\n            {\n                \"component\": self.model_config.head_value_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"values_L{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_keys:\n        hooks += [\n            {\n                \"component\": self.model_config.head_key_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"keys_L{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_queries:\n        hooks += [\n            {\n                \"component\": self.model_config.head_query_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"queries_L{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_head_out:\n        hooks += [\n            {\n                \"component\": self.model_config.attn_o_proj_input_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    head_out_hook,\n                    cache=cache,\n                    cache_key=\"head_out_\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_heads=self.model_config.num_attention_heads,\n                    head_dim=self.model_config.head_dim,\n                    o_proj_weight=get_attribute_from_name(\n                        self.hf_model,\n                        self.model_config.attn_out_proj_weight.format(i),\n                    ),\n                    o_proj_bias=get_attribute_from_name(\n                        self.hf_model,\n                        self.model_config.attn_out_proj_bias.format(i),\n                    ),\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_attn_in:\n        hooks += [\n            {\n                \"component\": self.model_config.attn_in_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"attn_in_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_attn_out:\n        hooks += [\n            {\n                \"component\": self.model_config.attn_out_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"attn_out_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    # if extraction_config.extract_avg:\n    #     # Define a hook that saves the activations of the residual stream\n    #     raise NotImplementedError(\n    #         \"The hook for the average is not working with token_index as a list\"\n    #     )\n\n    #     # hooks.extend(\n    #     #     [\n    #     #         {\n    #     #             \"component\": self.model_config.residual_stream_hook_name.format(\n    #     #                 i\n    #     #             ),\n    #     #             \"intervention\": partial(\n    #     #                 avg_hook,\n    #     #                 cache=cache,\n    #     #                 cache_key=\"resid_avg_{}\".format(i),\n    #     #                 last_image_idx=last_image_idxs, #type\n    #     #                 end_image_idx=end_image_idxs,\n    #     #             ),\n    #     #         }\n    #     #         for i in range(0, self.model_config.num_hidden_layers)\n    #     #     ]\n    #     # )\n    if extraction_config.extract_resid_mid:\n        hooks += [\n            {\n                \"component\": self.model_config.intermediate_stream_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_mid_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n        # if we want to extract the output of the heads\n    if extraction_config.extract_mlp_out:\n        hooks += [\n            {\n                \"component\": self.model_config.mlp_out_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"mlp_out_{i}\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_last_layernorm:\n        hooks += [\n            {\n                \"component\": self.model_config.last_layernorm_hook_name,\n                \"intervention\": partial(\n                    layernom_hook,\n                    cache=cache,\n                    cache_key=\"last_layernorm\",\n                    token_indexes=token_indexes,\n                    avg=extraction_config.avg,\n                ),\n            }\n        ]\n    # ABLATION AND PATCHING\n    if interventions is not None:\n        hooks += self.intervention_manager.create_intervention_hooks(\n            interventions=interventions, token_dict=token_dict\n        )\n    if extraction_config.extract_head_values_projected:\n        hooks += [\n            {\n                \"component\": self.model_config.head_value_hook_name.format(i),\n                \"intervention\": partial(\n                    projected_value_vectors_head,\n                    cache=cache,\n                    token_indexes=token_indexes,\n                    layer=i,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                    num_key_value_heads=self.model_config.num_key_value_heads,\n                    hidden_size=self.model_config.hidden_size,\n                    d_head=self.model_config.head_dim,\n                    out_proj_weight=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                    ),\n                    out_proj_bias=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                    ),\n                    head=head,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_keys_projected:\n        hooks += [\n            {\n                \"component\": self.model_config.head_key_hook_name.format(i),\n                \"intervention\": partial(\n                    projected_key_vectors_head,\n                    cache=cache,\n                    token_indexes=token_indexes,\n                    layer=i,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                    num_key_value_heads=self.model_config.num_key_value_heads,\n                    hidden_size=self.model_config.hidden_size,\n                    d_head=self.model_config.head_dim,\n                    out_proj_weight=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                    ),\n                    out_proj_bias=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                    ),\n                    head=head,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_queries_projected:\n        hooks += [\n            {\n                \"component\": self.model_config.head_query_hook_name.format(i),\n                \"intervention\": partial(\n                    projected_query_vectors_head,\n                    cache=cache,\n                    token_indexes=token_indexes,\n                    layer=i,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                    num_key_value_heads=self.model_config.num_key_value_heads,\n                    hidden_size=self.model_config.hidden_size,\n                    d_head=self.model_config.head_dim,\n                    out_proj_weight=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                    ),\n                    out_proj_bias=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                    ),\n                    head=head,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_attn_pattern:\n        if extraction_config.avg_over_example:\n            if external_cache is None:\n                logger.warning(\n                    \"\"\"The external_cache is None. The average could not be computed since missing an external cache where store the iterations.\n                    \"\"\"\n                )\n            elif batch_idx is None:\n                logger.warning(\n                    \"\"\"The batch_idx is None. The average could not be computed since missing the batch index.\n\n                    \"\"\"\n                )\n            else:\n                # move the cache to the same device of the model\n                external_cache.to(self.first_device)\n                hooks += [\n                    {\n                        \"component\": self.model_config.attn_matrix_hook_name.format(\n                            i\n                        ),\n                        \"intervention\": partial(\n                            avg_attention_pattern_head,\n                            token_indexes=token_indexes,\n                            layer=i,\n                            attn_pattern_current_avg=external_cache,\n                            batch_idx=batch_idx,\n                            cache=cache,\n                            # avg=extraction_config.avg,\n                            extract_avg_value=extraction_config.extract_head_values_projected,\n                        ),\n                    }\n                    for i in range(0, self.model_config.num_hidden_layers)\n                ]\n        else:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_matrix_hook_name.format(i),\n                    \"intervention\": partial(\n                        attention_pattern_head,\n                        token_indexes=token_indexes,\n                        cache=cache,\n                        layer=i,\n                        head=head,\n                        attn_pattern_avg=extraction_config.attn_pattern_avg,\n                        attn_pattern_row_partition=None\n                        if extraction_config.attn_pattern_row_positions is None\n                        else tuple(token_dict[\"attn_pattern_row_positions\"]),\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        # if additional hooks are not empty, add them to the hooks list\n    if self.additional_hooks:\n        for hook in self.additional_hooks:\n            hook[\"intervention\"] = partial(\n                hook[\"intervention\"],\n                cache=cache,\n                token_indexes=token_indexes,\n                token_dict=token_dict,\n                **hook[\"intervention\"],\n            )\n            hooks.append(hook)\n    return hooks\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.device","title":"<code>device()</code>","text":"<p>Return the device (e.g., 'cuda', 'cpu') where the model is located.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def device(self):\n    \"\"\"\n    Return the device (e.g., 'cuda', 'cpu') where the model is located.\n    \"\"\"\n    return self.first_device\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.eval","title":"<code>eval()</code>","text":"<p>Set the model to evaluation mode.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def eval(self):\n    \"\"\"\n    Set the model to evaluation mode.\n    \"\"\"\n    self.hf_model.eval()\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.extract_cache","title":"<code>extract_cache(dataloader, target_token_positions, extraction_config=ExtractionConfig(), interventions=None, batch_saver=lambda x: None, move_to_cpu_after_forward=True, **kwargs)</code>","text":"<p>Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>- dataloader</code> <code>iterable</code> <p>dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)</p> required <code>- extracted_token_position</code> <code>Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]</code> <p>list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\", -1, (2,10)]). See TokenIndex.get_token_index for more details</p> required <code>- batch_saver</code> <code>Callable</code> <p>function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)</p> required <code>- move_to_cpu_after_forward</code> <code>bool</code> <p>if True, move the activations to the cpu right after the any forward pass of the model</p> required <code>- dict_token_index</code> <code>Optional[Tensor]</code> <p>If provided, specifies the index in the vocabulary for which to compute gradients of logits with respect to input embeddings. Requires extraction_config.extract_input_embeddings_for_grad to be True.</p> required <code>- **kwargs</code> <p>additional arguments to control hooks generation, basically accept any argument handled by the <code>.forward</code> method (i.e. ablation_queries, patching_queries, extract_resid_in)</p> required <p>Returns:</p> Name Type Description <code>final_cache</code> <p>dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataloader = [{\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]]), \"labels\": torch.tensor([1])}, ...]\n&gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[\"last\"], batch_saver=lambda x: {\"labels\": x[\"labels\"]})\n{'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'labels': tensor([1]), 'mapping_index': {'last': [0]}}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def extract_cache(\n    self,\n    dataloader,\n    target_token_positions: Union[\n        List[Union[str, int, Tuple[int, int]]],\n        List[str],\n        List[int],\n        List[Tuple[int, int]],\n    ],\n    extraction_config: ExtractionConfig = ExtractionConfig(),\n    interventions: Optional[List[Intervention]] = None,\n    batch_saver: Callable = lambda x: None,\n    move_to_cpu_after_forward: bool = True,\n    # save_other_batch_elements: bool = False,\n    **kwargs,\n):\n    r\"\"\"\n    Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.\n\n    Arguments:\n        - dataloader (iterable): dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)\n        - extracted_token_position (Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\", -1, (2,10)]). See TokenIndex.get_token_index for more details\n        - batch_saver (Callable): function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)\n        - move_to_cpu_after_forward (bool): if True, move the activations to the cpu right after the any forward pass of the model\n        - dict_token_index (Optional[torch.Tensor]): If provided, specifies the index in the vocabulary for which to compute gradients of logits with respect to input embeddings. Requires extraction_config.extract_input_embeddings_for_grad to be True.\n        - **kwargs: additional arguments to control hooks generation, basically accept any argument handled by the `.forward` method (i.e. ablation_queries, patching_queries, extract_resid_in)\n\n    Returns:\n        final_cache: dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n    Examples:\n        &gt;&gt;&gt; dataloader = [{\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]]), \"labels\": torch.tensor([1])}, ...]\n        &gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[\"last\"], batch_saver=lambda x: {\"labels\": x[\"labels\"]})\n        {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'labels': tensor([1]), 'mapping_index': {'last': [0]}}\n    \"\"\"\n\n    logger.info(\"HookedModel: Extracting cache\")\n\n    # get the function to save in the cache the additional element from the batch sime\n\n    logger.info(\"HookedModel: Forward pass started\")\n    all_cache = ActivationCache()  # a list of dictoionaries, each dictionary contains the activations of the model for a batch (so a dict of tensors)\n    attn_pattern = (\n        ActivationCache()\n    )  # Initialize the dictionary to hold running averages\n\n    # if register_agregation is in the kwargs, we will register the aggregation of the attention pattern\n    if \"register_aggregation\" in kwargs:\n        all_cache.register_aggregation(\n            kwargs[\"register_aggregation\"][0], kwargs[\"register_aggregation\"][1]\n        )\n        attn_pattern.register_aggregation(\n            kwargs[\"register_aggregation\"][0], kwargs[\"register_aggregation\"][1]\n        )\n\n    # example_dict = {}\n    n_batches = 0  # Initialize batch counter\n\n    for batch in progress(\n        dataloader, desc=\"Extracting cache\", total=len(dataloader)\n    ):\n        # log_memory_usage(\"Extract cache - Before batch\")\n        # tokens, others = batch\n        # inputs = {k: v.to(self.first_device) for k, v in tokens.items()}\n\n        # get input_ids, attention_mask, and if available, pixel_values from batch (that is a dictionary)\n        # then move them to the first device\n\n        inputs = self.input_handler.prepare_inputs(\n            batch, self.first_device\n        )  # require_grads is False, gradients handled by hook if needed\n        others = {k: v for k, v in batch.items() if k not in inputs}\n\n        cache = self.forward(\n            inputs,\n            target_token_positions=target_token_positions,\n            pivot_positions=batch.get(\"pivot_positions\", None),\n            external_cache=attn_pattern,\n            batch_idx=n_batches,\n            extraction_config=extraction_config,\n            interventions=interventions,\n            vocabulary_index=batch.get(\"vocabulary_index\", None),\n            **kwargs,\n        )\n\n        # Compute input gradients if requested\n\n        # possible memory leak from here -___---------------&gt;\n        additional_dict = batch_saver(\n            others\n        )  # TODO: Maybe keep the batch_saver in a different cache\n        if additional_dict is not None:\n            # cache = {**cache, **additional_dict}if a\n            cache.update(additional_dict)\n\n        if move_to_cpu_after_forward:\n            cache.cpu()\n\n        n_batches += 1  # Increment batch counter# Process and remove \"pattern_\" keys from cache\n        all_cache.cat(cache)\n\n        del cache\n\n        # Use the new cleanup_tensors method from InputHandler to free memory\n        self.input_handler.cleanup_tensors(inputs, others)\n\n        torch.cuda.empty_cache()\n\n    logger.debug(\"Forward pass finished - started to aggregate different batch\")\n    all_cache.update(attn_pattern)\n    # all_cache[\"example_dict\"] = example_dict\n    # logger.info(\"HookedModel: Aggregation finished\")\n\n    torch.cuda.empty_cache()\n\n    # add a metadata field to the cache\n    all_cache.add_metadata(\n        target_token_positions=target_token_positions,\n        model_name=self.config.model_name,\n        extraction_config=extraction_config.to_dict(),\n        interventions=interventions,\n    )\n\n    return all_cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.forward","title":"<code>forward(inputs, target_token_positions=['all'], pivot_positions=None, extraction_config=ExtractionConfig(), interventions=None, external_cache=None, batch_idx=None, move_to_cpu=False, vocabulary_index=None, **kwargs)</code>","text":"<p>Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</p> required <code>target_token_positions</code> <code>Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]</code> <p>tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\", -1, (2,10)]). See TokenIndex.get_token_index for more details</p> <code>['all']</code> <code>pivot_positions</code> <code>Optional[list[int]]</code> <p>list of split positions of the tokens</p> <code>None</code> <code>extraction_config</code> <code>ExtractionConfig</code> <p>configuration of the extraction of the activations of the model</p> <code>ExtractionConfig()</code> <code>ablation_queries</code> <code>Optional[DataFrame | None]</code> <p>dataframe with the ablation queries to perform during forward pass</p> required <code>patching_queries</code> <code>Optional[DataFrame | None]</code> <p>dataframe with the patching queries to perform during forward pass</p> required <code>external_cache</code> <code>Optional[ActivationCache]</code> <p>external cache to use in the forward pass</p> <code>None</code> <code>attn_heads</code> <code>Union[list[dict], Literal['all']]</code> <p>list of dictionaries with the layer and head to extract the attention pattern or 'all' to</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>index of the batch in the dataloader</p> <code>None</code> <code>move_to_cpu</code> <code>bool</code> <p>if True, move the activations to the cpu</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cache</code> <code>ActivationCache</code> <p>dictionary with the activations of the model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n&gt;&gt;&gt; model.forward(inputs, target_token_positions=[\"last\"], extract_resid_out=True)\n{'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'input_ids': tensor([[101, 1234, 1235, 102]]), 'mapping_index': {'last': [0]}}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@conditional_no_grad()\n# @torch.no_grad()\ndef forward(\n    self,\n    inputs,\n    target_token_positions: Union[\n        List[Union[str, int, Tuple[int, int]]],\n        List[str],\n        List[int],\n        List[Tuple[int, int]],\n    ] = [\"all\"],\n    pivot_positions: Optional[List[int]] = None,\n    extraction_config: ExtractionConfig = ExtractionConfig(),\n    interventions: Optional[List[Intervention]] = None,\n    external_cache: Optional[ActivationCache] = None,\n    # attn_heads: Union[list[dict], Literal[\"all\"]] = \"all\",\n    batch_idx: Optional[int] = None,\n    move_to_cpu: bool = False,\n    vocabulary_index: Optional[int] = None,\n    **kwargs,\n) -&gt; ActivationCache:\n    r\"\"\"\n    Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.\n\n    Args:\n        inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n        target_token_positions (Union[Union[str, int, Tuple[int, int]], List[Union[str, int, Tuple[int, int]]]]): tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\", -1, (2,10)]). See TokenIndex.get_token_index for more details\n        pivot_positions (Optional[list[int]]): list of split positions of the tokens\n        extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model\n        ablation_queries (Optional[pd.DataFrame | None]): dataframe with the ablation queries to perform during forward pass\n        patching_queries (Optional[pd.DataFrame | None]): dataframe with the patching queries to perform during forward pass\n        external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n        attn_heads (Union[list[dict], Literal[\"all\"]]): list of dictionaries with the layer and head to extract the attention pattern or 'all' to\n        batch_idx (Optional[int]): index of the batch in the dataloader\n        move_to_cpu (bool): if True, move the activations to the cpu\n\n    Returns:\n        cache (ActivationCache): dictionary with the activations of the model\n\n    Examples:\n        &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n        &gt;&gt;&gt; model.forward(inputs, target_token_positions=[\"last\"], extract_resid_out=True)\n        {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'input_ids': tensor([[101, 1234, 1235, 102]]), 'mapping_index': {'last': [0]}}\n    \"\"\"\n\n    if target_token_positions is None and extraction_config.is_not_empty():\n        raise ValueError(\n            \"target_token_positions must be passed if we want to extract the activations of the model\"\n        )\n\n    cache = ActivationCache()\n    string_tokens = self.to_string_tokens(\n        self.input_handler.get_input_ids(inputs).squeeze()\n    )\n    token_index_finder = TokenIndex(\n        self.config.model_name, pivot_positions=pivot_positions\n    )\n    token_indexes, token_dict = token_index_finder.get_token_index(\n        tokens=target_token_positions,\n        string_tokens=string_tokens,\n        return_type=\"all\",\n    )\n    if extraction_config.attn_pattern_row_positions is not None:\n        token_row_indexes, _ = token_index_finder.get_token_index(\n            tokens=extraction_config.attn_pattern_row_positions,\n            string_tokens=string_tokens,\n            return_type=\"all\",\n        )\n        token_dict[\"attn_pattern_row_positions\"] = token_row_indexes\n\n    assert isinstance(token_indexes, list), \"Token index must be a list\"\n    assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n\n    hooks = self.create_hooks(  # TODO: add **kwargs\n        inputs=inputs,\n        token_dict=token_dict,\n        token_indexes=token_indexes,\n        cache=cache,\n        extraction_config=extraction_config,\n        interventions=interventions,\n        batch_idx=batch_idx,\n        external_cache=external_cache,\n    )\n\n    hook_handlers = self.set_hooks(hooks)\n    inputs = self.input_handler.prepare_inputs(\n        inputs, self.first_device, self.config.torch_dtype\n    )\n    # forward pass\n    output = self.hf_model(\n        **inputs,\n        # output_original_output=True,\n        # output_attentions=extract_attn_pattern,\n    )\n\n    # save the logit of the target_token_positions\n    flatten_target_token_positions = [\n        item for sublist in token_indexes for item in sublist\n    ]\n    if extraction_config.save_logits:\n        cache[\"logits\"] = output.logits[:, flatten_target_token_positions, :]\n    # since attention_patterns are returned in the output, we need to adapt to the cache structure\n    if move_to_cpu:\n        cache.cpu()\n        if external_cache is not None:\n            external_cache.cpu()\n\n    stored_token_dict = {}\n    mapping_index = {}\n    current_index = 0\n\n    for token in target_token_positions:\n        mapping_index[token] = []\n        if isinstance(token_dict, int):\n            mapping_index[token].append(current_index)\n            stored_token_dict[token] = token_dict\n            current_index += 1\n        elif isinstance(token_dict, dict):\n            stored_token_dict[token] = token_dict[token]\n            for idx in range(len(token_dict[token])):\n                mapping_index[token].append(current_index)\n                current_index += 1\n        elif isinstance(token_dict, list):\n            stored_token_dict[token] = token_dict\n            for idx in range(len(token_dict)):\n                mapping_index[token].append(current_index)\n                current_index += 1\n        else:\n            raise ValueError(\"Token dict must be an int, a dict or a list\")\n    # update the mapping index in the cache if avg\n    if extraction_config.avg:\n        for i, token in enumerate(target_token_positions):\n            mapping_index[token] = [i]\n        mapping_index[\"info\"] = \"avg\"\n    cache[\"mapping_index\"] = mapping_index\n    cache[\"token_dict\"] = stored_token_dict\n    self.remove_hooks(hook_handlers)\n\n    if extraction_config.keep_gradient:\n        assert vocabulary_index is not None, (\n            \"dict_token_index must be provided if extract_input_embeddings_for_grad is True\"\n        )\n        self._compute_input_gradients(cache, output.logits, vocabulary_index)\n\n    return cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.generate","title":"<code>generate(inputs, generation_config=None, target_token_positions=None, return_text=False, **kwargs)</code>","text":"<p>WARNING: This method could be buggy in the return dict of the output. Pay attention!</p> <p>Generate new tokens using the model and the inputs passed as argument Args:     inputs (dict): dictionary with the inputs of the model {\"input_ids\": ..., \"attention_mask\": ..., \"pixel_values\": ...}     generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration     **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries) Returns:     output (ActivationCache): dictionary with the output of the model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n&gt;&gt;&gt; model.generate(inputs)\n{'sequences': tensor([[101, 1234, 1235, 102]])}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    inputs,\n    generation_config: Optional[GenerationConfig] = None,\n    target_token_positions: Optional[List[str]] = None,\n    return_text: bool = False,\n    **kwargs,\n) -&gt; ActivationCache:\n    r\"\"\"\n    __WARNING__: This method could be buggy in the return dict of the output. Pay attention!\n\n    Generate new tokens using the model and the inputs passed as argument\n    Args:\n        inputs (dict): dictionary with the inputs of the model {\"input_ids\": ..., \"attention_mask\": ..., \"pixel_values\": ...}\n        generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration\n        **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)\n    Returns:\n        output (ActivationCache): dictionary with the output of the model\n\n    Examples:\n        &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n        &gt;&gt;&gt; model.generate(inputs)\n        {'sequences': tensor([[101, 1234, 1235, 102]])}\n    \"\"\"\n    # Initialize cache for logits\n    # raise NotImplementedError(\"This method is not working. It needs to be fixed\")\n    cache = ActivationCache()\n    hook_handlers = None\n    if (\n        target_token_positions is not None\n        or self.additional_interventions is not None\n    ):\n        string_tokens = self.to_string_tokens(\n            self.input_handler.get_input_ids(inputs).squeeze()\n        )\n        token_indexes, token_dict = TokenIndex(\n            self.config.model_name, pivot_positions=None\n        ).get_token_index(tokens=[], string_tokens=string_tokens, return_type=\"all\")\n        assert isinstance(token_indexes, list), \"Token index must be a list\"\n        assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n        hooks = self.create_hooks(\n            inputs=inputs,\n            token_dict=token_dict,\n            token_indexes=token_indexes,\n            cache=cache,\n            **kwargs,\n        )\n        hook_handlers = self.set_hooks(hooks)\n\n    inputs = self.input_handler.prepare_inputs(inputs, self.first_device)\n    # print(inputs.keys())\n    output = self.hf_model.generate(\n        **inputs,  # type: ignore\n        generation_config=generation_config,\n        # output_scores=False,  # type: ignore\n    )\n    if hook_handlers:\n        self.remove_hooks(hook_handlers)\n    if return_text:\n        return self.hf_tokenizer.decode(output[0], skip_special_tokens=True)  # type: ignore\n    if not cache.is_empty():\n        # if the cache is not empty, we will return the cache\n        output = {\"generation_output\": output, \"cache\": cache}\n    return output  # type: ignore\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_image_placeholder","title":"<code>get_image_placeholder()</code>","text":"<p>Return the image placeholder string used by the tokenizer for multimodal models.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_image_placeholder(self) -&gt; str:\n    \"\"\"\n    Return the image placeholder string used by the tokenizer for multimodal models.\n    \"\"\"\n    return self.image_placeholder\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_last_layernorm","title":"<code>get_last_layernorm()</code>","text":"<p>Return the last layer normalization module of the model.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_last_layernorm(self):\n    \"\"\"\n    Return the last layer normalization module of the model.\n    \"\"\"\n    return get_attribute_by_name(self.hf_model, self.model_config.last_layernorm)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_lm_head","title":"<code>get_lm_head()</code>","text":"<p>Return the language modeling head (output projection layer) of the model.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_lm_head(self):\n    \"\"\"\n    Return the language modeling head (output projection layer) of the model.\n    \"\"\"\n    return get_attribute_by_name(self.hf_model, self.model_config.unembed_matrix)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_module_from_string","title":"<code>get_module_from_string(component)</code>","text":"<p>Return a module from the model given the string of the module.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>str</code> <p>the string of the module</p> required <p>Returns:</p> Name Type Description <code>module</code> <code>Module</code> <p>the module of the model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model.get_module_from_string(\"model.layers[0].self_attn\")\nBertAttention(...)\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_module_from_string(self, component: str):\n    r\"\"\"\n    Return a module from the model given the string of the module.\n\n    Args:\n        component (str): the string of the module\n\n    Returns:\n        module (torch.nn.Module): the module of the model\n\n    Examples:\n        &gt;&gt;&gt; model.get_module_from_string(\"model.layers[0].self_attn\")\n        BertAttention(...)\n    \"\"\"\n    return self.hf_model.retrieve_modules_from_names(component)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_processor","title":"<code>get_processor()</code>","text":"<p>Return the processor of the model (None if the model does not have a processor, i.e. text only model)</p> <p>Returns:</p> Name Type Description <code>processor</code> <p>the processor of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_processor(self):\n    r\"\"\"\n    Return the processor of the model (None if the model does not have a processor, i.e. text only model)\n\n    Args:\n        None\n\n    Returns:\n        processor: the processor of the model\n    \"\"\"\n    if self.processor is None:\n        raise ValueError(\"The model does not have a processor\")\n    return self.processor\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_text_tokenizer","title":"<code>get_text_tokenizer()</code>","text":"<p>If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer</p> <p>Returns:</p> Name Type Description <code>tokenizer</code> <p>the tokenizer of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_text_tokenizer(self):\n    r\"\"\"\n    If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer\n\n    Args:\n        None\n\n    Returns:\n        tokenizer: the tokenizer of the model\n    \"\"\"\n    if self.processor is not None:\n        if not hasattr(self.processor, \"tokenizer\"):\n            raise ValueError(\"The processor does not have a tokenizer\")\n        return self.processor.tokenizer  # type: ignore\n    return self.hf_tokenizer\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_tokenizer","title":"<code>get_tokenizer()</code>","text":"<p>Return the tokenizer associated with the model.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_tokenizer(self):\n    \"\"\"\n    Return the tokenizer associated with the model.\n    \"\"\"\n    return self.hf_tokenizer\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.is_multimodal","title":"<code>is_multimodal()</code>","text":"<p>Return True if the model supports multimodal inputs (e.g., images), False otherwise.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def is_multimodal(self) -&gt; bool:\n    \"\"\"\n    Return True if the model supports multimodal inputs (e.g., images), False otherwise.\n    \"\"\"\n    if self.processor is not None:\n        return True\n    return False\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.register_forward_hook","title":"<code>register_forward_hook(component, hook_function)</code>","text":"<p>Register a forward hook on a model component.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>str</code> <p>Name of the model component.</p> required <code>hook_function</code> <code>Callable</code> <p>Function to call during forward pass.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def hook_function(module, input, output):\n&gt;&gt;&gt;     # your code here\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt; model.register_forward_hook(\"model.layers[0].self_attn\", hook_function)\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def register_forward_hook(self, component: str, hook_function: Callable):\n    r\"\"\"\n    Register a forward hook on a model component.\n\n    Args:\n        component (str): Name of the model component.\n        hook_function (Callable): Function to call during forward pass.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; def hook_function(module, input, output):\n        &gt;&gt;&gt;     # your code here\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt; model.register_forward_hook(\"model.layers[0].self_attn\", hook_function)\n    \"\"\"\n    self.additional_hooks.append(\n        {\n            \"component\": component,\n            \"intervention\": hook_function,\n        }\n    )\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.register_interventions","title":"<code>register_interventions(interventions)</code>","text":"<p>Register a list of interventions to be applied during forward passes.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def register_interventions(self, interventions: List[Intervention]):\n    \"\"\"\n    Register a list of interventions to be applied during forward passes.\n    \"\"\"\n    self.additional_interventions = interventions\n    logger.debug(f\"HookedModel: Registered {len(interventions)} interventions\")\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.remove_hooks","title":"<code>remove_hooks(hook_handlers)</code>","text":"<p>Remove all hooks from the model using the provided handlers.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def remove_hooks(self, hook_handlers):\n    \"\"\"\n    Remove all hooks from the model using the provided handlers.\n    \"\"\"\n    for hook_handler in hook_handlers:\n        hook_handler.remove()\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.restore_original_modules","title":"<code>restore_original_modules()</code>","text":"<p>Restore the original modules of the model, removing any custom substitutions.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def restore_original_modules(self):\n    \"\"\"\n    Restore the original modules of the model, removing any custom substitutions.\n    \"\"\"\n    logger.info(\"HookedModel: Restoring original modules.\")\n    self.module_wrapper_manager.restore_original_attention_module(self.hf_model)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.set_custom_modules","title":"<code>set_custom_modules()</code>","text":"<p>Substitute custom modules (e.g., attention) into the model for advanced interpretability.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def set_custom_modules(self):\n    \"\"\"\n    Substitute custom modules (e.g., attention) into the model for advanced interpretability.\n    \"\"\"\n    logger.info(\"HookedModel: Setting custom modules.\")\n    self.module_wrapper_manager.substitute_attention_module(self.hf_model)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.set_hooks","title":"<code>set_hooks(hooks)</code>","text":"<p>Set the hooks in the model</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[dict]</code> <p>list of dictionaries with the component and the intervention to perform in the forward pass of the model</p> required <p>Returns:</p> Name Type Description <code>hook_handlers</code> <code>list</code> <p>list of hook handlers</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def set_hooks(self, hooks: List[Dict[str, Any]]):\n    r\"\"\"\n    Set the hooks in the model\n\n    Args:\n        hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n\n    Returns:\n        hook_handlers (list): list of hook handlers\n    \"\"\"\n\n    if len(hooks) == 0:\n        return []\n\n    hook_handlers = []\n    for hook in hooks:\n        component = hook[\"component\"]\n        hook_function = hook[\"intervention\"]\n\n        # get the last module string (.input or .output) and remove it from the component string\n        last_module = component.split(\".\")[-1]\n        # now remove the last module from the component string\n        component = component[: -len(last_module) - 1]\n        # check if the component exists in the model\n        try:\n            self.assert_module_exists(component)\n        except ValueError as e:\n            logger.error(\n                f\"Error: {e}. Probably the module {component} do not exists in the model. If the module is the attention_matrix_hook, try callig HookedModel.set_custom_hooks() or setting attn_implementation == 'custom_eager'.  Now we will skip the hook for the component {component}\"\n            )\n            continue\n        if last_module == \"input\":\n            hook_handlers.append(\n                get_module_by_path(\n                    self.hf_model, component\n                ).register_forward_pre_hook(\n                    partial(hook_function, output=None), with_kwargs=True\n                )\n            )\n        elif last_module == \"output\":\n            hook_handlers.append(\n                get_module_by_path(self.hf_model, component).register_forward_hook(\n                    hook_function, with_kwargs=True\n                )\n            )\n        else:\n            logger.warning(\n                f\"Warning: the last module of the component {component} is not 'input' or 'output'. We will skip this hook\"\n            )\n\n    return hook_handlers\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.to_string_tokens","title":"<code>to_string_tokens(tokens)</code>","text":"<p>Transform a list or a tensor of tokens in a list of string tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Union[list, Tensor]</code> <p>the tokens to transform in string tokens</p> required <p>Returns:</p> Name Type Description <code>string_tokens</code> <code>list</code> <p>the list of string tokens</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokens = [101, 1234, 1235, 102]\n&gt;&gt;&gt; model.to_string_tokens(tokens)\n['[CLS]', 'hello', 'world', '[SEP]']\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def to_string_tokens(\n    self,\n    tokens: Union[list, torch.Tensor],\n):\n    r\"\"\"\n    Transform a list or a tensor of tokens in a list of string tokens.\n\n    Args:\n        tokens (Union[list, torch.Tensor]): the tokens to transform in string tokens\n\n    Returns:\n        string_tokens (list): the list of string tokens\n\n    Examples:\n        &gt;&gt;&gt; tokens = [101, 1234, 1235, 102]\n        &gt;&gt;&gt; model.to_string_tokens(tokens)\n        ['[CLS]', 'hello', 'world', '[SEP]']\n    \"\"\"\n    if isinstance(tokens, torch.Tensor):\n        if tokens.dim() == 1:\n            tokens = tokens.tolist()\n        else:\n            tokens = tokens.squeeze().tolist()\n    string_tokens = []\n    for tok in tokens:\n        string_tokens.append(self.hf_tokenizer.decode(tok))  # type: ignore\n    return string_tokens\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.use_full_model","title":"<code>use_full_model()</code>","text":"<p>Switch to the full model (including multimodal components if available).</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def use_full_model(self):\n    \"\"\"\n    Switch to the full model (including multimodal components if available).\n    \"\"\"\n    if self.processor is not None:\n        logger.debug(\"HookedModel: Using full model capabilities\")\n        if self.base_model is not None:\n            self.hf_model = self.base_model\n            self.model_config.restore_full_model()\n            self.base_model = None\n    else:\n        if self.base_model is not None:\n            self.hf_model = self.base_model\n        logger.debug(\"HookedModel: Using full text only model capabilities\")\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.use_language_model_only","title":"<code>use_language_model_only()</code>","text":"<p>Switch to using only the language model component (text-only mode).</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def use_language_model_only(self):\n    \"\"\"\n    Switch to using only the language model component (text-only mode).\n    \"\"\"\n    if self.hf_language_model is None:\n        logger.warning(\n            \"HookedModel: The model does not have a separate language model that can be used\",\n        )\n    else:\n        # check if we are already using the language model\n        if self.hf_model == self.hf_language_model:\n            return\n        self.base_model = self.hf_model\n        self.hf_model = self.hf_language_model\n        self.model_config.use_language_model()\n        logger.debug(\"HookedModel: Using only language model capabilities\")\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModelConfig","title":"<code>HookedModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration of the HookedModel</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>the name of the model to load</p> required <code>device_map</code> <code>Literal['balanced', 'cuda', 'cpu', 'auto']</code> <p>the device to use for the model</p> <code>'balanced'</code> <code>torch_dtype</code> <code>dtype</code> <p>the dtype of the model</p> <code>bfloat16</code> <code>attn_implementation</code> <code>Literal['eager', 'flash_attention_2']</code> <p>the implementation of the attention</p> <code>'custom_eager'</code> <code>batch_size</code> <code>int</code> <p>the batch size of the model. FOR NOW, ONLY BATCH SIZE 1 IS SUPPORTED. USE AT YOUR OWN RISK</p> <code>1</code> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@dataclass\nclass HookedModelConfig:\n    \"\"\"\n    Configuration of the HookedModel\n\n    Arguments:\n        model_name (str): the name of the model to load\n        device_map (Literal[\"balanced\", \"cuda\", \"cpu\", \"auto\"]): the device to use for the model\n        torch_dtype (torch.dtype): the dtype of the model\n        attn_implementation (Literal[\"eager\", \"flash_attention_2\"]): the implementation of the attention\n        batch_size (int): the batch size of the model. FOR NOW, ONLY BATCH SIZE 1 IS SUPPORTED. USE AT YOUR OWN RISK\n    \"\"\"\n\n    model_name: str\n    device_map: Literal[\"balanced\", \"cuda\", \"cpu\", \"auto\"] = \"balanced\"\n    torch_dtype: torch.dtype = torch.bfloat16\n    attn_implementation: Literal[\"eager\", \"custom_eager\"] = (\n        \"custom_eager\"  # TODO: add flash_attention_2 in custom module to support it\n    )\n    batch_size: int = 1\n</code></pre>"},{"location":"api/interpretability/hooks/","title":"Hooks","text":""},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook","title":"<code>ablate_pos_keep_self_attn_hook(module, args, kwargs, output, ablation_queries)</code>","text":"<p>Hook function to ablate the tokens in the attention mask but keeping the self attn weigths. It will set to 0 the row of tokens to ablate except for the las position</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def ablate_pos_keep_self_attn_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    ablation_queries: pd.DataFrame,\n):\n    r\"\"\"\n    Hook function to ablate the tokens in the attention\n    mask but keeping the self attn weigths.\n    It will set to 0 the row of tokens to ablate except for\n    the las position\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    Warning(\"This function is deprecated. Use ablate_attn_mat_hook instead\")\n    attn_matrix = b.data\n    # initial_shape = attn_matrix.shape\n\n    for head, pos in zip(\n        ablation_queries[\"head\"], ablation_queries[\"pos_token_to_ablate\"]\n    ):\n        attn_matrix[0, head, pos, :-1] = 0\n\n    b.copy_(attn_matrix)\n\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.attention_pattern_head","title":"<code>attention_pattern_head(module, args, kwargs, output, token_indexes, layer, cache, head='all', act_on_input=False, attn_pattern_avg='none', attn_pattern_row_partition=None)</code>","text":"<p>Hook function to extract the attention pattern of the heads. It will extract the attention pattern. As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p> <p>Parameters:</p> Name Type Description Default <code>- args</code> <p>the input args of the hook function</p> required <code>- kwargs</code> <p>the input kwargs of the hook function</p> required <code>- output</code> <p>the output of the hook function</p> required <code>- token_indexes (List[Tuple]) </code> <p>the indexes of the tokens to extract the attention pattern</p> required <code>- layer</code> <code>int</code> <p>the layer of the model</p> required <code>- cache</code> <code>ActivationCache</code> <p>the cache where to save the activations</p> required <code>- head</code> <code>Union[str, int]</code> <p>the head of the model. If \"all\" is passed, it will extract all the heads of the layer</p> required <code>- attn_pattern_avg</code> <code>Literal['mean', 'sum', 'baseline_ratio', 'none']</code> <p>the method to average the attention pattern</p> required <code>- attn_pattern_row_partition</code> <code>List[int]</code> <p>the indexes of the tokens to partition the attention pattern</p> required Avg strategies <p>If the attn_pattern_avg is not \"none\", the attention pattern is divided in blocks and the average value of each block is computed, using the method specified in attn_pattern_avg. The idea is to partition the attention pattern into groups of tokens, and then compute a single average value for each group. The pattern is divided into len(attn_pattern_row_partition) x len(token_indexes) blocks, where each block B_ij is defined to have the indeces of the rows in attn_pattern_row_partition[i] and the columns in token_indexes[j]. If attn_pattern_row_partition is None, then the rows are the same as token_indexes.</p> <p>0| a_00 0    0    0    0    0    0              token_indexes = [(1,3), (4)] 1| a_10 a_11 0    0    0    0    0              attn_pattern_row_partition = [(0,1)] 2| a_20 a_21 a_22 0    0    0    0 3| a_30 a_31 a_32 a_33 0    0    0 4| a_40 a_41 a_42 a_43 a_44 0    0 5| a_50 a_51 a_52 a_53 a_54 a_55 0 6| a_60 a_61 a_62 a_63 a_64 a_65 a_66</p> <pre><code>0    1     2   3     4    5    6\n</code></pre> <ul> <li>Block B_00:<ul> <li>Rows: 0,1</li> <li>Columns: 1,2,3</li> <li>Block: [a_01, a_02, a_03, a_11, a_12, a_13]</li> </ul> </li> <li>Block B_01:<ul> <li>Rows: 0,1</li> <li>Columns: 4</li> <li>Block: [a_04, a_14]</li> </ul> </li> </ul> <p>If attn_pattern_avg is \"mean\", the average value for each block is computed as the mean of the block, so the output will be: batch n_row_blocks n_col_blocks So in this case, the output will be: batch 1 2 where the first value is the average of the first block and the second value is the average of the second block.</p> <p>The method to compute a single value for each block is specified by the attn_pattern_avg parameter, and can be one of the following: - \"mean\": Compute the mean of the block. - \"sum\": Compute the sum of the block. - \"baseline_ratio\": Compute the ratio of the observed average attention to the expected average attention. The expected average attention is computed by assuming that attention is uniform across the block. So, for each row in attn_pattern_row_partition, we compute the fraction of allowed keys that belong to token_indexes. The expected average attention is the sum of these fractions divided by the number of rows. The final ratio is the observed average attention divided by the expected average attention.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def attention_pattern_head(\n    module,\n    args,\n    kwargs,\n    output,\n    token_indexes,\n    layer,\n    cache,\n    head: Union[str, int] = \"all\",\n    act_on_input=False,\n    attn_pattern_avg: Literal[\"mean\", \"sum\", \"baseline_ratio\", \"none\"] = \"none\",\n    attn_pattern_row_partition=None,\n):\n    \"\"\"\n    Hook function to extract the attention pattern of the heads. It will extract the attention pattern.\n    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)\n\n    Arguments:\n        - args: the input args of the hook function\n        - kwargs: the input kwargs of the hook function\n        - output: the output of the hook function\n        - token_indexes (List[Tuple]) : the indexes of the tokens to extract the attention pattern\n        - layer (int): the layer of the model\n        - cache (ActivationCache): the cache where to save the activations\n        - head (Union[str, int]): the head of the model. If \"all\" is passed, it will extract all the heads of the layer\n        - attn_pattern_avg (Literal[\"mean\", \"sum\", \"baseline_ratio\", \"none\"]): the method to average the attention pattern\n        - attn_pattern_row_partition (List[int]): the indexes of the tokens to partition the attention pattern\n\n    Avg strategies:\n        If the attn_pattern_avg is not \"none\", the attention pattern is divided in blocks and the average value of each block is computed, using the method specified in attn_pattern_avg.\n        The idea is to partition the attention pattern into groups of tokens, and then compute a single average value for each group. The pattern is divided into len(attn_pattern_row_partition) x len(token_indexes) blocks, where each block B_ij is defined to have the indeces of the rows in attn_pattern_row_partition[i] and the columns in token_indexes[j]. If attn_pattern_row_partition is None, then the rows are the same as token_indexes.\n\n        0| a_00 0    0    0    0    0    0              token_indexes = [(1,3), (4)]\n        1| a_10 a_11 0    0    0    0    0              attn_pattern_row_partition = [(0,1)]\n        2| a_20 a_21 a_22 0    0    0    0\n        3| a_30 a_31 a_32 a_33 0    0    0\n        4| a_40 a_41 a_42 a_43 a_44 0    0\n        5| a_50 a_51 a_52 a_53 a_54 a_55 0\n        6| a_60 a_61 a_62 a_63 a_64 a_65 a_66\n           ------------------------------\n            0    1     2   3     4    5    6\n\n        - Block B_00:\n            - Rows: 0,1\n            - Columns: 1,2,3\n            - Block: [a_01, a_02, a_03, a_11, a_12, a_13]\n        - Block B_01:\n            - Rows: 0,1\n            - Columns: 4\n            - Block: [a_04, a_14]\n\n\n        If attn_pattern_avg is \"mean\", the average value for each block is computed as the mean of the block, so the output will be: batch n_row_blocks n_col_blocks\n        So in this case, the output will be: batch 1 2 where the first value is the average of the first block and the second value is the average of the second block.\n\n        The method to compute a single value for each block is specified by the attn_pattern_avg parameter, and can be one of the following:\n        - \"mean\": Compute the mean of the block.\n        - \"sum\": Compute the sum of the block.\n        - \"baseline_ratio\": Compute the ratio of the observed average attention to the expected average attention. The expected average attention is computed by assuming that attention is uniform across the block. So, for each row in attn_pattern_row_partition, we compute the fraction of allowed keys that belong to token_indexes. The expected average attention is the sum of these fractions divided by the number of rows. The final ratio is the observed average attention divided by the expected average attention.\n\n\n    \"\"\"\n    # first get the attention pattern\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    attn_pattern = b.data.detach().clone()  # (batch, num_heads,seq_len, seq_len)\n\n    if head == \"all\":\n        head_indices = range(attn_pattern.size(1))\n    else:\n        head_indices = [head]\n\n    if attn_pattern_row_partition is not None:\n        token_indexes_group1 = attn_pattern_row_partition\n    else:\n        token_indexes_group1 = token_indexes\n\n    # For each token group (each tuple in token_indexes), compute a single average value.\n    if attn_pattern_row_partition is not None:\n        for h in head_indices:\n            # For head h, pattern has shape [batch, seq_len, seq_len].\n            group_avgs = []\n\n            # Generate all combinations of groups\n            for group1 in token_indexes_group1:\n                for group2 in token_indexes:\n                    # Extract the attention block for this combination.\n                    attn_block = attn_pattern[:, h, list(group1), :][:, :, list(group2)]\n\n                    # Depending on the selected averaging method, compute a metric.\n                    if attn_pattern_avg == \"mean\":\n                        # Simple mean over the block.\n                        avg_val = torch.mean(attn_block, dim=(-2, -1))  # shape: [batch]\n\n                    elif attn_pattern_avg == \"sum\":\n                        # Simple sum over the block.\n                        avg_val = torch.sum(attn_block, dim=(-2, -1))\n\n                    elif attn_pattern_avg == \"baseline_ratio\":\n                        # ---- Step 1. Compute the observed average attention in the block.\n                        observed_val = torch.mean(\n                            attn_block, dim=(-2, -1)\n                        )  # shape: [batch]\n\n                        # ---- Step 2. Compute the baseline expectation.\n                        # For each row (i.e. token index) in group1, we calculate the fraction of allowed keys\n                        # that belong to group2. Because the attention is lower-triangular,\n                        # a row with index 'i' can only attend to tokens with indices &lt;= i.\n                        # Thus, for each row 'i' in group1, the expected fraction (if uniform) is:\n                        #      (# of tokens in group2 with index &lt;= i) / (i+1)\n                        baseline_list = []\n                        for i in group1:\n                            # Count the number of tokens in group2 that are allowed for row i.\n                            allowed_count = sum(1 for j in group2 if j &lt;= i)\n                            # Total keys available for row i (assuming indices start at 0).\n                            total_allowed = i + 1\n                            # Avoid division by zero (should not happen if i&gt;=0).\n                            baseline_ratio = (\n                                allowed_count / total_allowed\n                                if total_allowed &gt; 0\n                                else 0.0\n                            )\n                            baseline_list.append(baseline_ratio)\n\n                        # Average the per-row baseline over all rows in group1.\n                        # This represents the expected average attention to group2 if it were uniformly distributed.\n                        baseline_val = sum(baseline_list) / len(baseline_list)\n\n                        # ---- Step 3. Compute the final ratio.\n                        # We compare the observed average attention to the baseline expectation.\n                        # A value &gt; 1 means that, on average, attention in this block is higher than expected.\n                        # Expand baseline_val to match the batch shape for element-wise division.\n                        baseline_tensor = torch.tensor(\n                            baseline_val, device=observed_val.device\n                        ).expand_as(observed_val)\n                        avg_val = observed_val / baseline_tensor\n                    else:\n                        avg_val = attn_block\n\n                    if attn_pattern_avg != \"none\":\n                        # Append the computed metric for this block (keeping the batch dimension).\n                        group_avgs.append(avg_val.unsqueeze(1))  # shape: [batch, 1]\n                    else:\n                        # If no averaging is requested, store the block directly.\n                        group_avgs.append(attn_block)\n\n            if attn_pattern_avg != \"none\":\n                pattern_avg = einops.rearrange(\n                    torch.cat(group_avgs, dim=1),\n                    \"batch (G1 G2) -&gt; batch G1 G2\",\n                    G1=len(token_indexes_group1),\n                    G2=len(token_indexes),\n                )\n            else:\n                try:\n                    pattern_avg = torch.cat(group_avgs, dim=1)\n                except:\n                    raise ValueError(\n                        f\"Error concatenating group_avgs with shapes {[x.shape for x in group_avgs]}\"\n                    )\n\n            # Add the pattern to the cache\n            cache[f\"pattern_L{layer}H{h}\"] = pattern_avg\n    else:\n        # Without averaging, flatten token_indexes into one list.\n        flatten_indexes = [item for tup in token_indexes for item in tup]\n        for h in head_indices:\n            # Slice both token dimensions using the flattened indexes.\n            pattern_slice = attn_pattern[:, h, flatten_indexes][:, :, flatten_indexes]\n            cache[f\"pattern_L{layer}H{h}\"] = pattern_slice\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.avg_attention_pattern_head","title":"<code>avg_attention_pattern_head(module, args, kwargs, output, token_indexes, layer, attn_pattern_current_avg, batch_idx, cache, avg=False, extract_avg_value=False, act_on_input=False)</code>","text":"<p>Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it. As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p> <p>Parameters:</p> Name Type Description Default <code>- b</code> <p>the input of the hook function. It's the output of the attention pattern of the heads</p> required <code>- s</code> <p>the state of the hook function. It's the state of the model</p> required <code>- layer</code> <p>the layer of the model</p> required <code>- head</code> <p>the head of the model</p> required <code>- attn_pattern_current_avg</code> <p>the current average attention pattern</p> required Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def avg_attention_pattern_head(\n    module,\n    args,\n    kwargs,\n    output,\n    token_indexes,\n    layer,\n    attn_pattern_current_avg,\n    batch_idx,\n    cache,\n    avg: bool = False,\n    extract_avg_value: bool = False,\n    act_on_input=False,\n):\n    \"\"\"\n    Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it.\n    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)\n\n    Args:\n        - b: the input of the hook function. It's the output of the attention pattern of the heads\n        - s: the state of the hook function. It's the state of the model\n        - layer: the layer of the model\n        - head: the head of the model\n        - attn_pattern_current_avg: the current average attention pattern\n    \"\"\"\n    # first get the attention pattern\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    attn_pattern = b.data.detach().clone()  # (batch, num_heads,seq_len, seq_len)\n    # attn_pattern = attn_pattern.to(torch.float32)\n    num_heads = attn_pattern.size(1)\n\n    token_indexes = [item for sublist in token_indexes for item in sublist]\n\n    for head in range(num_heads):\n        key = f\"avg_pattern_L{layer}H{head}\"\n        if key not in attn_pattern_current_avg:\n            attn_pattern_current_avg[key] = attn_pattern[:, head, token_indexes][\n                :, :, token_indexes\n            ]\n        else:\n            attn_pattern_current_avg[key] += (\n                attn_pattern[:, head, token_indexes][:, :, token_indexes]\n                - attn_pattern_current_avg[key]\n            ) / (batch_idx + 1)\n        attn_pattern_current_avg[key] = attn_pattern_current_avg[key]\n        # var_key = f\"M2_pattern_L{layer}H{head}\"\n        # if var_key not in attn_pattern_current_avg:\n        #     attn_pattern_current_avg[var_key] = torch.zeros_like(attn_pattern[:, head])\n        # attn_pattern_current_avg[var_key] = attn_pattern_current_avg[var_key] + (attn_pattern[:, head] - attn_pattern_current_avg[key]) * (attn_pattern[:, head] - attn_pattern_current_avg[var_key])\n\n        if extract_avg_value:\n            value_key = f\"projected_value_L{layer}H{head}\"\n            try:\n                values = cache[value_key]\n            except KeyError:\n                print(f\"Values not found for {value_key}\")\n                return\n            # get the attention pattern for the values\n            value_norm = torch.norm(values, dim=-1)\n\n            norm_matrix = (\n                value_norm.unsqueeze(1).expand_as(attn_pattern[:, head]).transpose(1, 2)\n            )\n\n            norm_matrix = norm_matrix * attn_pattern[:, head]\n\n            if value_key not in attn_pattern_current_avg:\n                attn_pattern_current_avg[value_key] = norm_matrix[..., token_indexes, :]\n            else:\n                attn_pattern_current_avg[value_key] += (\n                    norm_matrix[..., token_indexes, :]\n                    - attn_pattern_current_avg[value_key]\n                ) / (batch_idx + 1)\n\n            # remove values from cache\n            del cache[value_key]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.avg_hook","title":"<code>avg_hook(module, args, kwargs, output, cache, cache_key, last_image_idx, end_image_idx)</code>","text":"<p>It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def avg_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache,\n    cache_key,\n    last_image_idx,\n    end_image_idx,\n):\n    r\"\"\"\n    It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    img_avg = torch.mean(\n        b.data.detach().clone()[:, 1 : last_image_idx + 1, :],\n        dim=1,\n    )\n    text_avg = torch.mean(b.data.detach().clone()[:, end_image_idx:, :], dim=1)\n    all_avg = torch.mean(b.data.detach().clone()[:, :, :], dim=1)\n\n    cache[f\"avg_{cache_key}\"] = torch.cat(\n        [img_avg.unsqueeze(1), text_avg.unsqueeze(1), all_avg.unsqueeze(1)], dim=1\n    )\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.compute_statistics","title":"<code>compute_statistics(tensor, dim=-1, keepdim=True, eps=1e-06)</code>","text":"<p>Computes the mean, variance, and second moment of a given tensor along a specified dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor.</p> required <code>dim</code> <code>int</code> <p>Dimension along which to compute statistics (default: -1).</p> <code>-1</code> <code>keepdim</code> <code>bool</code> <p>Whether to keep the reduced dimension (default: True).</p> <code>True</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-06</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(mean, variance, second_moment)</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def compute_statistics(tensor, dim=-1, keepdim=True, eps=1e-6):\n    \"\"\"\n    Computes the mean, variance, and second moment of a given tensor along a specified dimension.\n\n    Args:\n        tensor (torch.Tensor): Input tensor.\n        dim (int): Dimension along which to compute statistics (default: -1).\n        keepdim (bool): Whether to keep the reduced dimension (default: True).\n        eps (float): Small constant for numerical stability.\n\n    Returns:\n        tuple: (mean, variance, second_moment)\n    \"\"\"\n    mean = tensor.mean(dim=dim, keepdim=keepdim)  # Compute mean\n    second_moment = tensor.pow(2).mean(\n        dim=dim, keepdim=keepdim\n    )  # Compute second moment\n    variance = second_moment - mean.pow(2)  # Compute variance using E[X\u00b2] - (E[X])\u00b2\n\n    return mean.squeeze(-1), variance.squeeze(-1), second_moment.squeeze(-1)\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.create_dynamic_hook","title":"<code>create_dynamic_hook(pyvene_hook, **kwargs)</code>","text":"<p>DEPRECATED: pyvene is not used anymore. This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def create_dynamic_hook(pyvene_hook: Callable, **kwargs):\n    r\"\"\"\n    DEPRECATED: pyvene is not used anymore.\n    This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.\n    \"\"\"\n    partial_hook = partial(pyvene_hook, **kwargs)\n\n    def wrap(*args, **kwargs):\n        return partial_hook(*args, **kwargs)\n\n    return wrap\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.embed_hook","title":"<code>embed_hook(module, args, kwargs, output, token_indexes, cache, cache_key)</code>","text":"<p>Hook function to extract the embeddings of the specified tokens and save them in the cache. Args:     module: The module being hooked.     args: Positional arguments.     kwargs: Keyword arguments.     output: Output from the module.     token_indexes: List of token indexes to extract.     cache: The cache object to store results.     cache_key: The key under which to store the embeddings.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def embed_hook(module, args, kwargs, output, token_indexes, cache, cache_key):\n    r\"\"\"\n    Hook function to extract the embeddings of the specified tokens and save them in the cache.\n    Args:\n        module: The module being hooked.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n        output: Output from the module.\n        token_indexes: List of token indexes to extract.\n        cache: The cache object to store results.\n        cache_key: The key under which to store the embeddings.\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    cache[cache_key] = []\n    for token_index in token_indexes:\n        cache[cache_key].append(b.data.detach().clone()[..., list(token_index)])\n    cache[cache_key] = tuple(cache[cache_key])\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.input_embedding_hook","title":"<code>input_embedding_hook(module, args, kwargs, output, cache, cache_key, token_indexes, keep_gradient=False, avg=False)</code>","text":"<p>Hook to capture the output of the embedding layer, enable gradients, and store it in the cache.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def input_embedding_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache,\n    cache_key,\n    token_indexes,\n    keep_gradient: bool = False,\n    avg: bool = False,\n):\n    r\"\"\"\n    Hook to capture the output of the embedding layer, enable gradients, and store it in the cache.\n    \"\"\"\n    embeddings_tensor = process_args_kwargs_output(args, kwargs, output)\n\n    if keep_gradient:\n        # Enable gradient tracking for the embeddings tensor\n        embeddings_tensor.requires_grad_(True).retain_grad()\n        cache[cache_key] = embeddings_tensor  # we slice in the end if keep gradient\n        return restore_same_args_kwargs_output(\n            embeddings_tensor, args, kwargs, output\n        )  # Return the original (potentially modified in-place) output structure\n    if avg:\n        token_avgs = []\n        for token_tuple in token_indexes:\n            # Slice out the tokens specified by the tuple.\n            token_slice = embeddings_tensor[:, list(token_tuple), :]\n            # Average over the token dimension (dim=1) and keep that dimension.\n            token_avg = torch.mean(token_slice, dim=1, keepdim=True)\n            token_avgs.append(token_avg)\n        cache[cache_key] = torch.cat(\n            token_avgs, dim=1\n        )  # Store the tensor that's part of the graph\n    else:\n        flatten_indexes = [item for tup in token_indexes for item in tup]\n        cache[cache_key] = embeddings_tensor[\n            :, flatten_indexes, :\n        ]  # Store the tensor that's part of the graph\n\n    return (\n        output  # Return the original (potentially modified in-place) output structure\n    )\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.intervention_attn_mat_hook","title":"<code>intervention_attn_mat_hook(module, args, kwargs, output, q_positions, k_positions, head, multiplication_value, patching_values=None, apply_softmax=False)</code>","text":"<p>Hook function to ablate the tokens in the attention mask. It will set to 0 the value vector of the tokens to ablate</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def intervention_attn_mat_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    q_positions,\n    k_positions,\n    head,\n    multiplication_value,\n    patching_values: Optional[Union[str, torch.Tensor]] = None,\n    apply_softmax: bool = False,\n    # ablation_queries: pd.DataFrame,\n):\n    r\"\"\"\n    Hook function to ablate the tokens in the attention\n    mask. It will set to 0 the value vector of the\n    tokens to ablate\n    \"\"\"\n    # Get the shape of the attention matrix\n    b = process_args_kwargs_output(args, kwargs, output)\n    batch_size, num_heads, seq_len_q, seq_len_k = b.shape\n\n    # Used during generation\n    if seq_len_q &lt; len(q_positions):\n        q_positions = 0\n\n    # Create boolean masks for queries and keys\n    q_mask = torch.zeros(seq_len_q, dtype=torch.bool, device=b.device)\n    q_mask[q_positions] = True  # Set positions to True\n\n    k_mask = torch.zeros(seq_len_k, dtype=torch.bool, device=b.device)\n    k_mask[k_positions] = True  # Set positions to TrueW\n\n    # Create a 2D mask using outer product\n    head_mask = torch.outer(q_mask, k_mask)  # Shape: (seq_len_q, seq_len_k)\n\n    # Expand mask to match the dimensions of the attention matrix\n    # Shape after expand: (batch_size, num_heads, seq_len_q, seq_len_k)\n    # head_mask = (\n    #     head_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads, -1, -1)\n    # )\n\n    # select the head\n    # head_mask = head_mask[:, head, :, :]\n\n    if patching_values is None or patching_values == \"ablation\":\n        logger.debug(\"No patching values provided, ablation will be performed\")\n        # Apply the ablation function directly to the attention matrix\n        b[:, head, head_mask] = multiply_pattern(\n            b[:, head, head_mask], multiplication_value\n        )\n\n    else:\n        # Apply the patching values to the attention matrix\n        logger.debug(\"Patching values provided, applying patching values\")\n        logger.debug(\n            \"Patching values shape: %s. It is expected to have shape seq_len x seq_len\",\n            patching_values.shape,\n        )\n\n        b[:, head, head_mask] = patching_values[head_mask]\n    if apply_softmax:\n        b[:, head] = torch.nn.functional.softmax(b[:, head], dim=-1)\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.intervention_heads_hook","title":"<code>intervention_heads_hook(module, args, kwargs, output, ablation_queries)</code>","text":"<p>Hook function to ablate the heads in the attention mask. It will set to 0 the output of the heads to ablate</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def intervention_heads_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    ablation_queries: pd.DataFrame,\n):\n    r\"\"\"\n    Hook function to ablate the heads in the attention\n    mask. It will set to 0 the output of the heads to\n    ablate\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    attention_matrix = b.clone().data\n\n    for head in ablation_queries[\"head\"]:\n        attention_matrix[0, head, :, :] = 0\n\n    b.copy_(attention_matrix)\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.intervention_query_key_value_hook","title":"<code>intervention_query_key_value_hook(module, args, kwargs, output, token_indexes, head, head_dim, num_key_value_groups, num_attention_heads, patching_values=None)</code>","text":"<p>Hook function to intervene on the query, key and value vectors. It first unpack the vectors from the output of the module and then apply the intervention and then repack the vectors.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def intervention_query_key_value_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    token_indexes,\n    head,\n    head_dim,\n    num_key_value_groups: int,\n    num_attention_heads: int,\n    patching_values: Optional[Union[str, torch.Tensor]] = None,\n):\n    r\"\"\"\n    Hook function to intervene on the query, key and value vectors. It first unpack the vectors from the output of the module and then apply the intervention and then repack the vectors.\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    # input_shape = b.shape[-1]\n    # hidden_shape = (*input_shape, -1, head_dim)\n    hidden_shape = b.shape\n    # b = b.view(hidden_shape).transpose(1, 2)\n\n    if (\n        num_key_value_groups &gt; 1 and b.size(-1) &lt; num_attention_heads * head_dim\n    ):  # we are in kv group attention\n        b = einops.rearrange(\n            b,\n            \"batch seq_len (num_attention_heads head_dim) -&gt; batch num_attention_heads seq_len head_dim\",\n            num_attention_heads=num_attention_heads // num_key_value_groups,\n            head_dim=head_dim,\n        )\n    # check if we are using kv group attention\n    else:\n        b = einops.rearrange(\n            b,\n            \"batch seq_len (num_attention_heads head_dim) -&gt; batch num_attention_heads seq_len head_dim\",\n            num_attention_heads=num_attention_heads,\n            head_dim=head_dim,\n        )\n\n    # tensor_head = b.data.detach().clone()[:, head, ...]\n    # Apply the intervention\n    if patching_values is None or patching_values == \"ablation\":\n        logger.debug(\n            \"No patching values provided, ablation will be performed on the query, key and value vectors\"\n        )\n        b[:, head, token_indexes, :] = 0\n    else:\n        logger.debug(\n            \"Patching values provided, applying patching values to the query, key and value vectors\"\n        )\n        b[:, head, list(token_indexes), :] = patching_values\n\n    # Repack the vectors\n    b = einops.rearrange(\n        b,\n        \"batch num_attention_heads seq_len head_dim -&gt; batch seq_len (num_attention_heads head_dim)\",\n    )\n\n    # Restore the args and kwargs\n    b = restore_same_args_kwargs_output(b, args, kwargs, output)\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.intervention_resid_hook","title":"<code>intervention_resid_hook(module, args, kwargs, output, token_indexes, patching_values=None)</code>","text":"<p>Hook function to ablate the tokens in the residual stream. It will set to 0 the value vector of the tokens to ablate</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def intervention_resid_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    token_indexes,\n    patching_values: Optional[Union[str, torch.Tensor]] = None,\n):\n    r\"\"\"\n    Hook function to ablate the tokens in the residual stream. It will set to 0 the value vector of the\n    tokens to ablate\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    # detach b to avoid modifying the original tensor\n    b = b.data.detach().clone()\n    if patching_values is None or patching_values == \"ablation\":\n        logger.debug(\n            \"No patching values provided, ablation will be performed on the residual stream\"\n        )\n        b[..., token_indexes, :] = 0\n    else:\n        logger.debug(\n            \"Patching values provided, applying patching values to the residual stream\"\n        )\n        assert b[..., list(token_indexes), :].shape == patching_values.shape, (\n            f\"Shape mismatch: activations is {b[..., list(token_indexes), :].shape} but patching values is {patching_values.shape}\"\n        )\n        b[..., list(token_indexes), :] = patching_values\n    return restore_same_args_kwargs_output(b, args, kwargs, output)\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.layernom_hook","title":"<code>layernom_hook(module, args, kwargs, output, token_indexes, cache, cache_key, avg=False)</code>","text":"<p>Compute and save mean, variance, and second moment for the specified token indexes. If avg is True, computes statistics for each token group; otherwise, flattens indexes. Args:     module: The module being hooked.     args: Positional arguments.     kwargs: Keyword arguments.     output: Output from the module.     token_indexes: List of token index groups.     cache: The cache object to store results.     cache_key: The key under which to store the statistics.     avg: Whether to average over each token group separately.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def layernom_hook(\n    module, args, kwargs, output, token_indexes, cache, cache_key, avg: bool = False\n):\n    \"\"\"\n    Compute and save mean, variance, and second moment for the specified token indexes.\n    If avg is True, computes statistics for each token group; otherwise, flattens indexes.\n    Args:\n        module: The module being hooked.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n        output: Output from the module.\n        token_indexes: List of token index groups.\n        cache: The cache object to store results.\n        cache_key: The key under which to store the statistics.\n        avg: Whether to average over each token group separately.\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    if avg:\n        token_avgs = []\n        for token_index in token_indexes:\n            slice_ = b.data.detach().clone()[..., list(token_index), :]\n            mean, variance, second_moment = compute_statistics(slice_)\n            token_avgs.append(\n                {\"mean\": mean, \"variance\": variance, \"second_moment\": second_moment}\n            )\n        cache[cache_key] = token_avgs\n    flatten_indexes = [item for sublist in token_indexes for item in sublist]\n    mean, variance, second_moment = compute_statistics(b[..., flatten_indexes, :])\n    cache[cache_key] = {\n        \"mean\": mean,\n        \"variance\": variance,\n        \"second_moment\": second_moment,\n    }\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.multiply_pattern","title":"<code>multiply_pattern(tensor, multiplication_value)</code>","text":"<p>Set the attention values to zero</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def multiply_pattern(tensor, multiplication_value):\n    r\"\"\"\n    Set the attention values to zero\n    \"\"\"\n    # return torch.zeros_like(tensor) + multiplication_value\n    return tensor * multiplication_value\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.process_args_kwargs_output","title":"<code>process_args_kwargs_output(args, kwargs, output)</code>","text":"<p>Extract the main tensor from output, args, or kwargs. Prioritizes output (first element if tuple), then first arg, then kwargs['hidden_states'] if present. Args:     args: Positional arguments from the hook.     kwargs: Keyword arguments from the hook.     output: Output from the hooked function. Returns:     The main tensor to be processed by the hook.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def process_args_kwargs_output(args, kwargs, output):\n    \"\"\"\n    Extract the main tensor from output, args, or kwargs.\n    Prioritizes output (first element if tuple), then first arg, then kwargs['hidden_states'] if present.\n    Args:\n        args: Positional arguments from the hook.\n        kwargs: Keyword arguments from the hook.\n        output: Output from the hooked function.\n    Returns:\n        The main tensor to be processed by the hook.\n    \"\"\"\n    if output is not None:\n        if isinstance(output, tuple):\n            b = output[0]\n        else:\n            b = output\n    else:\n        if len(args) &gt; 0:\n            b = args[0]\n        else:\n            candidate_keys = [\"hidden_states\"]\n            for key in candidate_keys:\n                if key in kwargs:\n                    b = kwargs[key]\n                    break\n    return b  # type:ignore\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.projected_key_vectors_head","title":"<code>projected_key_vectors_head(module, args, kwargs, output, layer, cache, token_indexes, num_attention_heads, num_key_value_heads, hidden_size, d_head, out_proj_weight, out_proj_bias, head='all', act_on_input=False, expand_head=True, avg=False)</code>","text":"<p>Hook function to extract the key vectors of the heads and project them through the attention output matrix. This shows the contribution that keys in each position could have to the residual stream through the attention mechanism.</p> <p>Like other hooks, it saves the activations in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>the input of the hook function (output of the key vectors)</p> required <code>layer</code> <p>the layer of the model</p> required <code>head</code> <code>Union[str, int]</code> <p>the head of the model. If \"all\" is passed, it will extract all the heads of the layer</p> <code>'all'</code> <code>expand_head</code> <code>bool</code> <p>bool to expand the head dimension when extracting the keys vectors</p> <code>True</code> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def projected_key_vectors_head(\n    module,\n    args,\n    kwargs,\n    output,\n    layer,\n    cache,\n    token_indexes,\n    num_attention_heads: int,\n    num_key_value_heads: int,\n    hidden_size: int,\n    d_head: int,\n    out_proj_weight,\n    out_proj_bias,\n    head: Union[str, int] = \"all\",\n    act_on_input=False,\n    expand_head: bool = True,\n    avg=False,\n):\n    r\"\"\"\n    Hook function to extract the key vectors of the heads and project them through the attention output matrix.\n    This shows the contribution that keys in each position could have to the residual stream through the attention mechanism.\n\n    Like other hooks, it saves the activations in the cache.\n\n    Args:\n        b: the input of the hook function (output of the key vectors)\n        layer: the layer of the model\n        head: the head of the model. If \"all\" is passed, it will extract all the heads of the layer\n        expand_head: bool to expand the head dimension when extracting the keys vectors\n    \"\"\"\n    # Get the key vectors\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    keys = b.data.detach().clone()  # (batch, num_heads, seq_len, head_dim)\n\n    # Reshape the key vectors to have a separate dimension for the different heads\n    keys = rearrange(\n        keys,\n        \"batch seq_len (num_key_value_heads d_heads) -&gt; batch num_key_value_heads seq_len d_heads\",\n        num_key_value_heads=num_key_value_heads,\n        d_heads=d_head,\n    )\n\n    # If needed, repeat KV heads to match attention heads (for grouped query attention)\n    keys = repeat_kv(keys, num_attention_heads // num_key_value_heads)\n\n    keys = rearrange(\n        keys,\n        \"batch num_head seq_len d_model -&gt; batch seq_len num_head d_model\",\n    )\n\n    # Reshape out_proj_weight to get the blocks for each head\n    out_proj_weight = out_proj_weight.t().view(\n        num_attention_heads,\n        d_head,\n        hidden_size,\n    )\n\n    # Apply bias if present\n    if out_proj_bias is not None:\n        out_proj_bias = out_proj_bias.view(1, 1, 1, hidden_size)\n\n    # Apply the projection for each head\n    projected_keys = einsum(\n        keys,\n        out_proj_weight,\n        \"batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model\",\n    )\n    if out_proj_bias is not None:\n        projected_keys = projected_keys + out_proj_bias\n\n    # Rearrange the tensor to have dimensions that we prefer\n    projected_keys = rearrange(\n        projected_keys,\n        \"batch seq_len num_head d_model -&gt; batch num_head seq_len d_model\",\n    )\n\n    # Process token indices\n    if avg:\n        # For each tuple, slice out the tokens and average over them\n        token_avgs = []\n        for token_tuple in token_indexes:\n            token_slice = projected_keys[..., list(token_tuple), :]\n            token_avg = torch.mean(token_slice, dim=-2, keepdim=True)\n            token_avgs.append(token_avg)\n        projected_keys = torch.cat(token_avgs, dim=-2)\n    else:\n        flatten_indexes = [item for tup in token_indexes for item in tup]\n        projected_keys = projected_keys[..., flatten_indexes, :]\n\n    # Save to cache based on selected heads\n    if head == \"all\":\n        for head_idx in range(num_attention_heads):\n            cache[f\"projected_key_L{layer}H{head_idx}\"] = projected_keys[:, head_idx]\n    else:\n        cache[f\"projected_key_L{layer}H{head}\"] = projected_keys[:, int(head)]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.projected_query_vectors_head","title":"<code>projected_query_vectors_head(module, args, kwargs, output, layer, cache, token_indexes, num_attention_heads, num_key_value_heads, hidden_size, d_head, out_proj_weight, out_proj_bias, head='all', act_on_input=False, expand_head=True, avg=False)</code>","text":"<p>Hook function to extract the query vectors of the heads and project them through the attention output matrix. This shows the contribution that queries in each position could have to the residual stream through the attention mechanism.</p> <p>Like other hooks, it saves the activations in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>the input of the hook function (output of the query vectors)</p> required <code>layer</code> <p>the layer of the model</p> required <code>head</code> <code>Union[str, int]</code> <p>the head of the model. If \"all\" is passed, it will extract all the heads of the layer</p> <code>'all'</code> <code>expand_head</code> <code>bool</code> <p>bool to expand the head dimension when extracting the query vectors</p> <code>True</code> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def projected_query_vectors_head(\n    module,\n    args,\n    kwargs,\n    output,\n    layer,\n    cache,\n    token_indexes,\n    num_attention_heads: int,\n    num_key_value_heads: int,\n    hidden_size: int,\n    d_head: int,\n    out_proj_weight,\n    out_proj_bias,\n    head: Union[str, int] = \"all\",\n    act_on_input=False,\n    expand_head: bool = True,\n    avg=False,\n):\n    r\"\"\"\n    Hook function to extract the query vectors of the heads and project them through the attention output matrix.\n    This shows the contribution that queries in each position could have to the residual stream through the attention mechanism.\n\n    Like other hooks, it saves the activations in the cache.\n\n    Args:\n        b: the input of the hook function (output of the query vectors)\n        layer: the layer of the model\n        head: the head of the model. If \"all\" is passed, it will extract all the heads of the layer\n        expand_head: bool to expand the head dimension when extracting the query vectors\n    \"\"\"\n    # Get the query vectors\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    queries = b.data.detach().clone()  # (batch, seq_len, num_heads*d_head)\n\n    # Reshape the query vectors to have a separate dimension for the heads\n    queries = rearrange(\n        queries,\n        \"batch seq_len (num_attention_heads d_heads) -&gt; batch num_attention_heads seq_len d_heads\",\n        num_attention_heads=num_attention_heads,\n        d_heads=d_head,\n    )\n\n    queries = rearrange(\n        queries,\n        \"batch num_head seq_len d_model -&gt; batch seq_len num_head d_model\",\n    )\n\n    # Reshape out_proj_weight to get the blocks for each head\n    out_proj_weight = out_proj_weight.t().view(\n        num_attention_heads,\n        d_head,\n        hidden_size,\n    )\n\n    # Apply bias if present\n    if out_proj_bias is not None:\n        out_proj_bias = out_proj_bias.view(1, 1, 1, hidden_size)\n\n    # Apply the projection for each head\n    projected_queries = einsum(\n        queries,\n        out_proj_weight,\n        \"batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model\",\n    )\n    if out_proj_bias is not None:\n        projected_queries = projected_queries + out_proj_bias\n\n    # Rearrange the tensor to have dimensions that we prefer\n    projected_queries = rearrange(\n        projected_queries,\n        \"batch seq_len num_head d_model -&gt; batch num_head seq_len d_model\",\n    )\n\n    # Process token indices\n    if avg:\n        # For each tuple, slice out the tokens and average over them\n        token_avgs = []\n        for token_tuple in token_indexes:\n            token_slice = projected_queries[..., list(token_tuple), :]\n            token_avg = torch.mean(token_slice, dim=-2, keepdim=True)\n            token_avgs.append(token_avg)\n        projected_queries = torch.cat(token_avgs, dim=-2)\n    else:\n        flatten_indexes = [item for tup in token_indexes for item in tup]\n        projected_queries = projected_queries[..., flatten_indexes, :]\n\n    # Save to cache based on selected heads\n    if head == \"all\":\n        for head_idx in range(num_attention_heads):\n            cache[f\"projected_query_L{layer}H{head_idx}\"] = projected_queries[\n                :, head_idx\n            ]\n    else:\n        cache[f\"projected_query_L{layer}H{head}\"] = projected_queries[:, int(head)]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.projected_value_vectors_head","title":"<code>projected_value_vectors_head(module, args, kwargs, output, layer, cache, token_indexes, num_attention_heads, num_key_value_heads, hidden_size, d_head, out_proj_weight, out_proj_bias, head='all', act_on_input=False, expand_head=True, avg=False)</code>","text":"<p>Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>the input of the hook function. It's the output of the values vectors of the heads</p> required <code>s</code> <p>the state of the hook function. It's the state of the model</p> required <code>layer</code> <p>the layer of the model</p> required <code>head</code> <code>Union[str, int]</code> <p>the head of the model. If \"all\" is passed, it will extract all the heads of the layer</p> <code>'all'</code> <code>expand_head</code> <code>bool</code> <p>bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like \"value_L0H0\", \"value_L0H1\", ...             while if False, we will have only one key for each layer, like \"value_L0\" and the dimension of the head will be taken into account in the tensor.</p> <code>True</code> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def projected_value_vectors_head(\n    module,\n    args,\n    kwargs,\n    output,\n    layer,\n    cache,\n    token_indexes,\n    num_attention_heads: int,\n    num_key_value_heads: int,\n    hidden_size: int,\n    d_head: int,\n    out_proj_weight,\n    out_proj_bias,\n    head: Union[str, int] = \"all\",\n    act_on_input=False,\n    expand_head: bool = True,\n    avg=False,\n):\n    r\"\"\"\n    Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection\n    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)\n\n    Args:\n        b: the input of the hook function. It's the output of the values vectors of the heads\n        s: the state of the hook function. It's the state of the model\n        layer: the layer of the model\n        head: the head of the model. If \"all\" is passed, it will extract all the heads of the layer\n        expand_head: bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like \"value_L0H0\", \"value_L0H1\", ...\n                        while if False, we will have only one key for each layer, like \"value_L0\" and the dimension of the head will be taken into account in the tensor.\n\n    \"\"\"\n    # first get the values vectors\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    values = b.data.detach().clone()  # (batch, num_heads,seq_len, head_dim)\n\n    # reshape the values vectors to have a separate dimension for the different heads\n    values = rearrange(\n        values,\n        \"batch seq_len (num_key_value_heads d_heads) -&gt; batch num_key_value_heads seq_len d_heads\",\n        num_key_value_heads=num_key_value_heads,\n        d_heads=d_head,\n    )\n\n    #        \"batch seq_len (num_key_value_heads d_heads) -&gt; batch seq_len num_key_value_heads d_heads\",\n\n    values = repeat_kv(values, num_attention_heads // num_key_value_heads)\n\n    values = rearrange(\n        values,\n        \"batch num_head seq_len d_model -&gt; batch seq_len num_head d_model\",\n    )\n\n    # reshape in order to get the blocks for each head\n    out_proj_weight = out_proj_weight.t().view(\n        num_attention_heads,\n        d_head,\n        hidden_size,\n    )\n\n    # apply bias if present (No in Chameleon)\n    if out_proj_bias is not None:\n        out_proj_bias = out_proj_bias.view(1, 1, 1, hidden_size)\n\n    # apply the projection for each head\n    projected_values = einsum(\n        values,\n        out_proj_weight,\n        \"batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model\",\n    )\n    if out_proj_bias is not None:\n        projected_values = projected_values + out_proj_bias\n\n    # rearrange the tensor to have dimension that we like more\n    projected_values = rearrange(\n        projected_values,\n        \"batch seq_len num_head d_model -&gt; batch num_head seq_len d_model\",\n    )\n\n    # slice for token index\n    # Assume projected_values has shape [batch, num_heads, tokens, d_model]\n    if avg:\n        # For each tuple, slice the tokens along dimension -2 and average over that token slice.\n        token_avgs = []\n        for token_tuple in token_indexes:\n            # Slice out the tokens for this tuple.\n            # Using ellipsis ensures we index the last two dimensions correctly.\n            token_slice = projected_values[..., list(token_tuple), :]\n            # Average over the token dimension (which is -2) while keeping that dimension.\n            token_avg = torch.mean(token_slice, dim=-2, keepdim=True)\n            token_avgs.append(token_avg)\n        # Concatenate the averaged slices along the token dimension (-2).\n        projected_values = torch.cat(token_avgs, dim=-2)\n    else:\n        # Flatten the list of token tuples into a single list of token indices.\n        flatten_indexes = [item for tup in token_indexes for item in tup]\n        projected_values = projected_values[..., flatten_indexes, :]\n\n    # Post-process the value vectors by selecting heads.\n    if head == \"all\":\n        for head_idx in range(num_attention_heads):\n            cache[f\"projected_value_L{layer}H{head_idx}\"] = projected_values[\n                :, head_idx\n            ]\n    else:\n        cache[f\"projected_value_L{layer}H{head}\"] = projected_values[:, int(head)]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.query_key_value_hook","title":"<code>query_key_value_hook(module, args, kwargs, output, cache, cache_key, token_indexes, layer, head_dim, num_key_value_groups, num_attention_heads, head='all', avg=False)</code>","text":"<p>Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def query_key_value_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache: ActivationCache,\n    cache_key,\n    token_indexes,\n    layer,\n    head_dim,\n    num_key_value_groups: int,\n    num_attention_heads: int,\n    head: Union[str, int] = \"all\",\n    avg: bool = False,\n):\n    r\"\"\"\n    Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    input_shape = b.shape[:-1]\n    hidden_shape = (*input_shape, -1, head_dim)\n\n    b = b.view(hidden_shape).transpose(1, 2)\n    # cache[cache_key] = b.data.detach().clone()[..., token_index, :]\n    if (\n        num_key_value_groups &gt; 1 and b.size(1) != num_attention_heads\n    ):  # we are in kv group attention\n        # we need to repeat the key and value states\n        b = repeat_kv(b, num_attention_heads // b.size(1))\n\n    # check if we are using kv group attention\n\n    info_string = \"Shape: batch seq_len d_head\"\n\n    heads = [idx for idx in range(b.size(1))] if head == \"all\" else [head]\n    for head_idx in heads:\n        # Compute the group index for keys/values if needed.\n        group_idx = head_idx // (b.size(1) // num_key_value_groups)\n        # Decide whether to use group_idx or head_idx based on cache_key.\n        if \"head_values_\" in cache_key or \"head_keys_\" in cache_key:\n            # Select the slice corresponding to the group index.\n            tensor_slice = b.data.detach().clone()[:, group_idx, ...]\n        else:\n            # Use the head index directly.\n            tensor_slice = b.data.detach().clone()[:, head_idx, ...]\n\n        # Process the token indexes.\n        if avg:\n            # For each token tuple, average over the tokens.\n            # Note: After slicing, the token dimension is the first dimension of tensor_slice,\n            # i.e. tensor_slice has shape (batch, tokens, d_head) so we average along dim=1.\n            tokens_avgs = []\n            for token_tuple in token_indexes:\n                # Slice tokens using the token_tuple.\n                token_subslice = tensor_slice[:, list(token_tuple), :]\n                # Average over the token dimension (dim=1) and keep that dimension.\n                token_avg = torch.mean(token_subslice, dim=1, keepdim=True)\n                tokens_avgs.append(token_avg)\n            # Concatenate the averages along the token dimension (dim=1).\n            processed_tokens = torch.cat(tokens_avgs, dim=1)\n        else:\n            # Flatten the token indexes from the list of tuples.\n            flatten_indexes = [item for tup in token_indexes for item in tup]\n            processed_tokens = tensor_slice[:, flatten_indexes, :]\n\n        # Build a unique key for the cache by including layer and head information.\n        key = f\"{cache_key}L{layer}H{head_idx}\"\n        cache.add_with_info(key, processed_tokens, info_string)\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.restore_same_args_kwargs_output","title":"<code>restore_same_args_kwargs_output(b, args, kwargs, output)</code>","text":"<p>Restore the structure of output, args, and kwargs after modification. Args:     b: The new tensor to insert.     args: Original positional arguments.     kwargs: Original keyword arguments.     output: Original output from the hooked function. Returns:     The updated output, or (args, kwargs) if output is None.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def restore_same_args_kwargs_output(b, args, kwargs, output):\n    \"\"\"\n    Restore the structure of output, args, and kwargs after modification.\n    Args:\n        b: The new tensor to insert.\n        args: Original positional arguments.\n        kwargs: Original keyword arguments.\n        output: Original output from the hooked function.\n    Returns:\n        The updated output, or (args, kwargs) if output is None.\n    \"\"\"\n\n    if output is not None:\n        if isinstance(output, tuple):\n            b = (b,) + output[1:]\n    if output is None:\n        if len(args) &gt; 0:\n            args = (b,) + args[1:]\n        else:\n            candidate_keys = [\"hidden_states\"]\n            for key in candidate_keys:\n                if key in kwargs:\n                    kwargs[key] = b\n                    break\n        return args, kwargs\n    return b  # type:ignore\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.save_resid_hook","title":"<code>save_resid_hook(module, args, kwargs, output, cache, cache_key, token_indexes, avg=False)</code>","text":"<p>Save the activations of the residual stream for the specified token indexes in the cache. If avg is True, saves averaged activations for each token group. Args:     module: The module being hooked.     args: Positional arguments.     kwargs: Keyword arguments.     output: Output from the module.     cache: The cache object to store results.     cache_key: The key under which to store the activations.     token_indexes: List of token index groups.     avg: Whether to average over each token group separately.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def save_resid_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache: ActivationCache,\n    cache_key,\n    token_indexes,\n    avg: bool = False,\n):\n    r\"\"\"\n    Save the activations of the residual stream for the specified token indexes in the cache.\n    If avg is True, saves averaged activations for each token group.\n    Args:\n        module: The module being hooked.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n        output: Output from the module.\n        cache: The cache object to store results.\n        cache_key: The key under which to store the activations.\n        token_indexes: List of token index groups.\n        avg: Whether to average over each token group separately.\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    # slice the tensor to get the activations of the token we want to extract\n    if avg:\n        token_avgs = []\n        for token_index in token_indexes:\n            slice_ = b.data.detach().clone()[..., list(token_index), :]\n            token_avgs.append(torch.mean(slice_, dim=-2, keepdim=True))\n\n        # cache[cache_key] = torch.cat(token_avgs, dim=-2)\n        cache.add_with_info(\n            cache_key,\n            torch.cat(token_avgs, dim=-2),\n            \"Shape: batch avg_over_target_token_position, d_model\",\n        )\n\n    else:\n        flatten_indexes = [item for sublist in token_indexes for item in sublist]\n        cache[cache_key] = b.data.detach().clone()[..., flatten_indexes, :]\n</code></pre>"},{"location":"api/interpretability/interventions/","title":"Interventions","text":""},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.Intervention","title":"<code>Intervention</code>  <code>dataclass</code>","text":"<p>User interface to define the intervention</p> <p>Parameters:</p> Name Type Description Default <code>- type</code> <p>Literal[\"columns\", \"rows\", \"full\", \"block-img-txt\", \"block-img-img\", \"keep-self-attn\"]: The type of intervention to be applied. \"columns\" will intervene on the columns of the attention matrix, \"rows\" will intervene on the rows of the attention matrix, \"full\" will intervene on the full attention matrix, \"block-img-txt\" will intervene on the block of image and text, \"block-img-img\" will intervene on the block of image and image, \"keep-self-attn\" will keep the self-attention of the model.</p> required <code>- activation</code> <p>str: The activation to be intervened. Should have the same format as returned from cache</p> required <code>- token_positions</code> <p>List[Union[str, int]]: The positions of the tokens that will be intervened or ablated</p> required <code>- patching_values</code> <p>Optional[Union[torch.Tensor, Literal[\"ablation\"]]]: The values to be substituted during the intervention. If None or \"ablation\" the values will be set to zero.</p> required Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>@dataclass\nclass Intervention:\n    \"\"\"\n    User interface to define the intervention\n\n    Arguments:\n        - type: Literal[\"columns\", \"rows\", \"full\", \"block-img-txt\", \"block-img-img\", \"keep-self-attn\"]: The type of intervention to be applied. \"columns\" will intervene on the columns of the attention matrix, \"rows\" will intervene on the rows of the attention matrix, \"full\" will intervene on the full attention matrix, \"block-img-txt\" will intervene on the block of image and text, \"block-img-img\" will intervene on the block of image and image, \"keep-self-attn\" will keep the self-attention of the model.\n        - activation: str: The activation to be intervened. Should have the same format as returned from cache\n        - token_positions: List[Union[str, int]]: The positions of the tokens that will be intervened or ablated\n        - patching_values: Optional[Union[torch.Tensor, Literal[\"ablation\"]]]: The values to be substituted during the intervention. If None or \"ablation\" the values will be set to zero.\n    \"\"\"\n\n    type: Literal[\n        \"columns\",\n        \"rows\",\n        \"full\",\n        \"block-img-txt\",\n        \"block-img-img\",\n        \"keep-self-attn\",\n        \"grid\",\n        \"columns_pre_softmax\",\n        \"rows_pre_softmax\",\n        \"grid_pre_softmax\"\n    ]\n    activation: str\n    token_positions: Union[List[Union[str, int]], Tuple[List[str], List[str]]]\n    patching_values: Optional[Union[torch.Tensor, Literal[\"ablation\"]]] = None\n    multiplication_value: float = 0.0\n    apply_softmax: bool = False\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n</code></pre>"},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.InterventionConfig","title":"<code>InterventionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Essential information to apply the intervention</p> <p>Parameters:</p> Name Type Description Default <code>- hook_name</code> <p>str: The name of the hook to be applied, should be the same as model_config</p> required <code>- hook_func</code> <p>Any: The function that will be applied to the hook</p> required <code>- apply_intervention_func</code> <p>Any: The function that is called to do the preliminary computation before attaching the hook to the model. This function is handled by the InterventionManager and is a pre-hook function. It useful since it will process the intervention data and return a clean hook_func that can be attached to the model.</p> required Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>class InterventionConfig(BaseModel):\n    \"\"\"\n    Essential information to apply the intervention\n\n    Arguments:\n        - hook_name: str: The name of the hook to be applied, should be the same as model_config\n        - hook_func: Any: The function that will be applied to the hook\n        - apply_intervention_func: Any: The function that is called to do the preliminary computation before attaching the hook to the model. This function is handled by the InterventionManager and is a pre-hook function. It useful since it will process the intervention data and return a clean hook_func that can be attached to the model.\n    \"\"\"\n\n    hook_name: str\n    hook_func: Any = None\n    apply_intervention_func: Any = None\n    head_dim: Optional[int] = None\n    num_key_value_groups: Optional[int] = None\n    num_attention_heads: Optional[int] = None\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n</code></pre>"},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.InterventionManager","title":"<code>InterventionManager</code>","text":"<p>Class to manage the interventions (ablation, patching, etc) on the model</p> <p>User should define intervention object</p> <p>Parameters:</p> Name Type Description Default <code>- model_config</code> <p>ModelConfig: The configuration of the model</p> required Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>class InterventionManager:\n    \"\"\"\n    Class to manage the interventions (ablation, patching, etc) on the model\n\n    User should define intervention object\n\n    Arguments:\n        - model_config: ModelConfig: The configuration of the model\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n    ):\n        self.model_config = model_config\n\n    def create_intervention_hooks(\n        self, interventions: List[Intervention], token_dict: dict\n    ):\n        \"\"\"\n        Function that given a list of interventions, returns a list of hooks to be applied to the model.\n\n        Arguments:\n            - interventions: List[Intervention]. The list of interventions to be applied to the model\n            - token_dict: dict. The dictionary containing the token positions in the model\n\n        Returns:\n            - List[Dict[str, Any]]: The list of hooks to be applied to the model\n        \"\"\"\n        self._register_interventions()  # Register the interventions. Here to support dynamical model_config changes\n\n        hooks = []\n        for intervention in interventions:\n            type_str = intervention[\"activation\"]\n            intervention_type = intervention[\"type\"]\n\n            # Find the matching regex key from supported_interventions\n            matched_config = None\n            for pattern, config_dict in self.supported_interventions.items():\n                if pattern.match(type_str):\n                    matched_config = config_dict\n                    break\n            if matched_config is None:\n                raise ValueError(\n                    f\"No supported intervention found for activation type {type_str}\"\n                )\n\n            # Check if the intervention_type is supported for that regex key.\n            if intervention_type not in matched_config:\n                raise ValueError(\n                    f\"Intervention type {intervention_type} is not supported for activation {type_str}\"\n                )\n\n            intervention_config = matched_config[intervention_type]\n            hook = intervention_config.apply_intervention_func(\n                hook_name=intervention_config.hook_name,\n                intervention=intervention,\n                token_dict=token_dict,\n                intervention_config = intervention_config,\n\n                # Pass additional parameters if needed\n                # e.g., num_key_value_groups, num_attention_heads, etc.\n                # head_dim=intervention_config.head_dim,\n            )\n            hooks.append(hook)\n        return hooks\n\n    def _register_interventions(self):\n        \"\"\"\n        Function to register the interventions supported by the model.\n        \"\"\"\n        self.supported_interventions = {\n            re.compile(r\"pattern_L\\d+H\\d+\"): {\n                \"columns\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=columns_attn_mat,\n                ),\n                \"rows\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=rows_attn_mat,\n                ),\n                \"grid\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=grid_attn_mat,\n                ),\n                \"block_img_txt\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=block_img_txt_attn_mat,\n                ),\n                \"colums_pre_softmax\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_pre_softmax_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=columns_attn_mat,\n                ),\n                \"rows_pre_softmax\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_pre_softmax_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=rows_attn_mat,\n                ),\n                \"grid_pre_softmax\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_pre_softmax_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=grid_attn_mat,\n                ),\n                \"block_img_txt_pre_softmax\": InterventionConfig(\n                    hook_name=self.model_config.attn_matrix_pre_softmax_hook_name,\n                    hook_func=intervention_attn_mat_hook,\n                    apply_intervention_func=block_img_txt_attn_mat,\n                ),\n            },\n            # Residual stream interventions\n            re.compile(r\"resid_out_\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.residual_stream_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"resid_in_\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.residual_stream_input_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"resid_mid_\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.intermediate_stream_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            # Attention input and output interventions\n            re.compile(r\"attn_in_\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.attn_in_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"attn_out_\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.attn_out_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            # MLP output interventions\n            re.compile(r\"mlp_out_\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.mlp_out_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            # Head component interventions\n            re.compile(r\"values_L\\d\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_value_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"keys_L\\d\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_key_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"queries_L\\d\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_query_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"head_values_L\\d+H\\d\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_value_hook_name,\n                    hook_func=intervention_query_key_value,\n                    apply_intervention_func=intervention_query_key_value,\n                    head_dim=self.model_config.head_dim,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                )\n            },\n            re.compile(r\"head_keys_L\\d+H\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_key_hook_name,\n                    hook_func=intervention_query_key_value,\n                    apply_intervention_func=intervention_query_key_value,\n                    head_dim=self.model_config.head_dim,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                )\n            },\n            re.compile(r\"head_queries_L\\d+H\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_query_hook_name,\n                    hook_func=intervention_query_key_value,\n                    apply_intervention_func=intervention_query_key_value,\n                    head_dim=self.model_config.head_dim,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                )\n            },\n            # Projected vectors interventions\n            re.compile(r\"projected_value_L\\d+H\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_value_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"projected_key_L\\d+H\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_key_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            re.compile(r\"projected_query_L\\d+H\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.head_query_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n            # Head output interventions\n            re.compile(r\"head_out_L\\d+H\\d+\"): {\n                \"full\": InterventionConfig(\n                    hook_name=self.model_config.attn_o_proj_input_hook_name,\n                    hook_func=intervention_resid_hook,\n                    apply_intervention_func=intervention_resid_full,\n                )\n            },\n        }\n</code></pre>"},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.InterventionManager.create_intervention_hooks","title":"<code>create_intervention_hooks(interventions, token_dict)</code>","text":"<p>Function that given a list of interventions, returns a list of hooks to be applied to the model.</p> <p>Parameters:</p> Name Type Description Default <code>- interventions</code> <p>List[Intervention]. The list of interventions to be applied to the model</p> required <code>- token_dict</code> <p>dict. The dictionary containing the token positions in the model</p> required <p>Returns:</p> Type Description <ul> <li>List[Dict[str, Any]]: The list of hooks to be applied to the model</li> </ul> Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>def create_intervention_hooks(\n    self, interventions: List[Intervention], token_dict: dict\n):\n    \"\"\"\n    Function that given a list of interventions, returns a list of hooks to be applied to the model.\n\n    Arguments:\n        - interventions: List[Intervention]. The list of interventions to be applied to the model\n        - token_dict: dict. The dictionary containing the token positions in the model\n\n    Returns:\n        - List[Dict[str, Any]]: The list of hooks to be applied to the model\n    \"\"\"\n    self._register_interventions()  # Register the interventions. Here to support dynamical model_config changes\n\n    hooks = []\n    for intervention in interventions:\n        type_str = intervention[\"activation\"]\n        intervention_type = intervention[\"type\"]\n\n        # Find the matching regex key from supported_interventions\n        matched_config = None\n        for pattern, config_dict in self.supported_interventions.items():\n            if pattern.match(type_str):\n                matched_config = config_dict\n                break\n        if matched_config is None:\n            raise ValueError(\n                f\"No supported intervention found for activation type {type_str}\"\n            )\n\n        # Check if the intervention_type is supported for that regex key.\n        if intervention_type not in matched_config:\n            raise ValueError(\n                f\"Intervention type {intervention_type} is not supported for activation {type_str}\"\n            )\n\n        intervention_config = matched_config[intervention_type]\n        hook = intervention_config.apply_intervention_func(\n            hook_name=intervention_config.hook_name,\n            intervention=intervention,\n            token_dict=token_dict,\n            intervention_config = intervention_config,\n\n            # Pass additional parameters if needed\n            # e.g., num_key_value_groups, num_attention_heads, etc.\n            # head_dim=intervention_config.head_dim,\n        )\n        hooks.append(hook)\n    return hooks\n</code></pre>"},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.columns_attn_mat","title":"<code>columns_attn_mat(hook_name, intervention, token_dict, intervention_config)</code>","text":"<p>Pre-Hook function to compute the columns to be intervened in the attention matrix.</p> Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>def columns_attn_mat(hook_name, intervention: Intervention, token_dict, intervention_config):\n    \"\"\"\n    Pre-Hook function to compute the columns to be intervened in the attention matrix.\n    \"\"\"\n    # compute the pre-hooks information and return the hook_func\n    keys_intervention_token_position = []\n    for token in intervention.token_positions:\n        keys_intervention_token_position.extend(token_dict[token])\n\n    queries_token_positions = [q for q in token_dict[\"all\"]]\n    try:\n        layer = int(re.search(r\"L(\\d+)\", intervention.activation).group(1))\n        head = int(re.search(r\"H(\\d+)\", intervention.activation).group(1))\n    except AttributeError:\n        raise ValueError(\n            f\"Activation {intervention['activation']} is not in the format pattern_L\\d+H\\d+\"\n        )\n\n    return {\n        \"component\": hook_name.format(layer),\n        \"intervention\": partial(\n            intervention_attn_mat_hook,\n            q_positions=queries_token_positions,\n            k_positions=keys_intervention_token_position,\n            patching_values=intervention.patching_values,\n            head=head,\n            multiplication_value=intervention.multiplication_value,\n            apply_softmax=intervention.apply_softmax,\n        ),\n    }\n</code></pre>"},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.intervention_query_key_value","title":"<code>intervention_query_key_value(hook_name, intervention, token_dict, intervention_config)</code>","text":"<p>Pre-Hook function to compute the values to be intervened in the attention matrix.</p> Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>def intervention_query_key_value(hook_name, intervention, token_dict, intervention_config):\n    \"\"\"\n    Pre-Hook function to compute the values to be intervened in the attention matrix.\n    \"\"\"\n    # compute the pre-hooks information and return the hook_func\n    target_positions = []\n    for token in intervention[\"token_positions\"]:\n        if isinstance(token, str) and token in token_dict:\n            # If the token is a string, get the positions from the token_dict\n            target_positions.extend(token_dict[token])\n        elif isinstance(token, int):\n            # If the token is an int, add it directly to the target positions\n            target_positions.append(token)\n\n    # get the integer layer number from teh activation string resid_out_L\\d+ or resid_in_L\\d+ or resid_mid_L\\d+\n    layer = int(\n        re.search(r\"(\\d+)\", intervention.activation).group(1)\n    )\n    head = int(\n        re.search(r\"H(\\d+)\", intervention.activation).group(1)\n    )\n\n    return {\n        \"component\": hook_name.format(layer),\n        \"intervention\": partial(\n            intervention_query_key_value_hook,\n            token_indexes=target_positions,\n            head=head,\n            head_dim=intervention_config.head_dim,\n            num_key_value_groups = intervention_config.num_key_value_groups,\n            num_attention_heads = intervention_config.num_attention_heads,\n            patching_values=intervention[\"patching_values\"],\n        ),\n    }\n</code></pre>"},{"location":"api/interpretability/interventions/#easyroutine.interpretability.interventions.rows_attn_mat","title":"<code>rows_attn_mat(hook_name, intervention, token_dict, intervention_config)</code>","text":"<p>Pre-Hook function to compute the columns to be intervened in the attention matrix.</p> Source code in <code>easyroutine/interpretability/interventions.py</code> <pre><code>def rows_attn_mat(hook_name, intervention: Intervention, token_dict, intervention_config):\n    \"\"\"\n    Pre-Hook function to compute the columns to be intervened in the attention matrix.\n    \"\"\"\n    # compute the pre-hooks information and return the hook_func\n    queries_token_positions = []\n    for token in intervention.token_positions:\n        queries_token_positions.extend(token_dict[token])\n\n    keys_token_positions = [k for k in token_dict[\"all\"]]\n    try:\n        layer = int(re.search(r\"L(\\d+)\", intervention.activation).group(1))\n        head = int(re.search(r\"H(\\d+)\", intervention.activation).group(1))\n    except AttributeError:\n        raise ValueError(\n            f\"Activation {intervention['activation']} is not in the format pattern_L\\d+H\\d+\"\n        )\n\n    return {\n        \"component\": hook_name.format(layer),\n        \"intervention\": partial(\n            intervention_attn_mat_hook,\n            q_positions=queries_token_positions,\n            k_positions=keys_token_positions,\n            patching_values=intervention.patching_values,\n            head=head,\n            multiplication_value=intervention.multiplication_value,\n            apply_softmax=intervention.apply_softmax,\n        ),\n    }\n</code></pre>"},{"location":"api/interpretability/models/","title":"Models","text":""},{"location":"api/interpretability/models/#easyroutine.interpretability.models.InputHandler","title":"<code>InputHandler</code>","text":"Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>class InputHandler:\n    def __init__(self, model_name: str):\n        self.model_name = model_name\n\n    def prepare_inputs(\n        self,\n        batch_dict: Dict[str, torch.Tensor],\n        device: Union[str, torch.device],\n        torch_dtype: torch.dtype = torch.bfloat16,\n        require_grads: bool = False,\n    ):\n        if self.model_name in [\n            \"facebook/chameleon-7b\",\n            \"facebook/chameleon-30b\",\n        ]:\n            if not \"pixel_values\" in batch_dict:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                }\n            else:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                    \"pixel_values\": batch_dict[\"pixel_values\"].to(torch_dtype),\n                }\n        elif self.model_name in [\"mistral-community/pixtral-12b\"]:\n            if \"pixel_values\" not in batch_dict:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                }\n            else:\n                if (\n                    isinstance(batch_dict[\"pixel_values\"][0], list)\n                    and len(batch_dict[\"pixel_values\"]) == 1\n                ):\n                    # batch_dict[\"pixel_values\"] = batch_dict[\"pixel_values\"][0]\n                    batch_dict[\"pixel_values\"] = [\n                        [\n                            image.to(torch_dtype)\n                            for image in batch_dict[\"pixel_values\"][0]\n                        ]\n                    ]\n                elif isinstance(batch_dict[\"pixel_values\"], torch.Tensor):\n                    batch_dict[\"pixel_values\"] = batch_dict[\"pixel_values\"].to(\n                        torch_dtype\n                    )\n                elif isinstance(batch_dict[\"pixel_values\"], list):\n                    batch_dict[\"pixel_values\"] = [\n                        image.to(torch_dtype) for image in batch_dict[\"pixel_values\"]\n                    ]\n                else:\n                    raise ValueError(\"Pixel values not recognized. Please fix!\")\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                    \"pixel_values\": batch_dict[\"pixel_values\"],\n                    \"image_sizes\": batch_dict[\"image_sizes\"]\n                }\n\n        elif self.model_name in [\"meta-llama/Llama-3.2-1B\", \"meta-llama/Llama-3.2-3B\"]:\n            input_dict = {\n                \"input_ids\": batch_dict[\"input_ids\"],\n                \"attention_mask\": batch_dict[\"attention_mask\"],\n            }\n        elif self.model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n            raise NotImplementedError(\"Emu3 model not implemented yet\")\n        elif self.model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n            input_dict = {\n                \"input_ids\": batch_dict[\"input_ids\"],\n                \"attention_mask\": batch_dict[\"attention_mask\"],\n            }\n        elif self.model_name in [\"llava-hf/llava-v1.6-mistral-7b-hf\"]:\n            if \"pixel_values\" not in batch_dict:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                }\n            else:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                    \"pixel_values\": batch_dict[\"pixel_values\"],\n                    \"image_sizes\": batch_dict[\"image_sizes\"],\n                }\n\n        elif self.model_name in [\"llava-hf/llava-onevision-qwen2-7b-ov-hf\"]:\n            if \"pixel_values\" not in batch_dict:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                }\n            else:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                    \"pixel_values\": batch_dict[\"pixel_values\"],\n                    \"image_sizes\": batch_dict[\"image_sizes\"],\n                }      \n        elif self.model_name in [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\", \"google/gemma-3-12b-it\", \"google/gemma-3-27b-it\"]:\n            if \"pixel_values\" not in batch_dict:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                }\n            else:\n                input_dict = {\n                    \"input_ids\": batch_dict[\"input_ids\"],\n                    \"attention_mask\": batch_dict[\"attention_mask\"],\n                    \"pixel_values\": batch_dict[\"pixel_values\"],\n                    \"token_type_ids\": batch_dict[\"token_type_ids\"],\n                }\n        elif self.model_name in [\"CohereForAI/aya-101\"]:\n            input_dict = {\n                \"input_ids\": batch_dict[\"input_ids\"],\n                \"decoder_input_ids\": batch_dict[\"input_ids\"],\n                \"attention_mask\": batch_dict[\"attention_mask\"],\n            }\n        else:\n            raise ValueError(f\"Unsupported model_name: {self.model_name}\")\n\n        for key, value in input_dict.items():\n            if isinstance(value, torch.Tensor):\n                value = value.to(device)\n            elif isinstance(value, list):\n                if isinstance(value[0], torch.Tensor):\n                    value = [v.to(device) for v in value]\n                elif isinstance(value[0], list):\n                    value = [[v.to(device) for v in vv] for vv in value]\n                else:\n                    raise ValueError(\n                        f\"Problem while moving the input to the device. The input with key {key} is not a torch.Tensor, a list of torch.Tensor or a list of list of torch.Tensor.\"\n                    )\n\n            input_dict[key] = value\n\n        if require_grads:\n            input_dict[\"input_ids\"].requires_grad = True\n        return input_dict\n\n    def get_input_ids(\n        self,\n        input_dict: Dict[str, torch.Tensor],\n    ):\n        return input_dict[\"input_ids\"]\n\n    def cleanup_tensors(self, inputs: Dict[str, Any], others: Dict[str, Any] = None):\n        \"\"\"\n        Clean up tensors to free GPU memory by detaching, moving to CPU, and deleting.\n\n        Args:\n            inputs (Dict[str, Any]): Dictionary of input tensors (usually from prepare_inputs)\n            others (Dict[str, Any], optional): Dictionary of other tensors not in inputs\n\n        Returns:\n            None\n        \"\"\"\n        # Clean up inputs from the batch\n        if inputs is not None:\n            for k_in in list(inputs.keys()):  # Iterate over a copy of keys\n                if isinstance(inputs[k_in], torch.Tensor):\n                    inputs[k_in] = inputs[k_in].detach().cpu()\n                del inputs[k_in]\n            del inputs\n\n        # Clean up others from the batch\n        if others is not None:\n            for k_oth in list(others.keys()):\n                if isinstance(others[k_oth], torch.Tensor):\n                    others[k_oth] = others[k_oth].detach().cpu()\n                del others[k_oth]\n            del others\n\n        # Explicitly run garbage collection to free memory\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.InputHandler.cleanup_tensors","title":"<code>cleanup_tensors(inputs, others=None)</code>","text":"<p>Clean up tensors to free GPU memory by detaching, moving to CPU, and deleting.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Dictionary of input tensors (usually from prepare_inputs)</p> required <code>others</code> <code>Dict[str, Any]</code> <p>Dictionary of other tensors not in inputs</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>def cleanup_tensors(self, inputs: Dict[str, Any], others: Dict[str, Any] = None):\n    \"\"\"\n    Clean up tensors to free GPU memory by detaching, moving to CPU, and deleting.\n\n    Args:\n        inputs (Dict[str, Any]): Dictionary of input tensors (usually from prepare_inputs)\n        others (Dict[str, Any], optional): Dictionary of other tensors not in inputs\n\n    Returns:\n        None\n    \"\"\"\n    # Clean up inputs from the batch\n    if inputs is not None:\n        for k_in in list(inputs.keys()):  # Iterate over a copy of keys\n            if isinstance(inputs[k_in], torch.Tensor):\n                inputs[k_in] = inputs[k_in].detach().cpu()\n            del inputs[k_in]\n        del inputs\n\n    # Clean up others from the batch\n    if others is not None:\n        for k_oth in list(others.keys()):\n            if isinstance(others[k_oth], torch.Tensor):\n                others[k_oth] = others[k_oth].detach().cpu()\n            del others[k_oth]\n        del others\n\n    # Explicitly run garbage collection to free memory\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration class for storing model specific parameters.</p> <p>Attributes:</p> Name Type Description <code>residual_stream_input_hook_name</code> <code>str</code> <p>Name of the residual stream torch module where attach the hook</p> <code>residual_stream_hook_name</code> <code>str</code> <p>Name of the residual stram torch module where attach the hook</p> <code>intermediate_stream_hook_name</code> <code>str</code> <p>Name of the intermediate stream torch module where attach the hook</p> <code>residual_stream_input_post_layernorm_hook_name</code> <code>str</code> <p>Name of the residual stream input post layer norm</p> <code>head_key_hook_name</code> <code>str</code> <p>Name of the attention key torch module where attach the hook</p> <code>head_value_hook_name</code> <code>str</code> <p>Name of the attention value torch module where attach the hook</p> <code>head_query_hook_name</code> <code>str</code> <p>Name of the attention key torch module where attach the hook</p> <code>attn_in_hook_name</code> <code>str</code> <p>Name of the attention input torch module where attach the hook</p> <code>attn_out_hook_name</code> <code>str</code> <p>Name of the attention output torch module where attach the hook</p> <code>attn_matrix_hook_name</code> <code>str</code> <p>Name of the attention matrix torch module where attach the hook</p> <code>mlp_out_hook_name</code> <code>str</code> <p>Name of the mlp output torch module where attach the hook</p> <code>attn_out_proj_weight</code> <code>str</code> <p>Name of the attention output projection weight</p> <code>attn_out_proj_bias</code> <code>str</code> <p>Name of the attention output projection bias</p> <code>embed_tokens</code> <code>str</code> <p>Name of the embedding tokens torch module where attach the hook</p> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers</p> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads</p> <code>hidden_size</code> <code>int</code> <p>Hidden size of the transformer model</p> <code>num_key_value_heads</code> <code>int</code> <p>Number of key value heads</p> <code>num_key_value_groups</code> <code>int</code> <p>Number of key value groups</p> <code>head_dim</code> <code>int</code> <p>Dimension of the attention head</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    r\"\"\"\n    Configuration class for storing model specific parameters.\n\n    Attributes:\n        residual_stream_input_hook_name (str): Name of the residual stream torch module where attach the hook\n        residual_stream_hook_name (str): Name of the residual stram torch module where attach the hook\n        intermediate_stream_hook_name (str): Name of the intermediate stream torch module where attach the hook\n        residual_stream_input_post_layernorm_hook_name (str): Name of the residual stream input post layer norm\n        head_key_hook_name (str): Name of the attention key torch module where attach the hook\n        head_value_hook_name (str): Name of the attention value torch module where attach the hook\n        head_query_hook_name (str): Name of the attention key torch module where attach the hook\n        attn_in_hook_name (str): Name of the attention input torch module where attach the hook\n        attn_out_hook_name (str): Name of the attention output torch module where attach the hook\n        attn_matrix_hook_name (str): Name of the attention matrix torch module where attach the hook\n        mlp_out_hook_name (str): Name of the mlp output torch module where attach the hook\n        attn_out_proj_weight (str): Name of the attention output projection weight\n        attn_out_proj_bias (str): Name of the attention output projection bias\n        embed_tokens (str): Name of the embedding tokens torch module where attach the hook\n        num_hidden_layers (int): Number of hidden layers\n        num_attention_heads (int): Number of attention heads\n        hidden_size (int): Hidden size of the transformer model\n        num_key_value_heads (int): Number of key value heads\n        num_key_value_groups (int): Number of key value groups\n        head_dim (int): Dimension of the attention head\n\n    \"\"\"\n\n    residual_stream_input_hook_name: str\n    residual_stream_hook_name: str\n    intermediate_stream_hook_name: str\n    residual_stream_input_post_layernorm_hook_name: str\n    head_key_hook_name: str\n    head_value_hook_name: str\n    head_query_hook_name: str\n    # head_out_hook_name: str\n    attn_in_hook_name: str\n    attn_in_hook_name: str\n    attn_out_hook_name: str\n    attn_o_proj_input_hook_name: str\n    attn_matrix_hook_name: str\n    attn_matrix_pre_softmax_hook_name: str\n    mlp_out_hook_name: str\n    last_layernorm_hook_name: str\n\n\n    attn_out_proj_weight: str\n    attn_out_proj_bias: str\n    embed_tokens: str\n    unembed_matrix: str\n    last_layernorm: str\n\n    num_hidden_layers: int\n    num_attention_heads: int\n    hidden_size: int\n    num_key_value_heads: int\n    num_key_value_groups: int\n    head_dim: int\n    layernorm_type: Literal[\"RMS\", \"LayerNorm\"]\n\n    def use_language_model(self):\n        \"\"\"\n        for the hook, remove the \"language_model\" prefix: \"language_model.model.layers[{}]\" -&gt; \"model.layers[{}]\" . Remove from the data ckass\n        \"\"\"\n        # iterate over the dataclass fields\n        for field in self.__dataclass_fields__.keys():\n            # remove the \"language_model.\" prefix\n            if \"hook\" in field:\n                setattr(self, field, getattr(self, field).replace(\"language_model.\", \"\"))\n\n    def restore_full_model(self):\n        for field in self.__dataclass_fields__.keys():\n            if \"hook\" in field:\n                setattr(self, field, getattr(self, field).replace(\"model.\",\"language_model.model.\"))\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelConfig.use_language_model","title":"<code>use_language_model()</code>","text":"<p>for the hook, remove the \"language_model\" prefix: \"language_model.model.layers[{}]\" -&gt; \"model.layers[{}]\" . Remove from the data ckass</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>def use_language_model(self):\n    \"\"\"\n    for the hook, remove the \"language_model\" prefix: \"language_model.model.layers[{}]\" -&gt; \"model.layers[{}]\" . Remove from the data ckass\n    \"\"\"\n    # iterate over the dataclass fields\n    for field in self.__dataclass_fields__.keys():\n        # remove the \"language_model.\" prefix\n        if \"hook\" in field:\n            setattr(self, field, getattr(self, field).replace(\"language_model.\", \"\"))\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>This class is a factory to load the model and the processor. It supports the following models:</p> Supported Models <p>The following models are supported by this factory:</p> <ul> <li>Chameleon-7b: A 7-billion parameter model for general-purpose tasks.</li> <li>Chameleon-30b: A larger version of the Chameleon series with 30 billion parameters.</li> <li>llava-hf/llava-v1.6-mistral-7b-hf: A 7-billion parameter model for multimodal tasks.</li> <li>Pixtral-12b: Optimized for image-to-text tasks.</li> <li>Emu3-Chat: Fine-tuned for conversational AI.</li> <li>Emu3-Gen: Specialized in text generation tasks.</li> <li>Emu3-Stage1: Pretrained for multi-stage training pipelines.</li> <li>Llava-onevision: A multimodal model for vision and language tasks.</li> <li>hf-internal-testing: A tiny model for internal testing purposes.</li> </ul> Adding a New Model <p>To add a new model: 1. Implement its logic in the <code>load_model</code> method. 2. Ensure it is correctly initialized and validated.</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>class ModelFactory:\n    r\"\"\"\n    This class is a factory to load the model and the processor. It supports the following models:\n\n    Supported Models:\n        The following models are supported by this factory:\n\n        - **Chameleon-7b**: A 7-billion parameter model for general-purpose tasks.\n        - **Chameleon-30b**: A larger version of the Chameleon series with 30 billion parameters.\n        - **llava-hf/llava-v1.6-mistral-7b-hf**: A 7-billion parameter model for multimodal tasks.\n        - **Pixtral-12b**: Optimized for image-to-text tasks.\n        - **Emu3-Chat**: Fine-tuned for conversational AI.\n        - **Emu3-Gen**: Specialized in text generation tasks.\n        - **Emu3-Stage1**: Pretrained for multi-stage training pipelines.\n        - **Llava-onevision**: A multimodal model for vision and language tasks.\n        - **hf-internal-testing**: A tiny model for internal testing purposes.\n\n    Adding a New Model:\n        To add a new model:\n        1. Implement its logic in the `load_model` method.\n        2. Ensure it is correctly initialized and validated.\n    \"\"\"\n\n    @staticmethod\n    def load_model(\n        model_name: str,\n        attn_implementation: str,\n        torch_dtype: torch.dtype,\n        device_map: str,\n    ) -&gt; Tuple[torch.nn.Module, Optional[torch.nn.Module], ModelConfig]:\n        r\"\"\"\n        Load the model and its configuration based on the model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            attn_implementation (str): Attention implementation type. (eager, flash-attn, sdp)\n            torch_dtype (torch.dtype): Data type of the model.\n            device_map (str): Device map for the model.\n\n        Returns:\n            model (HuggingFaceModel): Model instance.\n            model_config (ModelConfig): Model configuration.\n        \"\"\"\n        if attn_implementation != \"eager\":\n            logger.warning(\n                \"ModelFactory: Using an attention type different from eager or custom eager could have unexpected behavior in some experiments!\",\n            )\n\n        language_model = None\n        if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n            model = ChameleonForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"model.layers[{}].input\",\n                residual_stream_hook_name=\"model.layers[{}].output\",\n                intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n                mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n                attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_matrix_pre_softmax_hook_name=\"model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                last_layernorm_hook_name=\"model.norm.input\",\n                attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"model.layers[0].input\",\n                unembed_matrix=\"lm_head.weight\",\n                last_layernorm=\"model.norm\",\n                num_hidden_layers=model.config.num_hidden_layers,\n                num_attention_heads=model.config.num_attention_heads,\n                hidden_size=model.config.hidden_size,\n                num_key_value_heads=model.config.num_key_value_heads,\n                num_key_value_groups=model.config.num_attention_heads\n                // model.config.num_key_value_heads,\n                head_dim=model.config.hidden_size // model.config.num_attention_heads,\n                layernorm_type=\"RMS\",\n            )\n\n        elif model_name in [ # TODO: Refactor this, bad nested if\n            \"mistral-community/pixtral-12b\",\n            \"llava-hf/llava-v1.6-mistral-7b-hf\",\n            \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n        ]:\n            if model_name == \"mistral-community/pixtral-12b\":\n                model = LlavaForConditionalGeneration.from_pretrained(\n                    model_name,\n                    torch_dtype=torch_dtype,\n                    device_map=device_map,\n                    attn_implementation=attn_implementation,\n                )\n                model_config = ModelConfig(\n                    residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                    residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                    intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                    residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                    head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                    head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                    attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                    attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                    attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                    attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                    mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                    last_layernorm_hook_name=\"language_model.model.norm.input\",\n                    attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                    attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                    embed_tokens=\"language_model.model.layers[0].input\",\n                    unembed_matrix=\"language_model.lm_head.weight\",\n                    last_layernorm=\"language_model.model.norm\",\n                    num_hidden_layers=model.language_model.config.num_hidden_layers,\n                    num_attention_heads=model.language_model.config.num_attention_heads,\n                    hidden_size=model.language_model.config.hidden_size,\n                    num_key_value_heads=model.language_model.config.num_key_value_heads,\n                    num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                    head_dim=model.language_model.config.head_dim,\n                    layernorm_type=\"RMS\",\n                )\n            elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n\n                model = LlavaNextForConditionalGeneration.from_pretrained(\n                    model_name,\n                    torch_dtype=torch_dtype,\n                    device_map=device_map,\n                    attn_implementation=attn_implementation,\n                )\n                model_config = ModelConfig(\n                    residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                    residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                    intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                    residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                    head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                    head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                    attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                    attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                    attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                    attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                    mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                    last_layernorm_hook_name=\"language_model.model.norm.input\",\n                    attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                    attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                    embed_tokens=\"language_model.model.layers[0].input\",\n                    unembed_matrix=\"language_model.lm_head.weight\",\n                    last_layernorm=\"language_model.model.norm\",\n                    num_hidden_layers=model.language_model.config.num_hidden_layers,\n                    num_attention_heads=model.language_model.config.num_attention_heads,\n                    hidden_size=model.language_model.config.hidden_size,\n                    num_key_value_heads=model.language_model.config.num_key_value_heads,\n                    num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                    head_dim=model.language_model.config.head_dim,\n                    layernorm_type=\"RMS\",\n                )\n\n\n            elif model_name ==\"llava-hf/llava-onevision-qwen2-7b-ov-hf\":\n\n                model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n                    model_name,\n                    torch_dtype=torch_dtype,\n                    device_map=device_map,\n                    attn_implementation=attn_implementation,\n                )\n                model_config = ModelConfig(\n                    residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                    residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                    intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                    residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                    head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                    head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                    attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                    attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                    attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                    attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                    mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                    last_layernorm_hook_name=\"language_model.model.norm.input\",\n                    attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                    attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                    embed_tokens=\"language_model.model.layers[0].input\",\n                    unembed_matrix=\"language_model.lm_head.weight\",\n                    last_layernorm=\"language_model.model.norm\",\n                    num_hidden_layers=model.language_model.config.num_hidden_layers,\n                    num_attention_heads=model.language_model.config.num_attention_heads,\n                    hidden_size=model.language_model.config.hidden_size,\n                    num_key_value_heads=model.language_model.config.num_key_value_heads,\n                    num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                    head_dim=model.language_model.config.hidden_size // model.language_model.config.num_attention_heads,\n                    layernorm_type=\"RMS\",\n                )\n            else:\n                raise ValueError(\"Unsupported model_name\")\n            language_model = model.language_model\n\n        elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n            raise NotImplementedError(\"Emu3 model not implemented yet\")\n\n        elif model_name in [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\", \"google/gemma-3-12b-it\", \"google/gemma-3-27b-it\"]:\n            model = Gemma3ForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                last_layernorm_hook_name=\"language_model.model.norm.input\",\n                attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"language_model.model.layers[0].input\",\n                unembed_matrix=\"language_model.lm_head.weight\",\n                last_layernorm=\"language_model.model.norm\",\n                num_hidden_layers=model.language_model.config.num_hidden_layers,\n                num_attention_heads=model.language_model.config.num_attention_heads,\n                hidden_size=model.language_model.config.hidden_size,\n                num_key_value_heads=model.language_model.config.num_key_value_heads,\n                num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                head_dim=model.language_model.config.head_dim,\n                layernorm_type=\"RMS\",\n            )\n            language_model = model.language_model\n\n\n        elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n            model = LlamaForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"model.layers[{}].input\",\n                residual_stream_hook_name=\"model.layers[{}].output\",\n                intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n                head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n                head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n                head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n                attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_matrix_pre_softmax_hook_name=\"model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n                last_layernorm_hook_name=\"model.norm.input\",\n                attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"model.layers[0].input\",\n                unembed_matrix=\"lm_head.weight\",\n                last_layernorm=\"model.norm\",\n                num_hidden_layers=model.config.num_hidden_layers,\n                num_attention_heads=model.config.num_attention_heads,\n                hidden_size=model.config.hidden_size,\n                num_key_value_heads=model.config.num_key_value_heads,\n                num_key_value_groups=model.config.num_attention_heads\n                // model.config.num_key_value_heads,\n                head_dim=model.config.hidden_size // model.config.num_attention_heads,\n                layernorm_type=\"RMS\",\n            )\n\n        elif model_name in [\"CohereForAI/aya-101\"]:\n            model = T5ForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            language_model = None\n            raise NotImplementedError(\"CohereForAI model not implemented yet\")\n            # model_config = ModelFactory._create_model_config(\n            #     model.config, prefix=\"encoder.\"\n            # )\n\n        else:\n            raise ValueError(\"Unsupported model_name\")\n        return model, language_model, model_config\n\n    @staticmethod\n    def _create_model_config(**kwargs):\n        raise NotImplementedError(\"This method should be implemented in the if\")\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelFactory.load_model","title":"<code>load_model(model_name, attn_implementation, torch_dtype, device_map)</code>  <code>staticmethod</code>","text":"<p>Load the model and its configuration based on the model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>attn_implementation</code> <code>str</code> <p>Attention implementation type. (eager, flash-attn, sdp)</p> required <code>torch_dtype</code> <code>dtype</code> <p>Data type of the model.</p> required <code>device_map</code> <code>str</code> <p>Device map for the model.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>HuggingFaceModel</code> <p>Model instance.</p> <code>model_config</code> <code>ModelConfig</code> <p>Model configuration.</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>@staticmethod\ndef load_model(\n    model_name: str,\n    attn_implementation: str,\n    torch_dtype: torch.dtype,\n    device_map: str,\n) -&gt; Tuple[torch.nn.Module, Optional[torch.nn.Module], ModelConfig]:\n    r\"\"\"\n    Load the model and its configuration based on the model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        attn_implementation (str): Attention implementation type. (eager, flash-attn, sdp)\n        torch_dtype (torch.dtype): Data type of the model.\n        device_map (str): Device map for the model.\n\n    Returns:\n        model (HuggingFaceModel): Model instance.\n        model_config (ModelConfig): Model configuration.\n    \"\"\"\n    if attn_implementation != \"eager\":\n        logger.warning(\n            \"ModelFactory: Using an attention type different from eager or custom eager could have unexpected behavior in some experiments!\",\n        )\n\n    language_model = None\n    if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n        model = ChameleonForConditionalGeneration.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        model_config = ModelConfig(\n            residual_stream_input_hook_name=\"model.layers[{}].input\",\n            residual_stream_hook_name=\"model.layers[{}].output\",\n            intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n            residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n            head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n            head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n            head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n            attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n            attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n            attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n            mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n            attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n            attn_matrix_pre_softmax_hook_name=\"model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n            last_layernorm_hook_name=\"model.norm.input\",\n            attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n            attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n            embed_tokens=\"model.layers[0].input\",\n            unembed_matrix=\"lm_head.weight\",\n            last_layernorm=\"model.norm\",\n            num_hidden_layers=model.config.num_hidden_layers,\n            num_attention_heads=model.config.num_attention_heads,\n            hidden_size=model.config.hidden_size,\n            num_key_value_heads=model.config.num_key_value_heads,\n            num_key_value_groups=model.config.num_attention_heads\n            // model.config.num_key_value_heads,\n            head_dim=model.config.hidden_size // model.config.num_attention_heads,\n            layernorm_type=\"RMS\",\n        )\n\n    elif model_name in [ # TODO: Refactor this, bad nested if\n        \"mistral-community/pixtral-12b\",\n        \"llava-hf/llava-v1.6-mistral-7b-hf\",\n        \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n    ]:\n        if model_name == \"mistral-community/pixtral-12b\":\n            model = LlavaForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                last_layernorm_hook_name=\"language_model.model.norm.input\",\n                attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"language_model.model.layers[0].input\",\n                unembed_matrix=\"language_model.lm_head.weight\",\n                last_layernorm=\"language_model.model.norm\",\n                num_hidden_layers=model.language_model.config.num_hidden_layers,\n                num_attention_heads=model.language_model.config.num_attention_heads,\n                hidden_size=model.language_model.config.hidden_size,\n                num_key_value_heads=model.language_model.config.num_key_value_heads,\n                num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                head_dim=model.language_model.config.head_dim,\n                layernorm_type=\"RMS\",\n            )\n        elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n\n            model = LlavaNextForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                last_layernorm_hook_name=\"language_model.model.norm.input\",\n                attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"language_model.model.layers[0].input\",\n                unembed_matrix=\"language_model.lm_head.weight\",\n                last_layernorm=\"language_model.model.norm\",\n                num_hidden_layers=model.language_model.config.num_hidden_layers,\n                num_attention_heads=model.language_model.config.num_attention_heads,\n                hidden_size=model.language_model.config.hidden_size,\n                num_key_value_heads=model.language_model.config.num_key_value_heads,\n                num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                head_dim=model.language_model.config.head_dim,\n                layernorm_type=\"RMS\",\n            )\n\n\n        elif model_name ==\"llava-hf/llava-onevision-qwen2-7b-ov-hf\":\n\n            model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n                mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                last_layernorm_hook_name=\"language_model.model.norm.input\",\n                attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"language_model.model.layers[0].input\",\n                unembed_matrix=\"language_model.lm_head.weight\",\n                last_layernorm=\"language_model.model.norm\",\n                num_hidden_layers=model.language_model.config.num_hidden_layers,\n                num_attention_heads=model.language_model.config.num_attention_heads,\n                hidden_size=model.language_model.config.hidden_size,\n                num_key_value_heads=model.language_model.config.num_key_value_heads,\n                num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                head_dim=model.language_model.config.hidden_size // model.language_model.config.num_attention_heads,\n                layernorm_type=\"RMS\",\n            )\n        else:\n            raise ValueError(\"Unsupported model_name\")\n        language_model = model.language_model\n\n    elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n        raise NotImplementedError(\"Emu3 model not implemented yet\")\n\n    elif model_name in [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\", \"google/gemma-3-12b-it\", \"google/gemma-3-27b-it\"]:\n        model = Gemma3ForConditionalGeneration.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        model_config = ModelConfig(\n            residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n            residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n            intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n            residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n            head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n            head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n            head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n            attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n            attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n            attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n            attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n            attn_matrix_pre_softmax_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n            mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n            last_layernorm_hook_name=\"language_model.model.norm.input\",\n            attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n            attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n            embed_tokens=\"language_model.model.layers[0].input\",\n            unembed_matrix=\"language_model.lm_head.weight\",\n            last_layernorm=\"language_model.model.norm\",\n            num_hidden_layers=model.language_model.config.num_hidden_layers,\n            num_attention_heads=model.language_model.config.num_attention_heads,\n            hidden_size=model.language_model.config.hidden_size,\n            num_key_value_heads=model.language_model.config.num_key_value_heads,\n            num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n            head_dim=model.language_model.config.head_dim,\n            layernorm_type=\"RMS\",\n        )\n        language_model = model.language_model\n\n\n    elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n        model = LlamaForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        model_config = ModelConfig(\n            residual_stream_input_hook_name=\"model.layers[{}].input\",\n            residual_stream_hook_name=\"model.layers[{}].output\",\n            intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n            residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n            head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n            head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n            head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n            attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n            attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n            attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n            attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n            attn_matrix_pre_softmax_hook_name=\"model.layers[{}].self_attn.attention_matrix_pre_softmax_hook.output\",\n            mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n            last_layernorm_hook_name=\"model.norm.input\",\n            attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n            attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n            embed_tokens=\"model.layers[0].input\",\n            unembed_matrix=\"lm_head.weight\",\n            last_layernorm=\"model.norm\",\n            num_hidden_layers=model.config.num_hidden_layers,\n            num_attention_heads=model.config.num_attention_heads,\n            hidden_size=model.config.hidden_size,\n            num_key_value_heads=model.config.num_key_value_heads,\n            num_key_value_groups=model.config.num_attention_heads\n            // model.config.num_key_value_heads,\n            head_dim=model.config.hidden_size // model.config.num_attention_heads,\n            layernorm_type=\"RMS\",\n        )\n\n    elif model_name in [\"CohereForAI/aya-101\"]:\n        model = T5ForConditionalGeneration.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        language_model = None\n        raise NotImplementedError(\"CohereForAI model not implemented yet\")\n        # model_config = ModelFactory._create_model_config(\n        #     model.config, prefix=\"encoder.\"\n        # )\n\n    else:\n        raise ValueError(\"Unsupported model_name\")\n    return model, language_model, model_config\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.TokenizerFactory","title":"<code>TokenizerFactory</code>","text":"<p>This class return the right tokenizer for the model. If the model is multimodal return is_a_process == True</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>class TokenizerFactory:\n    r\"\"\"\n    This class return the right tokenizer for the model. If the model is multimodal return is_a_process == True\n    \"\"\"\n\n    @staticmethod\n    def load_tokenizer(model_name: str, torch_dtype: torch.dtype, device_map: str):\n        r\"\"\"\n        Load the tokenizer based on the model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            torch_dtype (torch.dtype): Data type of the model.\n            device_map (str): Device map for the model.\n\n        Returns:\n            processor (Tokenizer): Processor instance.\n            is_a_processor (bool): True if the model is multimodal, False otherwise.\n        \"\"\"\n        if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n            processor = ChameleonProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"meta-llama/Llama-3.2-1B\", \"meta-llama/Llama-3.2-3B\"]:\n            processor = LlamaTokenizerFast.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = False\n        elif model_name in [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\", \"google/gemma-3-12b-it\", \"google/gemma-3-27b-it\"]:\n            processor = Gemma3Processor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"mistral-community/pixtral-12b\"]:\n            processor = PixtralProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"llava-hf/llava-v1.6-mistral-7b-hf\"]:\n            processor = LlavaNextProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"llava-hf/llava-onevision-qwen2-7b-ov-hf\"]:\n            processor = LlavaOnevisionProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n            raise NotImplementedError(\"Emu3 model not implemented yet\")\n        elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n            processor = LlamaTokenizer.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = False\n        elif model_name in [\"CohereForAI/aya-101\"]:\n            processor = T5TokenizerFast.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = False\n\n        else:\n            raise ValueError(\"Unsupported model_name\")\n\n        return processor, is_a_processor\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.TokenizerFactory.load_tokenizer","title":"<code>load_tokenizer(model_name, torch_dtype, device_map)</code>  <code>staticmethod</code>","text":"<p>Load the tokenizer based on the model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>torch_dtype</code> <code>dtype</code> <p>Data type of the model.</p> required <code>device_map</code> <code>str</code> <p>Device map for the model.</p> required <p>Returns:</p> Name Type Description <code>processor</code> <code>Tokenizer</code> <p>Processor instance.</p> <code>is_a_processor</code> <code>bool</code> <p>True if the model is multimodal, False otherwise.</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>@staticmethod\ndef load_tokenizer(model_name: str, torch_dtype: torch.dtype, device_map: str):\n    r\"\"\"\n    Load the tokenizer based on the model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        torch_dtype (torch.dtype): Data type of the model.\n        device_map (str): Device map for the model.\n\n    Returns:\n        processor (Tokenizer): Processor instance.\n        is_a_processor (bool): True if the model is multimodal, False otherwise.\n    \"\"\"\n    if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n        processor = ChameleonProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"meta-llama/Llama-3.2-1B\", \"meta-llama/Llama-3.2-3B\"]:\n        processor = LlamaTokenizerFast.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = False\n    elif model_name in [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\", \"google/gemma-3-12b-it\", \"google/gemma-3-27b-it\"]:\n        processor = Gemma3Processor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"mistral-community/pixtral-12b\"]:\n        processor = PixtralProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"llava-hf/llava-v1.6-mistral-7b-hf\"]:\n        processor = LlavaNextProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"llava-hf/llava-onevision-qwen2-7b-ov-hf\"]:\n        processor = LlavaOnevisionProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n        raise NotImplementedError(\"Emu3 model not implemented yet\")\n    elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n        processor = LlamaTokenizer.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = False\n    elif model_name in [\"CohereForAI/aya-101\"]:\n        processor = T5TokenizerFast.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = False\n\n    else:\n        raise ValueError(\"Unsupported model_name\")\n\n    return processor, is_a_processor\n</code></pre>"},{"location":"api/interpretability/module_wrapper/","title":"Module Wrapper","text":""},{"location":"api/interpretability/module_wrapper/#introduction","title":"Introduction","text":"<p>Module Wrapper is the submodule that is responsible for managing the module wrappers. The module wrappers are essential to add custom hook where in the original transfomer codebase the hook is not available. For example, the <code>transformer</code> module does not have a hook to get the attention matrix of a head. The module wrapper is used to add this hook. The <code>module_wrapper</code>  submodel is composed of the following files:     - <code>manager.py</code>: The manager file is responsible for managing the module wrappers. It is the standard interface to add the wrap around models.     - <code>base.py</code>: The base file is the base class for the module wrapper. Implement a base form of a Wrapper class.     - <code>model_name_attention.py</code>: The model name attention file is the module wrapper for the attention matrix of a single model. When add a new model, add a new file with the name <code>model_name_attention.py</code> and implement the <code>ModelNameAttention</code> class. It is basically a copy of the forward pass of the attention module with the addition of the hook to get the attention matrix. </p>"},{"location":"api/interpretability/module_wrapper/#manager-wrappers-and-abstract-base-class","title":"Manager Wrappers and Abstract Base Class","text":""},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory","title":"<code>AttentionWrapperFactory</code>","text":"<p>Maps a given model name to the correct attention wrapper class.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>class AttentionWrapperFactory:\n    \"\"\"\n    Maps a given model name to the correct attention wrapper class.\n    \"\"\"\n\n    AVAILABLE_MODULE_WRAPPERS: dict = {\n        ChameleonAttentionWrapper.original_name(): ChameleonAttentionWrapper,\n        LlamaAttentionWrapper.original_name(): LlamaAttentionWrapper,\n        T5AttentionWrapper.original_name(): T5AttentionWrapper,\n        MistralAttentionWrapper.original_name(): MistralAttentionWrapper,\n        Gemma3AttentionWrapper.original_name(): Gemma3AttentionWrapper,\n        Qwen2AttentionWrapper.original_name(): Qwen2AttentionWrapper,\n\n    }\n\n    # MODEL_NAME_TO_WRAPPER = {\n    #     \"facebook/chameleon-7b\": ChameleonAttentionWrapper,\n    #     \"facebook/chameleon-30b\": ChameleonAttentionWrapper,\n    #     \"mistral-community/pixtral-12b\": LlamaAttentionWrapper,\n    #     \"llava-hf/llava-v1.6-mistral-7b-hf\": LlamaAttentionWrapper,\n    #     \"hf-internal-testing/tiny-random-LlamaForCausalLM\": LlamaAttentionWrapper,\n    #     \"ChoereForAI/aya-101\": T5AttentionWrapper,\n    # }\n\n    @staticmethod\n    def get_wrapper_classes(\n        model: nn.Module,\n    ) -&gt; Dict[\n        str,\n        Union[\n            Type[ChameleonAttentionWrapper],\n            Type[LlamaAttentionWrapper],\n            Type[T5AttentionWrapper],\n            Type[MistralAttentionWrapper],\n            Type[Gemma3AttentionWrapper],\n            Type[Qwen2AttentionWrapper]\n        ],\n    ]:\n        \"\"\"\n        Returns a dictionary mapping module names to their corresponding wrapper classes\n        for all supported modules found in the model.\n\n        Args:\n            model (nn.Module): The model to analyze\n\n        Returns:\n            Dict[str, Type]: Dictionary mapping original module names to wrapper classes\n        \"\"\"\n        all_modules = find_all_modules(model, return_only_names=True)\n        found_wrappers = {}\n\n        for (\n            candidate_name,\n            candidate_wrapper,\n        ) in AttentionWrapperFactory.AVAILABLE_MODULE_WRAPPERS.items():\n            if candidate_name in all_modules:\n                logger.info(f\"Found a wrapper for {candidate_name}\")\n                found_wrappers[candidate_name] = candidate_wrapper\n\n        if not found_wrappers:\n            logger.warning(f\"No wrappers found for any module in {model}\")\n\n        return found_wrappers\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory.get_wrapper_classes","title":"<code>get_wrapper_classes(model)</code>  <code>staticmethod</code>","text":"<p>Returns a dictionary mapping module names to their corresponding wrapper classes for all supported modules found in the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to analyze</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[Type[ChameleonAttentionWrapper], Type[LlamaAttentionWrapper], Type[T5AttentionWrapper], Type[MistralAttentionWrapper], Type[Gemma3AttentionWrapper], Type[Qwen2AttentionWrapper]]]</code> <p>Dict[str, Type]: Dictionary mapping original module names to wrapper classes</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>@staticmethod\ndef get_wrapper_classes(\n    model: nn.Module,\n) -&gt; Dict[\n    str,\n    Union[\n        Type[ChameleonAttentionWrapper],\n        Type[LlamaAttentionWrapper],\n        Type[T5AttentionWrapper],\n        Type[MistralAttentionWrapper],\n        Type[Gemma3AttentionWrapper],\n        Type[Qwen2AttentionWrapper]\n    ],\n]:\n    \"\"\"\n    Returns a dictionary mapping module names to their corresponding wrapper classes\n    for all supported modules found in the model.\n\n    Args:\n        model (nn.Module): The model to analyze\n\n    Returns:\n        Dict[str, Type]: Dictionary mapping original module names to wrapper classes\n    \"\"\"\n    all_modules = find_all_modules(model, return_only_names=True)\n    found_wrappers = {}\n\n    for (\n        candidate_name,\n        candidate_wrapper,\n    ) in AttentionWrapperFactory.AVAILABLE_MODULE_WRAPPERS.items():\n        if candidate_name in all_modules:\n            logger.info(f\"Found a wrapper for {candidate_name}\")\n            found_wrappers[candidate_name] = candidate_wrapper\n\n    if not found_wrappers:\n        logger.warning(f\"No wrappers found for any module in {model}\")\n\n    return found_wrappers\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager","title":"<code>ModuleWrapperManager</code>","text":"<p>Handles the logic of replacing original modules within a given model with custom wrappers. Supports multiple module types per model. Also allows restoring the original modules if needed.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>class ModuleWrapperManager:\n    \"\"\"\n    Handles the logic of replacing original modules within a given model\n    with custom wrappers. Supports multiple module types per model.\n    Also allows restoring the original modules if needed.\n    \"\"\"\n\n    def __init__(self, model: nn.Module, log_level: str = \"INFO\"):\n        \"\"\"\n        Initializes the manager with a given model.\n\n        Args:\n            model (nn.Module): The model whose modules will be wrapped\n            log_level (str): Logging level\n        \"\"\"\n        # Get all available wrappers for modules in this model\n        self.module_wrappers = AttentionWrapperFactory.get_wrapper_classes(model)\n\n        # Dictionary to store {module_path: {module_type: original_module}}\n        self.original_modules = {}\n\n        # Store target module names for quick lookup\n        self.target_module_names = list(self.module_wrappers.keys())\n\n    def __contains__(self, module_name: str) -&gt; bool:\n        \"\"\"\n        Check if a module name is supported by this manager.\n\n        Args:\n            module_name (str): The name of the module to check\n\n        Returns:\n            bool: True if the module is supported, False otherwise\n        \"\"\"\n        return module_name in self.target_module_names\n\n    def substitute_attention_module(self, model: nn.Module) -&gt; None:\n        \"\"\"\n        Public method that performs the substitution of all supported modules in the model.\n        Logs each replacement.\n\n        Args:\n            model (nn.Module): The model whose modules will be wrapped\n        \"\"\"\n        self._traverse_and_modify(model, parent_path=\"\", mode=\"substitute\")\n\n    def restore_original_attention_module(self, model: nn.Module) -&gt; None:\n        \"\"\"\n        Public method that restores the original modules in the model.\n        Logs each restoration.\n\n        Args:\n            model (nn.Module): The model whose modules will be restored\n        \"\"\"\n        self._traverse_and_modify(model, parent_path=\"\", mode=\"restore\")\n\n    def _traverse_and_modify(\n        self, module: nn.Module, parent_path: str, mode: str\n    ) -&gt; None:\n        \"\"\"\n        Recursively traverses `module` and either substitutes or restores each matching\n        submodule, depending on `mode`.\n\n        - mode=\"substitute\": Replaces original modules with wrappers\n        - mode=\"restore\": Replaces wrapper modules with their originals\n\n        Args:\n            module (nn.Module): The current module to inspect\n            parent_path (str): A string that tracks the 'path' of this submodule in the overall model hierarchy\n            mode (str): Either \"substitute\" or \"restore\"\n        \"\"\"\n        for name, child in list(module.named_children()):\n            # Identify the submodule path (e.g. \"encoder.layer.0.attention\")\n            submodule_path = f\"{parent_path}.{name}\" if parent_path else name\n\n            if mode == \"substitute\":\n                # Look for any matching original module class names\n                child_class_name = child.__class__.__name__\n                if child_class_name in self.target_module_names:\n                    # Get the appropriate wrapper for this module type\n                    wrapper_class = self.module_wrappers[child_class_name]\n\n                    # Store the original module\n                    if submodule_path not in self.original_modules:\n                        self.original_modules[submodule_path] = {}\n                    self.original_modules[submodule_path][child_class_name] = child\n\n                    # Wrap it\n                    wrapped_module = wrapper_class(child)\n                    setattr(module, name, wrapped_module)\n\n                    logger.debug(\n                        f\"Substituted '{submodule_path}' with wrapper for {child_class_name}.\"\n                    )\n                else:\n                    # Recurse\n                    self._traverse_and_modify(child, submodule_path, mode=\"substitute\")\n\n            elif mode == \"restore\":\n                # Check if this is any kind of wrapper we know about\n                child_class_name = child.__class__.__name__\n                for orig_name, wrapper_class in self.module_wrappers.items():\n                    if child_class_name == wrapper_class.__name__:\n                        # Found a wrapper, check if we have the original\n                        if (\n                            submodule_path in self.original_modules\n                            and orig_name in self.original_modules[submodule_path]\n                        ):\n                            # Restore the original module\n                            original_module = self.original_modules[submodule_path][\n                                orig_name\n                            ]\n                            setattr(module, name, original_module)\n                            logger.info(\n                                f\"Restored '{submodule_path}' to original {orig_name}.\"\n                            )\n                            break\n                else:\n                    # If not a wrapper or no original found, recurse\n                    self._traverse_and_modify(child, submodule_path, mode=\"restore\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.__contains__","title":"<code>__contains__(module_name)</code>","text":"<p>Check if a module name is supported by this manager.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The name of the module to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the module is supported, False otherwise</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def __contains__(self, module_name: str) -&gt; bool:\n    \"\"\"\n    Check if a module name is supported by this manager.\n\n    Args:\n        module_name (str): The name of the module to check\n\n    Returns:\n        bool: True if the module is supported, False otherwise\n    \"\"\"\n    return module_name in self.target_module_names\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.__init__","title":"<code>__init__(model, log_level='INFO')</code>","text":"<p>Initializes the manager with a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model whose modules will be wrapped</p> required <code>log_level</code> <code>str</code> <p>Logging level</p> <code>'INFO'</code> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def __init__(self, model: nn.Module, log_level: str = \"INFO\"):\n    \"\"\"\n    Initializes the manager with a given model.\n\n    Args:\n        model (nn.Module): The model whose modules will be wrapped\n        log_level (str): Logging level\n    \"\"\"\n    # Get all available wrappers for modules in this model\n    self.module_wrappers = AttentionWrapperFactory.get_wrapper_classes(model)\n\n    # Dictionary to store {module_path: {module_type: original_module}}\n    self.original_modules = {}\n\n    # Store target module names for quick lookup\n    self.target_module_names = list(self.module_wrappers.keys())\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.restore_original_attention_module","title":"<code>restore_original_attention_module(model)</code>","text":"<p>Public method that restores the original modules in the model. Logs each restoration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model whose modules will be restored</p> required Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def restore_original_attention_module(self, model: nn.Module) -&gt; None:\n    \"\"\"\n    Public method that restores the original modules in the model.\n    Logs each restoration.\n\n    Args:\n        model (nn.Module): The model whose modules will be restored\n    \"\"\"\n    self._traverse_and_modify(model, parent_path=\"\", mode=\"restore\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.substitute_attention_module","title":"<code>substitute_attention_module(model)</code>","text":"<p>Public method that performs the substitution of all supported modules in the model. Logs each replacement.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model whose modules will be wrapped</p> required Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def substitute_attention_module(self, model: nn.Module) -&gt; None:\n    \"\"\"\n    Public method that performs the substitution of all supported modules in the model.\n    Logs each replacement.\n\n    Args:\n        model (nn.Module): The model whose modules will be wrapped\n    \"\"\"\n    self._traverse_and_modify(model, parent_path=\"\", mode=\"substitute\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.AttentionMatrixHookModule","title":"<code>AttentionMatrixHookModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>Computation of the attention matrix. Note: it has been added just for adding custom hooks.</p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>class AttentionMatrixHookModule(nn.Module):\n    \"\"\"Computation of the attention matrix. *Note*: it has been added just for adding custom hooks.\"\"\"\n\n    def forward(\n            self,\n            attention_matrix: torch.Tensor,\n    ):\n        return attention_matrix\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper","title":"<code>BaseAttentionWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A base class for wrapping an original attention module.</p> Provides <p><code>_orig_module</code> to store the real (unwrapped) attention. A robust <code>__getattr__</code> that checks:     1) self.dict     2) self._modules     3) the base class     4) fallback to <code>_orig_module</code></p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>class BaseAttentionWrapper(nn.Module):\n    \"\"\"\n    A base class for wrapping an original attention module.\n\n    Provides:\n        `_orig_module` to store the real (unwrapped) attention.\n        A robust `__getattr__` that checks:\n            1) self.__dict__\n            2) self._modules\n            3) the base class\n            4) fallback to `_orig_module`\n    \"\"\"\n\n    def __init__(self, original_module: nn.Module):\n        super().__init__()\n        # store the original module in a private attribute\n        object.__setattr__(self, \"_orig_module\", original_module)\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        If name is not in this wrapper, fall back to the original module.\n        Also checks `self._modules` for submodules, because PyTorch\n        automatically places them there.\n        \"\"\"\n        # 1) get this wrapper's __dict__\n        wrapper_dict = object.__getattribute__(self, \"__dict__\")\n\n        # 2) if name is in our own instance dictionary, return it\n        if name in wrapper_dict:\n            return wrapper_dict[name]\n\n        # 3) if name is in our submodules, return it\n        modules_dict = wrapper_dict[\"_modules\"]\n        if name in modules_dict:\n            return modules_dict[name]\n\n        # 4) check if name is in our class (methods, etc.)\n        cls = object.__getattribute__(self, \"__class__\")\n        if hasattr(cls, name):\n            return getattr(cls, name)\n\n        # 5) fallback to _orig_module\n        orig = wrapper_dict[\"_orig_module\"]\n        return getattr(orig, name)\n\n    @staticmethod\n    def original_name() -&gt; str:\n        \"\"\"\n        By default, you might override this in each derived class if you want\n        your manager code to know which original class name this wrapper replaces.\n        \"\"\"\n        return \"BaseAttention\"\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>If name is not in this wrapper, fall back to the original module. Also checks <code>self._modules</code> for submodules, because PyTorch automatically places them there.</p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>def __getattr__(self, name: str):\n    \"\"\"\n    If name is not in this wrapper, fall back to the original module.\n    Also checks `self._modules` for submodules, because PyTorch\n    automatically places them there.\n    \"\"\"\n    # 1) get this wrapper's __dict__\n    wrapper_dict = object.__getattribute__(self, \"__dict__\")\n\n    # 2) if name is in our own instance dictionary, return it\n    if name in wrapper_dict:\n        return wrapper_dict[name]\n\n    # 3) if name is in our submodules, return it\n    modules_dict = wrapper_dict[\"_modules\"]\n    if name in modules_dict:\n        return modules_dict[name]\n\n    # 4) check if name is in our class (methods, etc.)\n    cls = object.__getattribute__(self, \"__class__\")\n    if hasattr(cls, name):\n        return getattr(cls, name)\n\n    # 5) fallback to _orig_module\n    orig = wrapper_dict[\"_orig_module\"]\n    return getattr(orig, name)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.original_name","title":"<code>original_name()</code>  <code>staticmethod</code>","text":"<p>By default, you might override this in each derived class if you want your manager code to know which original class name this wrapper replaces.</p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>@staticmethod\ndef original_name() -&gt; str:\n    \"\"\"\n    By default, you might override this in each derived class if you want\n    your manager code to know which original class name this wrapper replaces.\n    \"\"\"\n    return \"BaseAttention\"\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#specific-module-wrappers","title":"Specific Module Wrappers","text":""},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper","title":"<code>LlamaAttentionWrapper</code>","text":"<p>               Bases: <code>BaseAttentionWrapper</code></p> <p>A wrapper around the original LlamaAttention. It has: - The same named attributes (q_proj, k_proj, etc.), which are references     to the original module's submodules/parameters. - A private reference (<code>_orig_attn</code>) to the entire original attention,     for falling back if something isn't found on the wrapper itself. - An additional <code>attention_matrix_hook</code> for intercepting attention.</p> Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code> <pre><code>class LlamaAttentionWrapper(BaseAttentionWrapper):\n    \"\"\"\n    A wrapper around the original LlamaAttention. It has:\n    - The same named attributes (q_proj, k_proj, etc.), which are references\n        to the original module's submodules/parameters.\n    - A private reference (`_orig_attn`) to the entire original attention,\n        for falling back if something isn't found on the wrapper itself.\n    - An additional `attention_matrix_hook` for intercepting attention.\n    \"\"\"\n\n    @staticmethod\n    def original_name():\n        return \"LlamaAttention\"\n\n    def __init__(self, original_attention: nn.Module):\n        \"\"\"\n        Store references to all relevant submodules so the wrapper\n        \"feels\" the same. Also store a reference to the original module\n        in a private attribute for fallback.\n        \"\"\"\n        super().__init__(original_attention)\n\n        # This is the private reference to the entire original attention.\n        # We'll fallback to it for any attribute we haven't explicitly set.\n        object.__setattr__(self, \"_orig_attn\", original_attention)\n\n        # Now replicate the original attention's submodules as attributes of *this* wrapper.\n        # These are direct references, not new modules:\n        self.q_proj = original_attention.q_proj\n        self.k_proj = original_attention.k_proj\n        self.v_proj = original_attention.v_proj\n        self.o_proj = original_attention.o_proj\n\n        # Copy over any scalar attributes you need\n        # self.num_heads = original_attention.num_heads\n        # self.num_key_value_heads = original_attention.num_key_value_heads\n        # self.num_key_value_groups = original_attention.num_key_value_groups\n        self.head_dim = original_attention.head_dim\n        # self.hidden_size = original_attention.hidden_size\n        self.attention_dropout = original_attention.attention_dropout\n        self.layer_idx = original_attention.layer_idx\n        self.config = original_attention.config\n\n        # Add your custom hook module\n        self.attention_matrix_pre_softmax_hook = AttentionMatrixHookModule()\n        self.attention_matrix_hook = AttentionMatrixHookModule()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n        attention_mask: Optional[torch.Tensor],\n        past_key_value: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[FlashAttentionKwargs],\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        input_shape = hidden_states.shape[:-1]\n        hidden_shape = (*input_shape, -1, self.head_dim)\n\n        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n\n        cos, sin = position_embeddings\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(\n                key_states, value_states, self.layer_idx, cache_kwargs\n            )\n\n        # Inline eager_attention_forward logic\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n\n        if attention_mask is not None:\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # Pre-softmax hook\n        attn_weights = self.attention_matrix_pre_softmax_hook(attn_weights)\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = self.attention_matrix_hook(attn_weights)\n        attn_weights = nn.functional.dropout(\n            attn_weights,\n            p=0.0 if not self.training else self.attention_dropout,\n            training=self.training,\n        )\n\n        attn_output = torch.matmul(attn_weights, value_states).transpose(1, 2).contiguous()\n        # End inline\n\n        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n        return attn_output, attn_weights # type: ignore\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper.__init__","title":"<code>__init__(original_attention)</code>","text":"<p>Store references to all relevant submodules so the wrapper \"feels\" the same. Also store a reference to the original module in a private attribute for fallback.</p> Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code> <pre><code>def __init__(self, original_attention: nn.Module):\n    \"\"\"\n    Store references to all relevant submodules so the wrapper\n    \"feels\" the same. Also store a reference to the original module\n    in a private attribute for fallback.\n    \"\"\"\n    super().__init__(original_attention)\n\n    # This is the private reference to the entire original attention.\n    # We'll fallback to it for any attribute we haven't explicitly set.\n    object.__setattr__(self, \"_orig_attn\", original_attention)\n\n    # Now replicate the original attention's submodules as attributes of *this* wrapper.\n    # These are direct references, not new modules:\n    self.q_proj = original_attention.q_proj\n    self.k_proj = original_attention.k_proj\n    self.v_proj = original_attention.v_proj\n    self.o_proj = original_attention.o_proj\n\n    # Copy over any scalar attributes you need\n    # self.num_heads = original_attention.num_heads\n    # self.num_key_value_heads = original_attention.num_key_value_heads\n    # self.num_key_value_groups = original_attention.num_key_value_groups\n    self.head_dim = original_attention.head_dim\n    # self.hidden_size = original_attention.hidden_size\n    self.attention_dropout = original_attention.attention_dropout\n    self.layer_idx = original_attention.layer_idx\n    self.config = original_attention.config\n\n    # Add your custom hook module\n    self.attention_matrix_pre_softmax_hook = AttentionMatrixHookModule()\n    self.attention_matrix_hook = AttentionMatrixHookModule()\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.llama_attention.repeat_kv","title":"<code>repeat_kv(hidden_states, n_rep)</code>","text":"<p>(batch, num_key_value_heads, seq_len, head_dim)     -&gt; (batch, num_attention_heads, seq_len, head_dim)</p> Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code> <pre><code>def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"\n    (batch, num_key_value_heads, seq_len, head_dim)\n        -&gt; (batch, num_attention_heads, seq_len, head_dim)\n    \"\"\"\n    bsz, num_kv_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(\n        bsz, num_kv_heads, n_rep, slen, head_dim\n    )\n    return hidden_states.reshape(bsz, num_kv_heads * n_rep, slen, head_dim)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.ChameleonAttentionWrapper","title":"<code>ChameleonAttentionWrapper</code>","text":"<p>               Bases: <code>BaseAttentionWrapper</code></p> <p>Attention wrapper for the Chameleon model.</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>class ChameleonAttentionWrapper(BaseAttentionWrapper):\n    \"\"\"\n    Attention wrapper for the Chameleon model.\n    \"\"\"\n\n    @staticmethod\n    def original_name():\n        return \"ChameleonAttention\"\n\n    def __init__(self, original_attention: nn.Module):\n        super().__init__(original_attention)\n\n        self.q_proj = original_attention.q_proj\n        self.k_proj = original_attention.k_proj\n        self.v_proj = original_attention.v_proj\n        self.q_norm = original_attention.q_norm\n        self.k_norm = original_attention.k_norm\n        self.o_proj = original_attention.o_proj\n        # self.softmax = original_attention.softmax\n        self.attention_dropout = original_attention.attention_dropout\n        self.training = original_attention.training\n        self.layer_idx = original_attention.layer_idx\n        self.num_heads = original_attention.num_heads\n        self.num_key_value_heads = original_attention.num_key_value_heads\n        self.num_key_value_groups = original_attention.num_key_value_groups\n        self.head_dim = original_attention.head_dim\n        self.hidden_size = original_attention.hidden_size\n        self.rotary_emb = original_attention.rotary_emb\n\n        self.attention_matrix_pre_softmax_hook = AttentionMatrixHookModule()        \n        self.attention_matrix_hook = AttentionMatrixHookModule()\n\n        self.original_attention = original_attention\n\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n        query_states = self.q_norm(query_states)\n\n        key_states = key_states.reshape(-1, self.num_key_value_heads, self.head_dim)\n        key_states = self.k_norm(key_states)\n\n        query_states = query_states.reshape(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        # attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n        attn_weights = self.attention_matrix_pre_softmax_hook(attn_weights)\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_weights = self.attention_matrix_hook(attn_weights)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value # type: ignore\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1)</code>","text":"<p>Applies Rotary Position Embedding to the query and key tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>`torch.Tensor`</code> <p>The query tensor.</p> required <code>k</code> <code>`torch.Tensor`</code> <p>The key tensor.</p> required <code>cos</code> <code>`torch.Tensor`</code> <p>The cosine part of the rotary embedding.</p> required <code>sin</code> <code>`torch.Tensor`</code> <p>The sine part of the rotary embedding.</p> required <code>position_ids</code> <code>`torch.Tensor`, *optional*</code> <p>Deprecated and unused.</p> <code>None</code> <code>unsqueeze_dim</code> <code>`int`, *optional*, defaults to 1</code> <p>The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.</p> <code>1</code> <p>Returns:     <code>tuple(torch.Tensor)</code> comprising of the query and key tensors rotated using the Rotary Position Embedding.</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>def apply_rotary_pos_emb( q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.repeat_kv","title":"<code>repeat_kv(hidden_states, n_rep)</code>","text":"<p>This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.rotate_half","title":"<code>rotate_half(x)</code>","text":"<p>Rotates half the hidden dims of the input.</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper","title":"<code>T5AttentionWrapper</code>","text":"<p>               Bases: <code>BaseAttentionWrapper</code></p> Source code in <code>easyroutine/interpretability/module_wrappers/T5_attention.py</code> <pre><code>class T5AttentionWrapper(BaseAttentionWrapper):\n    @staticmethod\n    def original_name() -&gt; str:\n        return \"T5Attention\"\n\n    def __init__(self, original_attention: nn.Module):\n        super().__init__(original_attention)\n        self.q = original_attention.q\n        self.k = original_attention.k\n        self.v = original_attention.v\n        self.o = original_attention.o\n        self.dropout = original_attention.dropout\n        self.layer_idx = original_attention.layer_idx\n        self.n_heads = original_attention.n_heads\n        self.key_value_proj_dim = original_attention.key_value_proj_dim\n        self.inner_dim = original_attention.inner_dim\n        self.has_relative_attention_bias = (\n            original_attention.has_relative_attention_bias\n        )\n        self.pruned_heads = original_attention.pruned_heads\n        self.attention_matrix_pre_softmax_hook = AttentionMatrixHookModule()\n        self.attention_matrix_hook = AttentionMatrixHookModule()\n        self.original_attention = original_attention\n\n    def forward(\n        self,\n        hidden_states,\n        mask=None,\n        key_value_states=None,\n        position_bias=None,\n        past_key_value=None,\n        layer_head_mask=None,\n        query_length=None,\n        use_cache=False,\n        output_attentions=False,\n        cache_position=None,\n    ):\n        \"\"\"\n        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n        \"\"\"\n        # Input is (batch_size, seq_length, dim)\n        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n        is_cross_attention = key_value_states is not None\n\n        query_states = self.q(hidden_states)\n        query_states = query_states.view(\n            batch_size, -1, self.n_heads, self.key_value_proj_dim\n        ).transpose(1, 2)\n\n        if past_key_value is not None:\n            is_updated = past_key_value.is_updated.get(self.layer_idx)\n            if is_cross_attention:\n                # after the first generated id, we can subsequently re-use all key/value_states from cache\n                curr_past_key_value = past_key_value.cross_attention_cache\n            else:\n                curr_past_key_value = past_key_value.self_attention_cache\n\n        current_states = key_value_states if is_cross_attention else hidden_states\n        if is_cross_attention and past_key_value is not None and is_updated:  # type: ignore\n            # reuse k,v, cross_attentions\n            key_states = curr_past_key_value.key_cache[self.layer_idx]  # type: ignore\n            value_states = curr_past_key_value.value_cache[self.layer_idx]  # type: ignore\n        else:\n            key_states = self.k(current_states)\n            value_states = self.v(current_states)\n            key_states = key_states.view(\n                batch_size, -1, self.n_heads, self.key_value_proj_dim\n            ).transpose(1, 2)\n            value_states = value_states.view(\n                batch_size, -1, self.n_heads, self.key_value_proj_dim\n            ).transpose(1, 2)\n\n            if past_key_value is not None:\n                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                cache_position = cache_position if not is_cross_attention else None\n                key_states, value_states = curr_past_key_value.update(  # type: ignore\n                    key_states,\n                    value_states,\n                    self.layer_idx,\n                    {\"cache_position\": cache_position},\n                )\n                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                if is_cross_attention:\n                    past_key_value.is_updated[self.layer_idx] = True\n\n        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd-&gt;bnqk\", query_states, key_states), compatible with onnx op&gt;9\n        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n\n        if position_bias is None:\n            key_length = key_states.shape[-2]\n            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n            real_seq_length = (\n                query_length if query_length is not None else cache_position[-1] + 1 # type: ignore\n            )  # type: ignore\n            if not self.has_relative_attention_bias:\n                position_bias = torch.zeros(\n                    (1, self.n_heads, seq_length, key_length),\n                    device=scores.device,\n                    dtype=scores.dtype,\n                )\n                if self.gradient_checkpointing and self.training:\n                    position_bias.requires_grad = True\n            else:\n                position_bias = self.compute_bias(\n                    real_seq_length,\n                    key_length,\n                    device=scores.device,\n                    cache_position=cache_position,\n                )\n                position_bias = position_bias[:, :, -seq_length:, :]\n\n            if mask is not None:\n                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                position_bias = position_bias + causal_mask\n\n        if self.pruned_heads:\n            mask = torch.ones(position_bias.shape[1])\n            mask[list(self.pruned_heads)] = 0\n            position_bias_masked = position_bias[:, mask.bool()]\n        else:\n            position_bias_masked = position_bias\n\n        scores += position_bias_masked\n\n        # (batch_size, n_heads, seq_length, key_length)\n        attn_weights = self.attention_matrix_pre_softmax_hook(scores)\n        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n        attn_weights = self.attention_matrix_hook(attn_weights)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.dropout, training=self.training\n        )\n\n        # Mask heads if we want to\n        if layer_head_mask is not None:\n            attn_weights = attn_weights * layer_head_mask\n\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n        attn_output = self.o(attn_output)\n\n        outputs = (attn_output, past_key_value, position_bias)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n        return outputs\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper.forward","title":"<code>forward(hidden_states, mask=None, key_value_states=None, position_bias=None, past_key_value=None, layer_head_mask=None, query_length=None, use_cache=False, output_attentions=False, cache_position=None)</code>","text":"<p>Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).</p> Source code in <code>easyroutine/interpretability/module_wrappers/T5_attention.py</code> <pre><code>def forward(\n    self,\n    hidden_states,\n    mask=None,\n    key_value_states=None,\n    position_bias=None,\n    past_key_value=None,\n    layer_head_mask=None,\n    query_length=None,\n    use_cache=False,\n    output_attentions=False,\n    cache_position=None,\n):\n    \"\"\"\n    Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n    \"\"\"\n    # Input is (batch_size, seq_length, dim)\n    # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n    batch_size, seq_length = hidden_states.shape[:2]\n\n    # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n    is_cross_attention = key_value_states is not None\n\n    query_states = self.q(hidden_states)\n    query_states = query_states.view(\n        batch_size, -1, self.n_heads, self.key_value_proj_dim\n    ).transpose(1, 2)\n\n    if past_key_value is not None:\n        is_updated = past_key_value.is_updated.get(self.layer_idx)\n        if is_cross_attention:\n            # after the first generated id, we can subsequently re-use all key/value_states from cache\n            curr_past_key_value = past_key_value.cross_attention_cache\n        else:\n            curr_past_key_value = past_key_value.self_attention_cache\n\n    current_states = key_value_states if is_cross_attention else hidden_states\n    if is_cross_attention and past_key_value is not None and is_updated:  # type: ignore\n        # reuse k,v, cross_attentions\n        key_states = curr_past_key_value.key_cache[self.layer_idx]  # type: ignore\n        value_states = curr_past_key_value.value_cache[self.layer_idx]  # type: ignore\n    else:\n        key_states = self.k(current_states)\n        value_states = self.v(current_states)\n        key_states = key_states.view(\n            batch_size, -1, self.n_heads, self.key_value_proj_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            batch_size, -1, self.n_heads, self.key_value_proj_dim\n        ).transpose(1, 2)\n\n        if past_key_value is not None:\n            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n            cache_position = cache_position if not is_cross_attention else None\n            key_states, value_states = curr_past_key_value.update(  # type: ignore\n                key_states,\n                value_states,\n                self.layer_idx,\n                {\"cache_position\": cache_position},\n            )\n            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n            if is_cross_attention:\n                past_key_value.is_updated[self.layer_idx] = True\n\n    # compute scores, equivalent of torch.einsum(\"bnqd,bnkd-&gt;bnqk\", query_states, key_states), compatible with onnx op&gt;9\n    scores = torch.matmul(query_states, key_states.transpose(3, 2))\n\n    if position_bias is None:\n        key_length = key_states.shape[-2]\n        # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n        real_seq_length = (\n            query_length if query_length is not None else cache_position[-1] + 1 # type: ignore\n        )  # type: ignore\n        if not self.has_relative_attention_bias:\n            position_bias = torch.zeros(\n                (1, self.n_heads, seq_length, key_length),\n                device=scores.device,\n                dtype=scores.dtype,\n            )\n            if self.gradient_checkpointing and self.training:\n                position_bias.requires_grad = True\n        else:\n            position_bias = self.compute_bias(\n                real_seq_length,\n                key_length,\n                device=scores.device,\n                cache_position=cache_position,\n            )\n            position_bias = position_bias[:, :, -seq_length:, :]\n\n        if mask is not None:\n            causal_mask = mask[:, :, :, : key_states.shape[-2]]\n            position_bias = position_bias + causal_mask\n\n    if self.pruned_heads:\n        mask = torch.ones(position_bias.shape[1])\n        mask[list(self.pruned_heads)] = 0\n        position_bias_masked = position_bias[:, mask.bool()]\n    else:\n        position_bias_masked = position_bias\n\n    scores += position_bias_masked\n\n    # (batch_size, n_heads, seq_length, key_length)\n    attn_weights = self.attention_matrix_pre_softmax_hook(scores)\n    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n    attn_weights = self.attention_matrix_hook(attn_weights)\n    attn_weights = nn.functional.dropout(\n        attn_weights, p=self.dropout, training=self.training\n    )\n\n    # Mask heads if we want to\n    if layer_head_mask is not None:\n        attn_weights = attn_weights * layer_head_mask\n\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n    attn_output = self.o(attn_output)\n\n    outputs = (attn_output, past_key_value, position_bias)\n\n    if output_attentions:\n        outputs = outputs + (attn_weights,)\n    return outputs\n</code></pre>"},{"location":"api/interpretability/token_index/","title":"Token index","text":""},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex","title":"<code>TokenIndex</code>","text":"<p>TokenIndex is one of the core class of the interpretability module. It is used to find the right indexes that correspond to the tokens in the input of the model. In this way we are able to extract the right hidden states and attention weights, based on the tokens we are interested in. It support mixed modalities inputs, with both text and images.</p> Source code in <code>easyroutine/interpretability/token_index.py</code> <pre><code>class TokenIndex:\n    r\"\"\"\n    TokenIndex is one of the core class of the interpretability module.\n    It is used to find the right indexes that correspond to the tokens in the input of the model.\n    In this way we are able to extract the right hidden states and attention weights, based on the tokens we are interested in.\n    It support mixed modalities inputs, with both text and images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        pivot_positions: Optional[List[int]] = None,\n        pivot_tokens: Optional[List[str]] = None,\n    ):\n        r\"\"\"\n        Args:\n            model_name: str (required): the name of the model\n            pivot_positions: List[int] (optional): a list of integers that represent the positions where to split the tokens.\n            pivot_tokens: List[str] (optional): a list of strings that represent the tokens where to split the tokens.\n\n\n        The pivot_positions and pivot_tokens are mutually exclusive.\n        The idea of the split is the following. Immagine to have an input string of tokens like this: [\"I\", \"love\", \"cats\", \"and\", \"dogs\". \"What\", \"about\", \"you?\"]\n        Then, i want to extract/ablate/intervene on the second sentence. I can do it by specifying the pivot_positions=[5] or pivot_tokens=[\"What\"].\n        In this way, the tokens will be split in two groups: [\"I\", \"love\", \"cats\", \"and\"] and [\"dogs\", \"What\", \"about\", \"you?\"] with names \"inputs-partition-0\" and \"inputs-partition-1\".\n        \"\"\"\n        self.model_name = model_name\n        self.pivot_tokens = pivot_tokens\n        self.pivot_positions = sorted(pivot_positions) if pivot_positions else []\n\n    def find_occurrences(self, lst: List[str], target: str) -&gt; List[int]:\n        return [i for i, x in enumerate(lst) if x == target]\n\n    def categorize_tokens(\n        self,\n        string_tokens: List[str],\n        include_delimiters_in_image: Optional[bool] = None,\n    ) -&gt; Dict[str, List[int]]:\n        if self.model_name not in SUPPORTED_MODELS:\n            raise ValueError(\"Unsupported model_name\")\n\n        # Unpack model tokens:\n        # start_image_token: token marking the beginning of an image section\n        # special: special tokens or sequences that might occur inside an image section\n        # end_image_token: token marking the end of an image section\n        start_image_token, special, end_image_token = SUPPORTED_MODELS[self.model_name]\n\n        # Convert special to list if it's a single token\n        if isinstance(special, str):\n            special = [special]\n        # Convert special tokens to list of lists for uniform processing\n        if special is None:\n            special_sequences = []\n        else:\n            special_sequences = [s if isinstance(s, list) else [s] for s in special]\n\n        image_start_tokens = []\n        image_end_tokens = []\n        image_tokens = []\n        last_line_image_tokens = []\n        text_tokens = []\n        special_tokens = []\n\n        # Case 1: When the start and end markers differ.\n        if start_image_token is None or start_image_token != end_image_token:\n            in_image_sequence = False\n            i = 0\n            while i &lt; len(string_tokens):\n                token = string_tokens[i]\n                if token == start_image_token and not in_image_sequence:\n                    in_image_sequence = True\n                    image_start_tokens.append(i)\n                    i += 1\n                    continue\n\n                if in_image_sequence and token == end_image_token:\n                    in_image_sequence = False\n                    image_end_tokens.append(i)\n                    last_line_image_tokens.append(i - 1)\n                    i += 1\n                    continue\n\n                if in_image_sequence:\n                    # Check for special token sequences\n                    sequence_found = False\n                    for seq in special_sequences:\n                        if i + len(seq) &lt;= len(string_tokens):\n                            # Check if the current position matches the sequence\n                            if string_tokens[i : i + len(seq)] == seq:\n                                special_tokens.extend(range(i, i + len(seq)))\n                                i += len(seq)\n                                sequence_found = True\n                                break\n                    if sequence_found:\n                        continue\n\n                    image_tokens.append(i)\n                else:\n                    text_tokens.append(i)\n                i += 1\n        # Case 2: When the start and end markers are identical.\n        else:\n            # Collect all indices where the marker appears.\n            marker_indices = [\n                i for i, token in enumerate(string_tokens) if token == start_image_token\n            ]\n            if marker_indices:\n                # Always record the first marker as image_start and the last as image_end.\n                image_start_tokens.append(marker_indices[0])\n                image_end_tokens.append(marker_indices[-1])\n                # If no explicit flag was provided, inspect the content between the markers:\n                if include_delimiters_in_image is None:\n                    # If any token between the first and last marker is not the marker itself,\n                    # we assume the markers are used as delimiters.\n                    if any(\n                        string_tokens[i] != start_image_token\n                        for i in range(marker_indices[0] + 1, marker_indices[-1])\n                    ):\n                        include_delimiters_in_image = False\n                    else:\n                        include_delimiters_in_image = True\n\n                if include_delimiters_in_image:\n                    # If markers are part of the image content, include every occurrence.\n                    image_tokens.extend(marker_indices)\n                else:\n                    # Otherwise, consider only the tokens strictly between the first and last markers.\n                    if len(marker_indices) &gt; 2:\n                        image_tokens.extend(marker_indices[1:-1])\n                    # (If there are exactly two markers, there is no \u201cinternal\u201d image token.)\n                # For this branch, define last_line_image as the position immediately before the last marker.\n                last_line_image_tokens.append(marker_indices[-1] - 1)\n\n            # Classify all tokens that are not the marker as text.\n            for i, token in enumerate(string_tokens):\n                if token != start_image_token:\n                    text_tokens.append(i)\n                # Optionally, if special tokens may occur outside the image region:\n                if special and token in special:\n                    special_tokens.append(i)\n\n        tokens_group, positions_group = self.group_tokens(string_tokens)\n        position_dict = {\n            f\"inputs-partition-{i}\": positions_group[i] for i in positions_group\n        }\n\n        return {\n            \"image_start\": image_start_tokens,\n            \"image_end\": image_end_tokens,\n            \"image\": image_tokens,\n            \"last_line_image\": last_line_image_tokens,\n            \"text\": text_tokens,\n            \"special\": special_tokens,\n            \"all\": list(range(len(string_tokens))),\n            **position_dict,\n        }\n\n    def group_tokens(\n        self, string_tokens: List[str]\n    ) -&gt; Tuple[Dict[int, List[str]], Dict[int, List[int]]]:\n        if self.pivot_tokens:\n            return self.group_tokens_by_pivot_tokens(string_tokens)\n        elif self.pivot_positions:\n            return self.group_tokens_by_positions(string_tokens)\n        else:\n            return {0: string_tokens}, {0: list(range(len(string_tokens)))}\n\n    def group_tokens_by_positions(\n        self, string_tokens: List[str]\n    ) -&gt; Tuple[Dict[int, List[str]], Dict[int, List[int]]]:\n        tokens_group, positions_group = {}, {}\n        for i, pos in enumerate(self.pivot_positions):\n            if i == 0:\n                positions_group[i] = [0, pos]\n            else:\n                positions_group[i] = [self.pivot_positions[i - 1], pos]\n        positions_group[len(self.pivot_positions)] = [\n            self.pivot_positions[-1],\n            len(string_tokens),\n        ]\n\n        # modify the positions_group to include all the indexes and not just the start and end\n        for i in range(len(positions_group)):\n            positions_group[i] = list(\n                range(positions_group[i][0], positions_group[i][1])\n            )\n\n        for i, group in positions_group.items():\n            tokens_group[i] = [string_tokens[j] for j in group]\n        return tokens_group, positions_group\n\n    def group_tokens_by_pivot_tokens(\n        self, string_tokens: List[str]\n    ) -&gt; Tuple[Dict[int, List[str]], Dict[int, List[int]]]:\n        tokens_group, positions_group = {}, {}\n        current_group = 0\n        start_pos = 0\n\n        for i, token in enumerate(string_tokens):\n            if isinstance(self.pivot_tokens, list) and token in self.pivot_tokens:\n                positions_group[current_group] = [start_pos, i]\n                tokens_group[current_group] = string_tokens[start_pos:i]\n                current_group += 1\n                start_pos = i + 1\n\n        positions_group[current_group] = [start_pos, len(string_tokens)]\n        tokens_group[current_group] = string_tokens[start_pos:]\n\n        return tokens_group, positions_group\n\n    def get_token_index(\n        self,\n        tokens: Union[\n            List[Union[str, int, Tuple[int, int]]],\n            List[str],\n            List[int],\n            List[Tuple[int, int]],\n        ],\n        string_tokens: List[str],\n        return_type: Literal[\"list\", \"dict\", \"all\"] = \"list\",\n    ) -&gt; Union[List[int], Dict, Tuple[List[int], Dict]]:\n        \"\"\"Get indices for specified tokens in the input sequence.\n\n        Arguments:\n            tokens: list of tokens to look up. Can be:\n                - String token identifiers (see Supported Tokens below)\n                - Integer indices (direct position access)\n                - Tuple[int, int] for slices (start, end)\n            string_tokens: Input token sequence to search in\n            return_type: Output format:\n                - \"list\": Returns [int, ...]\n                - \"dict\": Returns {\"token_type\": [indices]}\n                - \"all\": Returns ([indices], {token_type: [indices]})\n\n        Returns:\n            Based on return_type, provides token indices in specified format\n\n        Token Types:\n            1. Direct Access:\n                - Integer index: Direct position (e.g. 5)\n                - Slice tuple: Range of positions (e.g. (2,5))\n                - Negative index: From end (e.g. -1)\n\n            2. Named Positions:\n                - \"last\": Last token\n                - \"last-2\": Second to last\n                - \"last-4\": Fourth to last\n\n            3. Text Tokens:\n                - \"all-text\": All text positions\n                - \"random-text\": Single random text position\n                - \"random-text-N\": N random text positions\n\n            4. Image Tokens:\n                - \"all-image\": All image positions\n                - \"last-image\": Last image position\n                - \"end-image\": Image end marker\n                - \"random-image\": Random image position\n                - \"random-image-N\": N random image positions\n\n            5. Special:\n                - \"special\": Model-specific tokens\n                - \"all\": Complete sequence\n                - \"special-pixtral\": Model-specific fixed positions\n                - \"inputs-partition-N\": Nth token group\n                - \"random-inputs-partition-N\": Random from group N\n\n        Examples:\n            # Direct index access\n            &gt;&gt;&gt; get_token_index([5], tokens)               # Single position\n            [5]\n            &gt;&gt;&gt; get_token_index([(2,5)], tokens)          # Slice range\n            [2,3,4]\n            &gt;&gt;&gt; get_token_index([-1], tokens)             # From end\n            [9]\n\n            # Named tokens\n            &gt;&gt;&gt; get_token_index([\"last\"], tokens)         # Last token\n            [9]\n            &gt;&gt;&gt; get_token_index([\"all-text\"], tokens)     # All text\n            [0,1,2,3]\n\n            # Mixed usage\n            &gt;&gt;&gt; get_token_index([\"last\", (2,5)], tokens, return_type=\"dict\")\n            {\"last\": [9], \"numeric\": [2,3,4]}\n\n            # Partition access\n            &gt;&gt;&gt; get_token_index([\"inputs-partition-0\"], tokens)  # First group\n            [0,1,2]\n            &gt;&gt;&gt; get_token_index([\"random-inputs-partition-0\"], tokens) # Random from group\n            [1]\n        \"\"\"\n        # Convert single token to list for uniform processing\n        # assert that we have a list of tokens\n        if not isinstance(tokens, list):\n            raise ValueError(\n                \"Tokens must be a list of strings or integers or tuples. Got {}\".format(\n                    type(tokens)\n                )\n            )\n\n        # Check if all tokens are supported or not\n        if not all(\n            token in SUPPORTED_TOKENS\n            or isinstance(token, int)\n            or isinstance(token, tuple)\n            or token.startswith(\"inputs-partition-\")\n            or token.startswith(\"random-inputs-partition-\")\n            or token.startswith(\"random-image\")\n            or token.startswith(\"random-text\")\n            for token in tokens\n        ):\n            raise ValueError(\n                f\"Unsupported token type into: {tokens}. Supported tokens are: {SUPPORTED_TOKENS} and inputs-partition-0, inputs-partition-1, etc or random-inputs-partition-0, random-inputs-partition-1, etc or integer indices or slices (tuple of start, end)\"\n            )\n\n        # Check if pivot_positions is required but not provided\n        if self.pivot_positions is None and any(\n            isinstance(token, str)\n            and (\n                token.startswith(\"inputs-partition-\")\n                or token.startswith(\"random-inputs-partition-\")\n            )\n            for token in tokens\n        ):\n            raise ValueError(\n                \"pivot_positions cannot be None when a group position token is requested\"\n            )\n\n        token_indexes = self.categorize_tokens(string_tokens)\n        tokens_positions, token_dict = self.get_tokens_positions(tokens, token_indexes)\n\n        if return_type == \"dict\":\n            return token_dict\n        if return_type == \"all\":\n            return tokens_positions, token_dict\n        return tokens_positions\n\n    def get_tokens_positions(\n        self,\n        tokens: List[Union[str, int, Tuple[int, int]]],\n        token_indexes: Dict[str, List[int]],\n    ) -&gt; List[int]:\n        tokens_positions = []\n        position_dict = {\n            k: v for k, v in token_indexes.items() if k.startswith(\"inputs-partition-\")\n        }\n        random_position_dict = {\n            f\"random-{k}\": random.sample(v, 1) for k, v in position_dict.items()\n        }\n        numeric_tokens = {}\n        for token in tokens:\n            if isinstance(token, int):\n                # tokens_positions.append((token))\n                # check if the absolute value of the token is less than the length of the string tokens\n                if abs(token) &gt;= len(token_indexes[\"all\"]):\n                    raise ValueError(f\"Token {token} is out of range\")\n                numeric_tokens[token] = [token]\n            elif isinstance(token, tuple):\n                start, end = token\n                # check if the start and end are within the range of the string tokens\n                if start &gt;= len(token_indexes[\"all\"]) or end &gt;= len(\n                    token_indexes[\"all\"]\n                ):\n                    raise ValueError(f\"Token {token} is out of range\")\n                elif start &gt; end and end &gt; 0:\n                    raise ValueError(\n                        f\"Token {token} is invalid. The start index must be less than the end index\"\n                    )\n                if start &gt; 0 and end &lt; 0:\n                    end = len(token_indexes[\"all\"]) - end\n                numeric_tokens[token] = list(range(start, end))\n            elif token.startswith(\"random-inputs-partition-\"):\n                group, n = self.parse_random_group_token(token)\n                random_position_dict[token] = random.sample(\n                    position_dict[f\"inputs-partition-{group}\"], int(n)\n                )\n            elif token.startswith(\"random-image\"):\n\n                n = token.split(\"-\")[-1]\n                if n.isdigit():\n                    n = int(n)\n                else:\n                    n = None\n                random_position_dict[token] = random.sample(\n                    token_indexes[\"image\"], n if n else 1\n                )\n            elif token.startswith(\"random-text\"):\n                if token == \"random-text\":\n                    random_position_dict[token] = random.sample(\n                        token_indexes[\"text\"], 1\n                    )\n                else:\n                    n = token.split(\"-\")[-1]\n                    if n.isdigit():\n                        n = int(n)\n                    else:\n                        n = None\n                    random_position_dict[token] = random.sample(\n                        token_indexes[\"text\"], n if n else 1\n                    )\n\n        token_dict = self.get_token_dict(token_indexes, random_position_dict)\n        # add the numeric indexe\n        token_dict = {**token_dict, **numeric_tokens}\n\n        for token in tokens:\n            if token_dict[token] is not None:\n                tokens_positions.append(tuple(token_dict[token]))  # type: ignore\n\n        return tokens_positions, token_dict\n\n    def parse_random_group_token(self, token: str) -&gt; Tuple[str, int]:\n        group_and_n = token.split(\"-\")[2:]\n        if len(group_and_n) &gt; 1:\n            group, n = group_and_n\n        else:\n            group = group_and_n[0]\n            n = 1\n        return group, int(n)\n\n    def get_token_dict(\n        self,\n        token_indexes: Dict[str, List[int]],\n        random_position_dict: Dict[str, List[int]] = {},\n    ) -&gt; Dict[str, Optional[List[int]]]:\n        return {\n            \"last\": [-1],\n            \"last-2\": [-2],\n            \"last-4\": [-4],\n            \"last-image\": token_indexes[\"last_line_image\"],\n            \"end-image\": token_indexes[\"image_end\"],\n            \"all-text\": token_indexes[\"text\"],\n            \"all\": token_indexes[\"all\"],\n            \"all-image\": token_indexes[\"image\"],\n            \"special\": token_indexes[\"special\"],\n            \"random-text\": None\n            if len(token_indexes[\"text\"]) == 0\n            else [random.choice(token_indexes[\"text\"])],\n            \"random-image\": None\n            if len(token_indexes[\"image\"]) == 0\n            else [random.choice(token_indexes[\"image\"])],\n            \"special-pixtral\": [1052, 1051, 1038, 991, 1037, 1047],\n            **{\n                k: v\n                for k, v in token_indexes.items()\n                if k.startswith(\"inputs-partition-\")\n            },\n            **random_position_dict,\n        }\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.__init__","title":"<code>__init__(model_name, pivot_positions=None, pivot_tokens=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>str (required): the name of the model</p> required <code>pivot_positions</code> <code>Optional[List[int]]</code> <p>List[int] (optional): a list of integers that represent the positions where to split the tokens.</p> <code>None</code> <code>pivot_tokens</code> <code>Optional[List[str]]</code> <p>List[str] (optional): a list of strings that represent the tokens where to split the tokens.</p> <code>None</code> <p>The pivot_positions and pivot_tokens are mutually exclusive. The idea of the split is the following. Immagine to have an input string of tokens like this: [\"I\", \"love\", \"cats\", \"and\", \"dogs\". \"What\", \"about\", \"you?\"] Then, i want to extract/ablate/intervene on the second sentence. I can do it by specifying the pivot_positions=[5] or pivot_tokens=[\"What\"]. In this way, the tokens will be split in two groups: [\"I\", \"love\", \"cats\", \"and\"] and [\"dogs\", \"What\", \"about\", \"you?\"] with names \"inputs-partition-0\" and \"inputs-partition-1\".</p> Source code in <code>easyroutine/interpretability/token_index.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pivot_positions: Optional[List[int]] = None,\n    pivot_tokens: Optional[List[str]] = None,\n):\n    r\"\"\"\n    Args:\n        model_name: str (required): the name of the model\n        pivot_positions: List[int] (optional): a list of integers that represent the positions where to split the tokens.\n        pivot_tokens: List[str] (optional): a list of strings that represent the tokens where to split the tokens.\n\n\n    The pivot_positions and pivot_tokens are mutually exclusive.\n    The idea of the split is the following. Immagine to have an input string of tokens like this: [\"I\", \"love\", \"cats\", \"and\", \"dogs\". \"What\", \"about\", \"you?\"]\n    Then, i want to extract/ablate/intervene on the second sentence. I can do it by specifying the pivot_positions=[5] or pivot_tokens=[\"What\"].\n    In this way, the tokens will be split in two groups: [\"I\", \"love\", \"cats\", \"and\"] and [\"dogs\", \"What\", \"about\", \"you?\"] with names \"inputs-partition-0\" and \"inputs-partition-1\".\n    \"\"\"\n    self.model_name = model_name\n    self.pivot_tokens = pivot_tokens\n    self.pivot_positions = sorted(pivot_positions) if pivot_positions else []\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.get_token_index","title":"<code>get_token_index(tokens, string_tokens, return_type='list')</code>","text":"<p>Get indices for specified tokens in the input sequence.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Union[List[Union[str, int, Tuple[int, int]]], List[str], List[int], List[Tuple[int, int]]]</code> <p>list of tokens to look up. Can be: - String token identifiers (see Supported Tokens below) - Integer indices (direct position access) - Tuple[int, int] for slices (start, end)</p> required <code>string_tokens</code> <code>List[str]</code> <p>Input token sequence to search in</p> required <code>return_type</code> <code>Literal['list', 'dict', 'all']</code> <p>Output format: - \"list\": Returns [int, ...] - \"dict\": Returns {\"token_type\": [indices]} - \"all\": Returns ([indices], {token_type: [indices]})</p> <code>'list'</code> <p>Returns:</p> Type Description <code>Union[List[int], Dict, Tuple[List[int], Dict]]</code> <p>Based on return_type, provides token indices in specified format</p> Token Types <ol> <li> <p>Direct Access:</p> <ul> <li>Integer index: Direct position (e.g. 5)</li> <li>Slice tuple: Range of positions (e.g. (2,5))</li> <li>Negative index: From end (e.g. -1)</li> </ul> </li> <li> <p>Named Positions:</p> <ul> <li>\"last\": Last token</li> <li>\"last-2\": Second to last</li> <li>\"last-4\": Fourth to last</li> </ul> </li> <li> <p>Text Tokens:</p> <ul> <li>\"all-text\": All text positions</li> <li>\"random-text\": Single random text position</li> <li>\"random-text-N\": N random text positions</li> </ul> </li> <li> <p>Image Tokens:</p> <ul> <li>\"all-image\": All image positions</li> <li>\"last-image\": Last image position</li> <li>\"end-image\": Image end marker</li> <li>\"random-image\": Random image position</li> <li>\"random-image-N\": N random image positions</li> </ul> </li> <li> <p>Special:</p> <ul> <li>\"special\": Model-specific tokens</li> <li>\"all\": Complete sequence</li> <li>\"special-pixtral\": Model-specific fixed positions</li> <li>\"inputs-partition-N\": Nth token group</li> <li>\"random-inputs-partition-N\": Random from group N</li> </ul> </li> </ol> <p>Examples:</p>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.get_token_index--direct-index-access","title":"Direct index access","text":"<pre><code>&gt;&gt;&gt; get_token_index([5], tokens)               # Single position\n[5]\n&gt;&gt;&gt; get_token_index([(2,5)], tokens)          # Slice range\n[2,3,4]\n&gt;&gt;&gt; get_token_index([-1], tokens)             # From end\n[9]\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.get_token_index--named-tokens","title":"Named tokens","text":"<pre><code>&gt;&gt;&gt; get_token_index([\"last\"], tokens)         # Last token\n[9]\n&gt;&gt;&gt; get_token_index([\"all-text\"], tokens)     # All text\n[0,1,2,3]\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.get_token_index--mixed-usage","title":"Mixed usage","text":"<pre><code>&gt;&gt;&gt; get_token_index([\"last\", (2,5)], tokens, return_type=\"dict\")\n{\"last\": [9], \"numeric\": [2,3,4]}\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.get_token_index--partition-access","title":"Partition access","text":"<pre><code>&gt;&gt;&gt; get_token_index([\"inputs-partition-0\"], tokens)  # First group\n[0,1,2]\n&gt;&gt;&gt; get_token_index([\"random-inputs-partition-0\"], tokens) # Random from group\n[1]\n</code></pre> Source code in <code>easyroutine/interpretability/token_index.py</code> <pre><code>def get_token_index(\n    self,\n    tokens: Union[\n        List[Union[str, int, Tuple[int, int]]],\n        List[str],\n        List[int],\n        List[Tuple[int, int]],\n    ],\n    string_tokens: List[str],\n    return_type: Literal[\"list\", \"dict\", \"all\"] = \"list\",\n) -&gt; Union[List[int], Dict, Tuple[List[int], Dict]]:\n    \"\"\"Get indices for specified tokens in the input sequence.\n\n    Arguments:\n        tokens: list of tokens to look up. Can be:\n            - String token identifiers (see Supported Tokens below)\n            - Integer indices (direct position access)\n            - Tuple[int, int] for slices (start, end)\n        string_tokens: Input token sequence to search in\n        return_type: Output format:\n            - \"list\": Returns [int, ...]\n            - \"dict\": Returns {\"token_type\": [indices]}\n            - \"all\": Returns ([indices], {token_type: [indices]})\n\n    Returns:\n        Based on return_type, provides token indices in specified format\n\n    Token Types:\n        1. Direct Access:\n            - Integer index: Direct position (e.g. 5)\n            - Slice tuple: Range of positions (e.g. (2,5))\n            - Negative index: From end (e.g. -1)\n\n        2. Named Positions:\n            - \"last\": Last token\n            - \"last-2\": Second to last\n            - \"last-4\": Fourth to last\n\n        3. Text Tokens:\n            - \"all-text\": All text positions\n            - \"random-text\": Single random text position\n            - \"random-text-N\": N random text positions\n\n        4. Image Tokens:\n            - \"all-image\": All image positions\n            - \"last-image\": Last image position\n            - \"end-image\": Image end marker\n            - \"random-image\": Random image position\n            - \"random-image-N\": N random image positions\n\n        5. Special:\n            - \"special\": Model-specific tokens\n            - \"all\": Complete sequence\n            - \"special-pixtral\": Model-specific fixed positions\n            - \"inputs-partition-N\": Nth token group\n            - \"random-inputs-partition-N\": Random from group N\n\n    Examples:\n        # Direct index access\n        &gt;&gt;&gt; get_token_index([5], tokens)               # Single position\n        [5]\n        &gt;&gt;&gt; get_token_index([(2,5)], tokens)          # Slice range\n        [2,3,4]\n        &gt;&gt;&gt; get_token_index([-1], tokens)             # From end\n        [9]\n\n        # Named tokens\n        &gt;&gt;&gt; get_token_index([\"last\"], tokens)         # Last token\n        [9]\n        &gt;&gt;&gt; get_token_index([\"all-text\"], tokens)     # All text\n        [0,1,2,3]\n\n        # Mixed usage\n        &gt;&gt;&gt; get_token_index([\"last\", (2,5)], tokens, return_type=\"dict\")\n        {\"last\": [9], \"numeric\": [2,3,4]}\n\n        # Partition access\n        &gt;&gt;&gt; get_token_index([\"inputs-partition-0\"], tokens)  # First group\n        [0,1,2]\n        &gt;&gt;&gt; get_token_index([\"random-inputs-partition-0\"], tokens) # Random from group\n        [1]\n    \"\"\"\n    # Convert single token to list for uniform processing\n    # assert that we have a list of tokens\n    if not isinstance(tokens, list):\n        raise ValueError(\n            \"Tokens must be a list of strings or integers or tuples. Got {}\".format(\n                type(tokens)\n            )\n        )\n\n    # Check if all tokens are supported or not\n    if not all(\n        token in SUPPORTED_TOKENS\n        or isinstance(token, int)\n        or isinstance(token, tuple)\n        or token.startswith(\"inputs-partition-\")\n        or token.startswith(\"random-inputs-partition-\")\n        or token.startswith(\"random-image\")\n        or token.startswith(\"random-text\")\n        for token in tokens\n    ):\n        raise ValueError(\n            f\"Unsupported token type into: {tokens}. Supported tokens are: {SUPPORTED_TOKENS} and inputs-partition-0, inputs-partition-1, etc or random-inputs-partition-0, random-inputs-partition-1, etc or integer indices or slices (tuple of start, end)\"\n        )\n\n    # Check if pivot_positions is required but not provided\n    if self.pivot_positions is None and any(\n        isinstance(token, str)\n        and (\n            token.startswith(\"inputs-partition-\")\n            or token.startswith(\"random-inputs-partition-\")\n        )\n        for token in tokens\n    ):\n        raise ValueError(\n            \"pivot_positions cannot be None when a group position token is requested\"\n        )\n\n    token_indexes = self.categorize_tokens(string_tokens)\n    tokens_positions, token_dict = self.get_tokens_positions(tokens, token_indexes)\n\n    if return_type == \"dict\":\n        return token_dict\n    if return_type == \"all\":\n        return tokens_positions, token_dict\n    return tokens_positions\n</code></pre>"},{"location":"api/interpretability/tools/","title":"Interpretability Tools","text":"<p>This section covers the tools provided by <code>easyroutine</code> for model interpretability beyond basic activation extraction.</p>"},{"location":"api/interpretability/tools/#logitlens","title":"LogitLens","text":"<p><code>LogitLens</code> is a powerful tool that enables you to interpret intermediate representations in transformer models by projecting them through the model's output head to the vocabulary space. This technique, often called \"logit lens\" in the mechanistic interpretability literature, allows researchers to understand what token predictions would be made at each layer, providing insights into how representations evolve through the network.</p>"},{"location":"api/interpretability/tools/#key-features","title":"Key Features","text":"<ul> <li>Project any intermediate activation to token probability space</li> <li>Apply proper layer normalization based on model architecture (supports both LayerNorm and RMSNorm)</li> <li>Compute metrics like logit differences between specific token directions</li> <li>Support for various model architectures with different normalization schemes</li> </ul>"},{"location":"api/interpretability/tools/#usage","title":"Usage","text":"<pre><code>from easyroutine.interpretability import HookedModel\nfrom easyroutine.interpretability.tools import LogitLens\n\n# Create a model\nmodel = HookedModel.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n# Create LogitLens from the model\nlogit_lens = LogitLens.from_model(model)\n\n# Run forward pass and collect activations\ncache = model.forward(\n    inputs, \n    target_token_positions=[\"last\"],\n    extraction_config=ExtractionConfig(\n        extract_resid_out=True,\n        extract_last_layernorm=True  # Required for proper normalization\n    )\n)\n\n# Analyze residual streams from different layers using LogitLens\nlayer = 12  # Specify which layer to analyze\nresults = logit_lens.compute(\n    activations=cache, \n    target_key=f\"resid_out_{layer}\", \n    apply_norm=True,\n    apply_softmax=True  # Set to True if you want probabilities instead of logits\n)\n\n# To compute logit difference between two specific tokens\ntoken_diff_results = logit_lens.compute(\n    activations=cache,\n    target_key=f\"resid_out_{layer}\",\n    token_directions=[(model.hf_tokenizer.encode(\" yes\")[0], model.hf_tokenizer.encode(\" no\")[0])],\n    metric=\"logit_diff\"\n)\n</code></pre>"},{"location":"api/interpretability/tools/#api-reference","title":"API Reference","text":""},{"location":"api/interpretability/tools/#logitlens-constructor","title":"LogitLens Constructor","text":"<pre><code>def __init__(self, unembedding_matrix, last_layer_norm, model_name, model_config):\n</code></pre> <p>Arguments: - <code>unembedding_matrix</code>: The weight matrix used in the model's output embedding layer - <code>last_layer_norm</code>: The model's final layer normalization module - <code>model_name</code>: Name of the model - <code>model_config</code>: Model configuration object containing architecture details</p>"},{"location":"api/interpretability/tools/#class-methods","title":"Class Methods","text":"<pre><code>@classmethod\ndef from_model(cls, model: HookedModel) -&gt; 'LogitLens'\n</code></pre> <p>Create a LogitLens instance from a HookedModel.</p> <pre><code>@classmethod\ndef from_model_name(cls, model_name: str) -&gt; 'LogitLens'\n</code></pre> <p>Create a LogitLens instance directly from a model name (will load the model).</p>"},{"location":"api/interpretability/tools/#core-methods","title":"Core Methods","text":"<pre><code>def compute(\n    self,\n    activations: ActivationCache,\n    target_key: str,\n    token_directions: Optional[Union[List[int], List[Tuple[int, int]]]] = None,\n    apply_norm: bool = True,\n    apply_softmax: bool = False,\n    metric: Optional[Literal[\"logit_diff\", \"accuracy\"]] = \"logit_diff\"\n) -&gt; dict:\n</code></pre> <p>Arguments: - <code>activations</code>: ActivationCache containing model activations - <code>target_key</code>: The key/pattern for activations to analyze (e.g., \"resid_out_{i}\") - <code>token_directions</code>: Optional list of token IDs or pairs to compute specific direction metrics - <code>apply_norm</code>: Whether to apply layer normalization to the activations - <code>apply_softmax</code>: Whether to apply softmax to get probabilities instead of raw logits - <code>metric</code>: Metric to compute if token_directions are provided (\"logit_diff\" or \"accuracy\")</p> <p>Returns: - Dictionary with computed logit lens results</p>"},{"location":"api/interpretability/tools/#example-analyzing-mid-layer-predictions","title":"Example: Analyzing Mid-layer Predictions","text":"<pre><code>import torch\nfrom easyroutine.interpretability import HookedModel, ExtractionConfig\nfrom easyroutine.interpretability.tools import LogitLens\n\nmodel = HookedModel.from_pretrained(\"gpt2\")\nlogit_lens = LogitLens.from_model(model)\n\n# Create a prompt\ntokenizer = model.get_tokenizer()\nprompt = \"The capital of France is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Extract activations from all layers\ncache = model.forward(\n    inputs, \n    target_token_positions=[\"last\"],\n    extraction_config=ExtractionConfig(\n        extract_resid_out=True,\n        extract_last_layernorm=True\n    )\n)\n\n# Analyze how the prediction for the last token evolves through layers\nresults = {}\nfor layer in range(model.model_config.num_hidden_layers):\n    layer_results = logit_lens.compute(\n        activations=cache, \n        target_key=f\"resid_out_{layer}\", \n        apply_norm=True,\n        apply_softmax=True\n    )\n\n    # Get top 5 tokens for the last position\n    logits = layer_results[f\"logit_lens_resid_out_{layer}\"][0, -1]\n    top_tokens = torch.topk(logits, 5)\n\n    results[layer] = [\n        (tokenizer.decode(idx.item()), prob.item()) \n        for idx, prob in zip(top_tokens.indices, top_tokens.values)\n    ]\n\n# Print results to see how predictions evolve\nfor layer, tokens in results.items():\n    print(f\"Layer {layer}: {tokens}\")\n</code></pre> <p>This example shows how to track the evolution of predictions through the layers of the model, providing insights into how the model builds up its final prediction.</p>"},{"location":"api/interpretability/tools/#activationsaver","title":"ActivationSaver","text":"<p><code>ActivationSaver</code> is a utility that provides a structured way to save and load model activations with rich metadata. This is particularly useful for large-scale experiments where you want to save activations for later analysis.</p>"},{"location":"api/interpretability/tools/#key-features_1","title":"Key Features","text":"<ul> <li>Structured filesystem organization for saved activations</li> <li>Automatic metadata tracking (model, configuration, extraction details)</li> <li>Tagging system for easier identification of specific runs</li> <li>Query functionality through companion <code>ActivationLoader</code> class</li> <li>Supports renaming experiments and organizing workspace</li> </ul>"},{"location":"api/interpretability/tools/#usage_1","title":"Usage","text":"<pre><code>from easyroutine.interpretability import HookedModel, ExtractionConfig\nfrom easyroutine.interpretability.activation_saver import ActivationSaver\n\n# Create a model and extract activations\nmodel = HookedModel.from_pretrained(\"gpt2\")\ninputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n\ncache = model.forward(\n    inputs, \n    target_token_positions=[\"last\"],\n    extraction_config=ExtractionConfig(extract_resid_out=True)\n)\n\n# Save activations with metadata\nsaver = ActivationSaver(base_dir=\"./saved_activations\", experiment_name=\"hello_world_test\")\nsave_dir = saver.save(\n    activations=cache,\n    model=model,\n    target_token_positions=[\"last\"],\n    interventions=None,\n    extraction_config=ExtractionConfig(extract_resid_out=True),\n    tag=\"first_test\"\n)\n\n# Or use the simplified method if the cache already contains metadata\n# This is the case with caches created using model.extract_cache()\ndataset_cache = model.extract_cache(\n    dataloader,\n    target_token_positions=[\"last\"],\n    extraction_config=ExtractionConfig(extract_resid_out=True)\n)\nsaver.save_cache(dataset_cache, tag=\"dataset_run\")\n</code></pre>"},{"location":"api/interpretability/tools/#api-reference_1","title":"API Reference","text":""},{"location":"api/interpretability/tools/#activationsaver_1","title":"ActivationSaver","text":"<pre><code>def __init__(self, base_dir: Union[Path, str], experiment_name: str = \"default\")\n</code></pre> <p>Arguments: - <code>base_dir</code>: Base directory where activations will be saved - <code>experiment_name</code>: Name of the experiment (creates a subdirectory)</p> <pre><code>@classmethod\ndef from_env(cls, experiment_name: str = \"default\")\n</code></pre> <p>Create a saver using the base directory specified in the environment variable <code>ACTIVATION_BASE_DIR</code></p> <pre><code>def save(\n    self,\n    activations: Union[torch.Tensor, dict, ActivationCache],\n    model: HookedModel,\n    target_token_positions,\n    interventions: Optional[List[Intervention]],\n    extraction_config: ExtractionConfig,\n    other_metadata: dict = {},\n    tag: Optional[str] = None,\n)\n</code></pre> <p>Arguments: - <code>activations</code>: The activation cache or tensor to save - <code>model</code>: The HookedModel used to generate the activations - <code>target_token_positions</code>: The token positions used during extraction - <code>interventions</code>: Any interventions applied during extraction - <code>extraction_config</code>: The extraction configuration used - <code>other_metadata</code>: Additional metadata to store - <code>tag</code>: Optional tag for easier identification</p> <p>Returns: - The directory path where activations were saved</p> <pre><code>def save_cache(\n    self,\n    cache: ActivationCache,\n    other_metadata: dict = {},\n    tag: Optional[str] = None,\n)\n</code></pre> <p>A simplified save method that expects the cache to already contain metadata.</p> <pre><code>def rename_experiment(self, new_experiment_name: str)\n</code></pre> <p>Renames the experiment directory and updates all metadata files within it.</p>"},{"location":"api/interpretability/tools/#activationloader","title":"ActivationLoader","text":"<pre><code>def __init__(self, base_dir: Path, experiment_name: str = \"default\")\n</code></pre> <p>Arguments: - <code>base_dir</code>: Base directory where activations are saved - <code>experiment_name</code>: Name of the experiment to load from</p> <pre><code>@classmethod\ndef from_env(cls, experiment_name: str = \"default\")\n</code></pre> <p>Create a loader using the base directory specified in the environment variable <code>ACTIVATION_BASE_DIR</code></p> <pre><code>@classmethod\ndef from_saver(cls, saver: ActivationSaver)\n</code></pre> <p>Create a loader from an existing ActivationSaver</p> <pre><code>def query(\n    self,\n    experiment_name: Optional[str] = None,\n    model_name: Optional[str] = None,\n    target_token_positions: Optional[List[Union[str, int]]] = None,\n    pivot_positions: Optional[List[int]] = None,\n    save_time: Optional[str] = None,\n    custom_keys: Optional[dict] = None,\n    extraction_config: Optional[ExtractionConfig] = None,\n    interventions: Optional[List[Intervention]] = None,\n    tag: Optional[str] = None,\n) -&gt; QueryResult\n</code></pre> <p>Search for saved activations based on various criteria.</p> <p>Arguments: - Various search criteria to filter saved activations</p> <p>Returns: - A QueryResult object containing matching runs</p>"},{"location":"api/interpretability/tools/#query-results","title":"Query Results","text":"<p>The <code>QueryResult</code> class provides a convenient interface for managing the results of queries:</p> <pre><code># List all matching runs\nprint(query_result)\n\n# Get detailed paths\nprint(query_result.get_paths())\n\n# Load a specific run by index (e.g., most recent with -1)\nactivations, metadata = query_result.load(-1)\n\n# Load by specific time folder\nactivations, metadata = query_result.load(\"2023-04-15_14-30-25\")\n\n# Remove a specific run\nquery_result.remove(-1)  # Remove the most recent run\n\n# Update experiment name for a run\nquery_result.update_run_experiment(-1, \"new_experiment_name\")\n</code></pre>"},{"location":"api/interpretability/tools/#example-comprehensive-workflow","title":"Example: Comprehensive Workflow","text":"<pre><code>import torch\nfrom easyroutine.interpretability import HookedModel, ExtractionConfig\nfrom easyroutine.interpretability.activation_saver import ActivationSaver, ActivationLoader\n\n# 1. Create model and dataset\nmodel = HookedModel.from_pretrained(\"gpt2\")\ntokenizer = model.get_tokenizer()\n\n# Create a simple dataset\ntexts = [\"Example text 1\", \"Example text 2\", \"Example text 3\"]\ndataset = [tokenizer(text, return_tensors=\"pt\") for text in texts]\n\n# 2. Extract and save activations\nsaver = ActivationSaver(base_dir=\"./saved_activations\", experiment_name=\"documentation_example\")\n\nfor i, inputs in enumerate(dataset):\n    # Extract activations for this batch\n    cache = model.forward(\n        inputs, \n        target_token_positions=[\"last\"],\n        extraction_config=ExtractionConfig(\n            extract_resid_out=True,\n            extract_attn_pattern=True\n        )\n    )\n\n    # Save with batch-specific tag\n    saver.save(\n        activations=cache,\n        model=model,\n        target_token_positions=[\"last\"],\n        interventions=None,\n        extraction_config=ExtractionConfig(\n            extract_resid_out=True,\n            extract_attn_pattern=True\n        ),\n        other_metadata={\"batch_id\": i},\n        tag=f\"batch_{i}\"\n    )\n\n# 3. Load and analyze results later\nloader = ActivationLoader.from_saver(saver)\n\n# Query for specific batch\nbatch_0_results = loader.query(tag=\"batch_0\")\nactivations, metadata = batch_0_results.load(-1)  # Load the most recent match\n\n# Query by custom metadata\nspecific_batch = loader.query(custom_keys={\"batch_id\": 1})\nactivations, metadata = specific_batch.load(-1)\n\n# Query by model name and extraction configuration\nall_gpt2_runs = loader.query(\n    model_name=\"gpt2\",\n    extraction_config=ExtractionConfig(extract_attn_pattern=True)\n)\n\nprint(f\"Found {len(all_gpt2_runs.results)} matching runs\")\n</code></pre> <p>This example demonstrates a complete workflow of extracting activations from multiple inputs, saving them with metadata, and later querying and loading them for analysis.</p>"}]}