
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://easyroutine.github.io/api/interpretability/hooks/">
      
      
        <link rel="prev" href="../hooked_model/">
      
      
        <link rel="next" href="../interventions/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>Hooks - EasyRoutine</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#easyroutine.interpretability.hooks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="EasyRoutine" class="md-header__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EasyRoutine
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Hooks
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="EasyRoutine" class="md-nav__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    EasyRoutine
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Api
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Api
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Interpretability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Interpretability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    API Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activation_cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Activation cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hooked_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hooked model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Hooks
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Hooks
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks" class="md-nav__link">
    <span class="md-ellipsis">
      hooks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_pos_keep_self_attn_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      avg_attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_hook" class="md-nav__link">
    <span class="md-ellipsis">
      avg_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.compute_statistics" class="md-nav__link">
    <span class="md-ellipsis">
      compute_statistics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.create_dynamic_hook" class="md-nav__link">
    <span class="md-ellipsis">
      create_dynamic_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.embed_hook" class="md-nav__link">
    <span class="md-ellipsis">
      embed_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.input_embedding_hook" class="md-nav__link">
    <span class="md-ellipsis">
      input_embedding_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_attn_mat_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_attn_mat_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_heads_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_heads_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_query_key_value_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_query_key_value_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_resid_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_resid_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.layernom_hook" class="md-nav__link">
    <span class="md-ellipsis">
      layernom_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.multiply_pattern" class="md-nav__link">
    <span class="md-ellipsis">
      multiply_pattern
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.process_args_kwargs_output" class="md-nav__link">
    <span class="md-ellipsis">
      process_args_kwargs_output
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_key_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_key_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_query_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_query_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_value_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_value_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.query_key_value_hook" class="md-nav__link">
    <span class="md-ellipsis">
      query_key_value_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.restore_same_args_kwargs_output" class="md-nav__link">
    <span class="md-ellipsis">
      restore_same_args_kwargs_output
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.save_resid_hook" class="md-nav__link">
    <span class="md-ellipsis">
      save_resid_hook
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../interventions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interventions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module_wrapper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Module Wrapper
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../token_index/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Token index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Interpretability Tools
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks" class="md-nav__link">
    <span class="md-ellipsis">
      hooks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_pos_keep_self_attn_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      avg_attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_hook" class="md-nav__link">
    <span class="md-ellipsis">
      avg_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.compute_statistics" class="md-nav__link">
    <span class="md-ellipsis">
      compute_statistics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.create_dynamic_hook" class="md-nav__link">
    <span class="md-ellipsis">
      create_dynamic_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.embed_hook" class="md-nav__link">
    <span class="md-ellipsis">
      embed_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.input_embedding_hook" class="md-nav__link">
    <span class="md-ellipsis">
      input_embedding_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_attn_mat_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_attn_mat_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_heads_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_heads_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_query_key_value_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_query_key_value_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.intervention_resid_hook" class="md-nav__link">
    <span class="md-ellipsis">
      intervention_resid_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.layernom_hook" class="md-nav__link">
    <span class="md-ellipsis">
      layernom_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.multiply_pattern" class="md-nav__link">
    <span class="md-ellipsis">
      multiply_pattern
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.process_args_kwargs_output" class="md-nav__link">
    <span class="md-ellipsis">
      process_args_kwargs_output
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_key_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_key_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_query_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_query_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_value_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_value_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.query_key_value_hook" class="md-nav__link">
    <span class="md-ellipsis">
      query_key_value_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.restore_same_args_kwargs_output" class="md-nav__link">
    <span class="md-ellipsis">
      restore_same_args_kwargs_output
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.save_resid_hook" class="md-nav__link">
    <span class="md-ellipsis">
      save_resid_hook
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Hooks</h1>

<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.hooks"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ablate_pos_keep_self_attn_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the tokens in the attention
mask but keeping the self attn weigths.
It will set to 0 the row of tokens to ablate except for
the las position</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ablate_pos_keep_self_attn_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the tokens in the attention</span>
<span class="sd">    mask but keeping the self attn weigths.</span>
<span class="sd">    It will set to 0 the row of tokens to ablate except for</span>
<span class="sd">    the las position</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="ne">Warning</span><span class="p">(</span><span class="s2">&quot;This function is deprecated. Use ablate_attn_mat_hook instead&quot;</span><span class="p">)</span>
    <span class="n">attn_matrix</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
    <span class="c1"># initial_shape = attn_matrix.shape</span>

    <span class="k">for</span> <span class="n">head</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">],</span> <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;pos_token_to_ablate&quot;</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">attn_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">b</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">attn_matrix</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.attention_pattern_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">attention_pattern_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">attn_pattern_avg</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">attn_pattern_row_partition</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the attention pattern of the heads. It will extract the attention pattern.
As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="args">args</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input args of the hook function</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="kwargs">kwargs</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input kwargs of the hook function</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="output">output</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the output of the hook function</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="token_indexes">token_indexes</span>(<span title="typing.List">List</span>[<span title="Tuple">Tuple</span>])</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the indexes of the tokens to extract the attention pattern</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>layer (int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>cache (ActivationCache</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the cache where to save the activations</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>head (Union[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model. If "all" is passed, it will extract all the heads of the layer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>attn_pattern_avg (Literal[&#34;mean&#34;, &#34;sum&#34;, &#34;baseline_ratio&#34;, &#34;none&#34;]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the method to average the attention pattern</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>attn_pattern_row_partition (List[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the indexes of the tokens to partition the attention pattern</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


<details class="avg-strategies" open>
  <summary>Avg strategies</summary>
  <p>If the attn_pattern_avg is not "none", the attention pattern is divided in blocks and the average value of each block is computed, using the method specified in attn_pattern_avg.
The idea is to partition the attention pattern into groups of tokens, and then compute a single average value for each group. The pattern is divided into len(attn_pattern_row_partition) x len(token_indexes) blocks, where each block B_ij is defined to have the indeces of the rows in attn_pattern_row_partition[i] and the columns in token_indexes[j]. If attn_pattern_row_partition is None, then the rows are the same as token_indexes.</p>
<p>0| a_00 0    0    0    0    0    0              token_indexes = [(1,3), (4)]
1| a_10 a_11 0    0    0    0    0              attn_pattern_row_partition = [(0,1)]
2| a_20 a_21 a_22 0    0    0    0
3| a_30 a_31 a_32 a_33 0    0    0
4| a_40 a_41 a_42 a_43 a_44 0    0
5| a_50 a_51 a_52 a_53 a_54 a_55 0
6| a_60 a_61 a_62 a_63 a_64 a_65 a_66</p>
<hr />
<pre><code>0    1     2   3     4    5    6
</code></pre>
<ul>
<li>Block B_00:<ul>
<li>Rows: 0,1</li>
<li>Columns: 1,2,3</li>
<li>Block: [a_01, a_02, a_03, a_11, a_12, a_13]</li>
</ul>
</li>
<li>Block B_01:<ul>
<li>Rows: 0,1</li>
<li>Columns: 4</li>
<li>Block: [a_04, a_14]</li>
</ul>
</li>
</ul>
<p>If attn_pattern_avg is "mean", the average value for each block is computed as the mean of the block, so the output will be: batch n_row_blocks n_col_blocks
So in this case, the output will be: batch 1 2 where the first value is the average of the first block and the second value is the average of the second block.</p>
<p>The method to compute a single value for each block is specified by the attn_pattern_avg parameter, and can be one of the following:
- "mean": Compute the mean of the block.
- "sum": Compute the sum of the block.
- "baseline_ratio": Compute the ratio of the observed average attention to the expected average attention. The expected average attention is computed by assuming that attention is uniform across the block. So, for each row in attn_pattern_row_partition, we compute the fraction of allowed keys that belong to token_indexes. The expected average attention is the sum of these fractions divided by the number of rows. The final ratio is the observed average attention divided by the expected average attention.</p>
</details>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">attention_pattern_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_pattern_avg</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;baseline_ratio&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">attn_pattern_row_partition</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the attention pattern of the heads. It will extract the attention pattern.</span>
<span class="sd">    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</span>

<span class="sd">    Arguments:</span>
<span class="sd">        - args: the input args of the hook function</span>
<span class="sd">        - kwargs: the input kwargs of the hook function</span>
<span class="sd">        - output: the output of the hook function</span>
<span class="sd">        - token_indexes (List[Tuple]) : the indexes of the tokens to extract the attention pattern</span>
<span class="sd">        - layer (int): the layer of the model</span>
<span class="sd">        - cache (ActivationCache): the cache where to save the activations</span>
<span class="sd">        - head (Union[str, int]): the head of the model. If &quot;all&quot; is passed, it will extract all the heads of the layer</span>
<span class="sd">        - attn_pattern_avg (Literal[&quot;mean&quot;, &quot;sum&quot;, &quot;baseline_ratio&quot;, &quot;none&quot;]): the method to average the attention pattern</span>
<span class="sd">        - attn_pattern_row_partition (List[int]): the indexes of the tokens to partition the attention pattern</span>

<span class="sd">    Avg strategies:</span>
<span class="sd">        If the attn_pattern_avg is not &quot;none&quot;, the attention pattern is divided in blocks and the average value of each block is computed, using the method specified in attn_pattern_avg.</span>
<span class="sd">        The idea is to partition the attention pattern into groups of tokens, and then compute a single average value for each group. The pattern is divided into len(attn_pattern_row_partition) x len(token_indexes) blocks, where each block B_ij is defined to have the indeces of the rows in attn_pattern_row_partition[i] and the columns in token_indexes[j]. If attn_pattern_row_partition is None, then the rows are the same as token_indexes.</span>

<span class="sd">        0| a_00 0    0    0    0    0    0              token_indexes = [(1,3), (4)]</span>
<span class="sd">        1| a_10 a_11 0    0    0    0    0              attn_pattern_row_partition = [(0,1)]</span>
<span class="sd">        2| a_20 a_21 a_22 0    0    0    0</span>
<span class="sd">        3| a_30 a_31 a_32 a_33 0    0    0</span>
<span class="sd">        4| a_40 a_41 a_42 a_43 a_44 0    0</span>
<span class="sd">        5| a_50 a_51 a_52 a_53 a_54 a_55 0</span>
<span class="sd">        6| a_60 a_61 a_62 a_63 a_64 a_65 a_66</span>
<span class="sd">           ------------------------------</span>
<span class="sd">            0    1     2   3     4    5    6</span>

<span class="sd">        - Block B_00:</span>
<span class="sd">            - Rows: 0,1</span>
<span class="sd">            - Columns: 1,2,3</span>
<span class="sd">            - Block: [a_01, a_02, a_03, a_11, a_12, a_13]</span>
<span class="sd">        - Block B_01:</span>
<span class="sd">            - Rows: 0,1</span>
<span class="sd">            - Columns: 4</span>
<span class="sd">            - Block: [a_04, a_14]</span>


<span class="sd">        If attn_pattern_avg is &quot;mean&quot;, the average value for each block is computed as the mean of the block, so the output will be: batch n_row_blocks n_col_blocks</span>
<span class="sd">        So in this case, the output will be: batch 1 2 where the first value is the average of the first block and the second value is the average of the second block.</span>

<span class="sd">        The method to compute a single value for each block is specified by the attn_pattern_avg parameter, and can be one of the following:</span>
<span class="sd">        - &quot;mean&quot;: Compute the mean of the block.</span>
<span class="sd">        - &quot;sum&quot;: Compute the sum of the block.</span>
<span class="sd">        - &quot;baseline_ratio&quot;: Compute the ratio of the observed average attention to the expected average attention. The expected average attention is computed by assuming that attention is uniform across the block. So, for each row in attn_pattern_row_partition, we compute the fraction of allowed keys that belong to token_indexes. The expected average attention is the sum of these fractions divided by the number of rows. The final ratio is the observed average attention divided by the expected average attention.</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># first get the attention pattern</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">attn_pattern</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads,seq_len, seq_len)</span>

    <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="n">head_indices</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">attn_pattern</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">head_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">head</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">attn_pattern_row_partition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">token_indexes_group1</span> <span class="o">=</span> <span class="n">attn_pattern_row_partition</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">token_indexes_group1</span> <span class="o">=</span> <span class="n">token_indexes</span>

    <span class="c1"># For each token group (each tuple in token_indexes), compute a single average value.</span>
    <span class="k">if</span> <span class="n">attn_pattern_row_partition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">head_indices</span><span class="p">:</span>
            <span class="c1"># For head h, pattern has shape [batch, seq_len, seq_len].</span>
            <span class="n">group_avgs</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Generate all combinations of groups</span>
            <span class="k">for</span> <span class="n">group1</span> <span class="ow">in</span> <span class="n">token_indexes_group1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">group2</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
                    <span class="c1"># Extract the attention block for this combination.</span>
                    <span class="n">attn_block</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">h</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">group1</span><span class="p">),</span> <span class="p">:][:,</span> <span class="p">:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">group2</span><span class="p">)]</span>

                    <span class="c1"># Depending on the selected averaging method, compute a metric.</span>
                    <span class="k">if</span> <span class="n">attn_pattern_avg</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
                        <span class="c1"># Simple mean over the block.</span>
                        <span class="n">avg_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">attn_block</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># shape: [batch]</span>

                    <span class="k">elif</span> <span class="n">attn_pattern_avg</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
                        <span class="c1"># Simple sum over the block.</span>
                        <span class="n">avg_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attn_block</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

                    <span class="k">elif</span> <span class="n">attn_pattern_avg</span> <span class="o">==</span> <span class="s2">&quot;baseline_ratio&quot;</span><span class="p">:</span>
                        <span class="c1"># ---- Step 1. Compute the observed average attention in the block.</span>
                        <span class="n">observed_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                            <span class="n">attn_block</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="p">)</span>  <span class="c1"># shape: [batch]</span>

                        <span class="c1"># ---- Step 2. Compute the baseline expectation.</span>
                        <span class="c1"># For each row (i.e. token index) in group1, we calculate the fraction of allowed keys</span>
                        <span class="c1"># that belong to group2. Because the attention is lower-triangular,</span>
                        <span class="c1"># a row with index &#39;i&#39; can only attend to tokens with indices &lt;= i.</span>
                        <span class="c1"># Thus, for each row &#39;i&#39; in group1, the expected fraction (if uniform) is:</span>
                        <span class="c1">#      (# of tokens in group2 with index &lt;= i) / (i+1)</span>
                        <span class="n">baseline_list</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">group1</span><span class="p">:</span>
                            <span class="c1"># Count the number of tokens in group2 that are allowed for row i.</span>
                            <span class="n">allowed_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">group2</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="n">i</span><span class="p">)</span>
                            <span class="c1"># Total keys available for row i (assuming indices start at 0).</span>
                            <span class="n">total_allowed</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
                            <span class="c1"># Avoid division by zero (should not happen if i&gt;=0).</span>
                            <span class="n">baseline_ratio</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="n">allowed_count</span> <span class="o">/</span> <span class="n">total_allowed</span>
                                <span class="k">if</span> <span class="n">total_allowed</span> <span class="o">&gt;</span> <span class="mi">0</span>
                                <span class="k">else</span> <span class="mf">0.0</span>
                            <span class="p">)</span>
                            <span class="n">baseline_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">baseline_ratio</span><span class="p">)</span>

                        <span class="c1"># Average the per-row baseline over all rows in group1.</span>
                        <span class="c1"># This represents the expected average attention to group2 if it were uniformly distributed.</span>
                        <span class="n">baseline_val</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">baseline_list</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">baseline_list</span><span class="p">)</span>

                        <span class="c1"># ---- Step 3. Compute the final ratio.</span>
                        <span class="c1"># We compare the observed average attention to the baseline expectation.</span>
                        <span class="c1"># A value &gt; 1 means that, on average, attention in this block is higher than expected.</span>
                        <span class="c1"># Expand baseline_val to match the batch shape for element-wise division.</span>
                        <span class="n">baseline_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                            <span class="n">baseline_val</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">observed_val</span><span class="o">.</span><span class="n">device</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">observed_val</span><span class="p">)</span>
                        <span class="n">avg_val</span> <span class="o">=</span> <span class="n">observed_val</span> <span class="o">/</span> <span class="n">baseline_tensor</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">avg_val</span> <span class="o">=</span> <span class="n">attn_block</span>

                    <span class="k">if</span> <span class="n">attn_pattern_avg</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                        <span class="c1"># Append the computed metric for this block (keeping the batch dimension).</span>
                        <span class="n">group_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># shape: [batch, 1]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># If no averaging is requested, store the block directly.</span>
                        <span class="n">group_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_block</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn_pattern_avg</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                <span class="n">pattern_avg</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">group_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                    <span class="s2">&quot;batch (G1 G2) -&gt; batch G1 G2&quot;</span><span class="p">,</span>
                    <span class="n">G1</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">token_indexes_group1</span><span class="p">),</span>
                    <span class="n">G2</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">token_indexes</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">pattern_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">group_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Error concatenating group_avgs with shapes </span><span class="si">{</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">group_avgs</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

            <span class="c1"># Add the pattern to the cache</span>
            <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pattern_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pattern_avg</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Without averaging, flatten token_indexes into one list.</span>
        <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">head_indices</span><span class="p">:</span>
            <span class="c1"># Slice both token dimensions using the flattened indexes.</span>
            <span class="n">pattern_slice</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">h</span><span class="p">,</span> <span class="n">flatten_indexes</span><span class="p">][:,</span> <span class="p">:,</span> <span class="n">flatten_indexes</span><span class="p">]</span>
            <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pattern_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pattern_slice</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.avg_attention_pattern_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">avg_attention_pattern_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">attn_pattern_current_avg</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">extract_avg_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it.
As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="b">b</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function. It's the output of the attention pattern of the heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="s">s</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the state of the hook function. It's the state of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="layer">layer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="head">head</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code><span title="attn_pattern_current_avg">attn_pattern_current_avg</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the current average attention pattern</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">avg_attention_pattern_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">attn_pattern_current_avg</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">extract_avg_value</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it.</span>
<span class="sd">    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</span>

<span class="sd">    Args:</span>
<span class="sd">        - b: the input of the hook function. It&#39;s the output of the attention pattern of the heads</span>
<span class="sd">        - s: the state of the hook function. It&#39;s the state of the model</span>
<span class="sd">        - layer: the layer of the model</span>
<span class="sd">        - head: the head of the model</span>
<span class="sd">        - attn_pattern_current_avg: the current average attention pattern</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># first get the attention pattern</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">attn_pattern</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads,seq_len, seq_len)</span>
    <span class="c1"># attn_pattern = attn_pattern.to(torch.float32)</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">token_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;avg_pattern_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">attn_pattern_current_avg</span><span class="p">:</span>
            <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">][</span>
                <span class="p">:,</span> <span class="p">:,</span> <span class="n">token_indexes</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">][:,</span> <span class="p">:,</span> <span class="n">token_indexes</span><span class="p">]</span>
                <span class="o">-</span> <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="c1"># var_key = f&quot;M2_pattern_L{layer}H{head}&quot;</span>
        <span class="c1"># if var_key not in attn_pattern_current_avg:</span>
        <span class="c1">#     attn_pattern_current_avg[var_key] = torch.zeros_like(attn_pattern[:, head])</span>
        <span class="c1"># attn_pattern_current_avg[var_key] = attn_pattern_current_avg[var_key] + (attn_pattern[:, head] - attn_pattern_current_avg[key]) * (attn_pattern[:, head] - attn_pattern_current_avg[var_key])</span>

        <span class="k">if</span> <span class="n">extract_avg_value</span><span class="p">:</span>
            <span class="n">value_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;projected_value_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">values</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Values not found for </span><span class="si">{</span><span class="n">value_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">return</span>
            <span class="c1"># get the attention pattern for the values</span>
            <span class="n">value_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">norm_matrix</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">value_norm</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">norm_matrix</span> <span class="o">=</span> <span class="n">norm_matrix</span> <span class="o">*</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">value_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">attn_pattern_current_avg</span><span class="p">:</span>
                <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm_matrix</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="n">norm_matrix</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="o">-</span> <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span>
                <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># remove values from cache</span>
            <span class="k">del</span> <span class="n">cache</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.avg_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">avg_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">last_image_idx</span><span class="p">,</span> <span class="n">end_image_idx</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">avg_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">last_image_idx</span><span class="p">,</span>
    <span class="n">end_image_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">img_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">last_image_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">text_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="n">end_image_idx</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">all_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;avg_</span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">img_avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">text_avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">all_avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.compute_statistics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_statistics</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Computes the mean, variance, and second moment of a given tensor along a specified dimension.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensor</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dim</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dimension along which to compute statistics (default: -1).</p>
              </div>
            </td>
            <td>
                  <code>-1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>keepdim</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to keep the reduced dimension (default: True).</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eps</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Small constant for numerical stability.</p>
              </div>
            </td>
            <td>
                  <code>1e-06</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>tuple</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(mean, variance, second_moment)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_statistics</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the mean, variance, and second moment of a given tensor along a specified dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (torch.Tensor): Input tensor.</span>
<span class="sd">        dim (int): Dimension along which to compute statistics (default: -1).</span>
<span class="sd">        keepdim (bool): Whether to keep the reduced dimension (default: True).</span>
<span class="sd">        eps (float): Small constant for numerical stability.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: (mean, variance, second_moment)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>  <span class="c1"># Compute mean</span>
    <span class="n">second_moment</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span>
    <span class="p">)</span>  <span class="c1"># Compute second moment</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">second_moment</span> <span class="o">-</span> <span class="n">mean</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Compute variance using E[X²] - (E[X])²</span>

    <span class="k">return</span> <span class="n">mean</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">variance</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">second_moment</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.create_dynamic_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_dynamic_hook</span><span class="p">(</span><span class="n">pyvene_hook</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>DEPRECATED: pyvene is not used anymore.
This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_dynamic_hook</span><span class="p">(</span><span class="n">pyvene_hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DEPRECATED: pyvene is not used anymore.</span>
<span class="sd">    This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">partial_hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">pyvene_hook</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wrap</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">partial_hook</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrap</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.embed_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">embed_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the embeddings of the specified tokens and save them in the cache.
Args:
    module: The module being hooked.
    args: Positional arguments.
    kwargs: Keyword arguments.
    output: Output from the module.
    token_indexes: List of token indexes to extract.
    cache: The cache object to store results.
    cache_key: The key under which to store the embeddings.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">embed_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the embeddings of the specified tokens and save them in the cache.</span>
<span class="sd">    Args:</span>
<span class="sd">        module: The module being hooked.</span>
<span class="sd">        args: Positional arguments.</span>
<span class="sd">        kwargs: Keyword arguments.</span>
<span class="sd">        output: Output from the module.</span>
<span class="sd">        token_indexes: List of token indexes to extract.</span>
<span class="sd">        cache: The cache object to store results.</span>
<span class="sd">        cache_key: The key under which to store the embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token_index</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_index</span><span class="p">)])</span>
    <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.input_embedding_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">input_embedding_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">keep_gradient</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook to capture the output of the embedding layer, enable gradients, and store it in the cache.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">input_embedding_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">keep_gradient</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook to capture the output of the embedding layer, enable gradients, and store it in the cache.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">embeddings_tensor</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">keep_gradient</span><span class="p">:</span>
        <span class="c1"># Enable gradient tracking for the embeddings tensor</span>
        <span class="n">embeddings_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">embeddings_tensor</span>  <span class="c1"># we slice in the end if keep gradient</span>
        <span class="k">return</span> <span class="n">restore_same_args_kwargs_output</span><span class="p">(</span>
            <span class="n">embeddings_tensor</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span>
        <span class="p">)</span>  <span class="c1"># Return the original (potentially modified in-place) output structure</span>
    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="n">token_avgs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_tuple</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
            <span class="c1"># Slice out the tokens specified by the tuple.</span>
            <span class="n">token_slice</span> <span class="o">=</span> <span class="n">embeddings_tensor</span><span class="p">[:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_tuple</span><span class="p">),</span> <span class="p">:]</span>
            <span class="c1"># Average over the token dimension (dim=1) and keep that dimension.</span>
            <span class="n">token_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token_slice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">token_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_avg</span><span class="p">)</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="n">token_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># Store the tensor that&#39;s part of the graph</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">]</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">embeddings_tensor</span><span class="p">[</span>
            <span class="p">:,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:</span>
        <span class="p">]</span>  <span class="c1"># Store the tensor that&#39;s part of the graph</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">output</span>  <span class="c1"># Return the original (potentially modified in-place) output structure</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.intervention_attn_mat_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">intervention_attn_mat_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">q_positions</span><span class="p">,</span> <span class="n">k_positions</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">multiplication_value</span><span class="p">,</span> <span class="n">patching_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the tokens in the attention
mask. It will set to 0 the value vector of the
tokens to ablate</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">intervention_attn_mat_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">q_positions</span><span class="p">,</span>
    <span class="n">k_positions</span><span class="p">,</span>
    <span class="n">head</span><span class="p">,</span>
    <span class="n">multiplication_value</span><span class="p">,</span>
    <span class="n">patching_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">apply_softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="c1"># ablation_queries: pd.DataFrame,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the tokens in the attention</span>
<span class="sd">    mask. It will set to 0 the value vector of the</span>
<span class="sd">    tokens to ablate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the shape of the attention matrix</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len_q</span><span class="p">,</span> <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Used during generation</span>
    <span class="k">if</span> <span class="n">seq_len_q</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_positions</span><span class="p">):</span>
        <span class="n">q_positions</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Create boolean masks for queries and keys</span>
    <span class="n">q_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len_q</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">q_mask</span><span class="p">[</span><span class="n">q_positions</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Set positions to True</span>

    <span class="n">k_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len_k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">k_mask</span><span class="p">[</span><span class="n">k_positions</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Set positions to TrueW</span>

    <span class="c1"># Create a 2D mask using outer product</span>
    <span class="n">head_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">q_mask</span><span class="p">,</span> <span class="n">k_mask</span><span class="p">)</span>  <span class="c1"># Shape: (seq_len_q, seq_len_k)</span>

    <span class="c1"># Expand mask to match the dimensions of the attention matrix</span>
    <span class="c1"># Shape after expand: (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    <span class="c1"># head_mask = (</span>
    <span class="c1">#     head_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads, -1, -1)</span>
    <span class="c1"># )</span>

    <span class="c1"># select the head</span>
    <span class="c1"># head_mask = head_mask[:, head, :, :]</span>

    <span class="k">if</span> <span class="n">patching_values</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">patching_values</span> <span class="o">==</span> <span class="s2">&quot;ablation&quot;</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;No patching values provided, ablation will be performed&quot;</span><span class="p">)</span>
        <span class="c1"># Apply the ablation function directly to the attention matrix</span>
        <span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">head_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">multiply_pattern</span><span class="p">(</span>
            <span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">head_mask</span><span class="p">],</span> <span class="n">multiplication_value</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Apply the patching values to the attention matrix</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Patching values provided, applying patching values&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;Patching values shape: </span><span class="si">%s</span><span class="s2">. It is expected to have shape seq_len x seq_len&quot;</span><span class="p">,</span>
            <span class="n">patching_values</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">head_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">patching_values</span><span class="p">[</span><span class="n">head_mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
        <span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.intervention_heads_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">intervention_heads_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the heads in the attention
mask. It will set to 0 the output of the heads to
ablate</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">intervention_heads_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the heads in the attention</span>
<span class="sd">    mask. It will set to 0 the output of the heads to</span>
<span class="sd">    ablate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>

    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">]:</span>
        <span class="n">attention_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">b</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.intervention_query_key_value_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">intervention_query_key_value_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">patching_values</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to intervene on the query, key and value vectors. It first unpack the vectors from the output of the module and then apply the intervention and then repack the vectors.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">intervention_query_key_value_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">head</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="p">,</span>
    <span class="n">num_key_value_groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">patching_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to intervene on the query, key and value vectors. It first unpack the vectors from the output of the module and then apply the intervention and then repack the vectors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="c1"># input_shape = b.shape[-1]</span>
    <span class="c1"># hidden_shape = (*input_shape, -1, head_dim)</span>
    <span class="n">hidden_shape</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># b = b.view(hidden_shape).transpose(1, 2)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">num_key_value_groups</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_attention_heads</span> <span class="o">*</span> <span class="n">head_dim</span>
    <span class="p">):</span>  <span class="c1"># we are in kv group attention</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span>
            <span class="n">b</span><span class="p">,</span>
            <span class="s2">&quot;batch seq_len (num_attention_heads head_dim) -&gt; batch num_attention_heads seq_len head_dim&quot;</span><span class="p">,</span>
            <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">num_attention_heads</span> <span class="o">//</span> <span class="n">num_key_value_groups</span><span class="p">,</span>
            <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># check if we are using kv group attention</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span>
            <span class="n">b</span><span class="p">,</span>
            <span class="s2">&quot;batch seq_len (num_attention_heads head_dim) -&gt; batch num_attention_heads seq_len head_dim&quot;</span><span class="p">,</span>
            <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># tensor_head = b.data.detach().clone()[:, head, ...]</span>
    <span class="c1"># Apply the intervention</span>
    <span class="k">if</span> <span class="n">patching_values</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">patching_values</span> <span class="o">==</span> <span class="s2">&quot;ablation&quot;</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;No patching values provided, ablation will be performed on the query, key and value vectors&quot;</span>
        <span class="p">)</span>
        <span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;Patching values provided, applying patching values to the query, key and value vectors&quot;</span>
        <span class="p">)</span>
        <span class="n">b</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_indexes</span><span class="p">),</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">patching_values</span>

    <span class="c1"># Repack the vectors</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span>
        <span class="n">b</span><span class="p">,</span>
        <span class="s2">&quot;batch num_attention_heads seq_len head_dim -&gt; batch seq_len (num_attention_heads head_dim)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Restore the args and kwargs</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">restore_same_args_kwargs_output</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.intervention_resid_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">intervention_resid_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">patching_values</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the tokens in the residual stream. It will set to 0 the value vector of the
tokens to ablate</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">intervention_resid_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">patching_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the tokens in the residual stream. It will set to 0 the value vector of the</span>
<span class="sd">    tokens to ablate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="c1"># detach b to avoid modifying the original tensor</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">patching_values</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">patching_values</span> <span class="o">==</span> <span class="s2">&quot;ablation&quot;</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;No patching values provided, ablation will be performed on the residual stream&quot;</span>
        <span class="p">)</span>
        <span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="s2">&quot;Patching values provided, applying patching values to the residual stream&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_indexes</span><span class="p">),</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">patching_values</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Shape mismatch: activations is </span><span class="si">{</span><span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="w"> </span><span class="nb">list</span><span class="p">(</span><span class="n">token_indexes</span><span class="p">),</span><span class="w"> </span><span class="p">:]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> but patching values is </span><span class="si">{</span><span class="n">patching_values</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_indexes</span><span class="p">),</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">patching_values</span>
    <span class="k">return</span> <span class="n">restore_same_args_kwargs_output</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.layernom_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">layernom_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Compute and save mean, variance, and second moment for the specified token indexes.
If avg is True, computes statistics for each token group; otherwise, flattens indexes.
Args:
    module: The module being hooked.
    args: Positional arguments.
    kwargs: Keyword arguments.
    output: Output from the module.
    token_indexes: List of token index groups.
    cache: The cache object to store results.
    cache_key: The key under which to store the statistics.
    avg: Whether to average over each token group separately.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">layernom_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute and save mean, variance, and second moment for the specified token indexes.</span>
<span class="sd">    If avg is True, computes statistics for each token group; otherwise, flattens indexes.</span>
<span class="sd">    Args:</span>
<span class="sd">        module: The module being hooked.</span>
<span class="sd">        args: Positional arguments.</span>
<span class="sd">        kwargs: Keyword arguments.</span>
<span class="sd">        output: Output from the module.</span>
<span class="sd">        token_indexes: List of token index groups.</span>
<span class="sd">        cache: The cache object to store results.</span>
<span class="sd">        cache_key: The key under which to store the statistics.</span>
<span class="sd">        avg: Whether to average over each token group separately.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="n">token_avgs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_index</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
            <span class="n">slice_</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_index</span><span class="p">),</span> <span class="p">:]</span>
            <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">second_moment</span> <span class="o">=</span> <span class="n">compute_statistics</span><span class="p">(</span><span class="n">slice_</span><span class="p">)</span>
            <span class="n">token_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span> <span class="s2">&quot;second_moment&quot;</span><span class="p">:</span> <span class="n">second_moment</span><span class="p">}</span>
            <span class="p">)</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_avgs</span>
    <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">second_moment</span> <span class="o">=</span> <span class="n">compute_statistics</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span>
        <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s2">&quot;second_moment&quot;</span><span class="p">:</span> <span class="n">second_moment</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.multiply_pattern" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">multiply_pattern</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">multiplication_value</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Set the attention values to zero</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">multiply_pattern</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">multiplication_value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the attention values to zero</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># return torch.zeros_like(tensor) + multiplication_value</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="o">*</span> <span class="n">multiplication_value</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.process_args_kwargs_output" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Extract the main tensor from output, args, or kwargs.
Prioritizes output (first element if tuple), then first arg, then kwargs['hidden_states'] if present.
Args:
    args: Positional arguments from the hook.
    kwargs: Keyword arguments from the hook.
    output: Output from the hooked function.
Returns:
    The main tensor to be processed by the hook.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract the main tensor from output, args, or kwargs.</span>
<span class="sd">    Prioritizes output (first element if tuple), then first arg, then kwargs[&#39;hidden_states&#39;] if present.</span>
<span class="sd">    Args:</span>
<span class="sd">        args: Positional arguments from the hook.</span>
<span class="sd">        kwargs: Keyword arguments from the hook.</span>
<span class="sd">        output: Output from the hooked function.</span>
<span class="sd">    Returns:</span>
<span class="sd">        The main tensor to be processed by the hook.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">output</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">candidate_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">candidate_keys</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                    <span class="k">break</span>
    <span class="k">return</span> <span class="n">b</span>  <span class="c1"># type:ignore</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.projected_key_vectors_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">projected_key_vectors_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">expand_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the key vectors of the heads and project them through the attention output matrix.
This shows the contribution that keys in each position could have to the residual stream through the attention mechanism.</p>
<p>Like other hooks, it saves the activations in the cache.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>b</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function (output of the key vectors)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="str">str</span>, <span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model. If "all" is passed, it will extract all the heads of the layer</p>
              </div>
            </td>
            <td>
                  <code>&#39;all&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>expand_head</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool to expand the head dimension when extracting the keys vectors</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">projected_key_vectors_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_key_value_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_proj_weight</span><span class="p">,</span>
    <span class="n">out_proj_bias</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">expand_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the key vectors of the heads and project them through the attention output matrix.</span>
<span class="sd">    This shows the contribution that keys in each position could have to the residual stream through the attention mechanism.</span>

<span class="sd">    Like other hooks, it saves the activations in the cache.</span>

<span class="sd">    Args:</span>
<span class="sd">        b: the input of the hook function (output of the key vectors)</span>
<span class="sd">        layer: the layer of the model</span>
<span class="sd">        head: the head of the model. If &quot;all&quot; is passed, it will extract all the heads of the layer</span>
<span class="sd">        expand_head: bool to expand the head dimension when extracting the keys vectors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the key vectors</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads, seq_len, head_dim)</span>

    <span class="c1"># Reshape the key vectors to have a separate dimension for the different heads</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">keys</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len (num_key_value_heads d_heads) -&gt; batch num_key_value_heads seq_len d_heads&quot;</span><span class="p">,</span>
        <span class="n">num_key_value_heads</span><span class="o">=</span><span class="n">num_key_value_heads</span><span class="p">,</span>
        <span class="n">d_heads</span><span class="o">=</span><span class="n">d_head</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># If needed, repeat KV heads to match attention heads (for grouped query attention)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">num_attention_heads</span> <span class="o">//</span> <span class="n">num_key_value_heads</span><span class="p">)</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">keys</span><span class="p">,</span>
        <span class="s2">&quot;batch num_head seq_len d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Reshape out_proj_weight to get the blocks for each head</span>
    <span class="n">out_proj_weight</span> <span class="o">=</span> <span class="n">out_proj_weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">num_attention_heads</span><span class="p">,</span>
        <span class="n">d_head</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Apply bias if present</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_proj_bias</span> <span class="o">=</span> <span class="n">out_proj_bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="c1"># Apply the projection for each head</span>
    <span class="n">projected_keys</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span>
        <span class="n">keys</span><span class="p">,</span>
        <span class="n">out_proj_weight</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">projected_keys</span> <span class="o">=</span> <span class="n">projected_keys</span> <span class="o">+</span> <span class="n">out_proj_bias</span>

    <span class="c1"># Rearrange the tensor to have dimensions that we prefer</span>
    <span class="n">projected_keys</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">projected_keys</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_model -&gt; batch num_head seq_len d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Process token indices</span>
    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="c1"># For each tuple, slice out the tokens and average over them</span>
        <span class="n">token_avgs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_tuple</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
            <span class="n">token_slice</span> <span class="o">=</span> <span class="n">projected_keys</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_tuple</span><span class="p">),</span> <span class="p">:]</span>
            <span class="n">token_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token_slice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">token_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_avg</span><span class="p">)</span>
        <span class="n">projected_keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">token_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">]</span>
        <span class="n">projected_keys</span> <span class="o">=</span> <span class="n">projected_keys</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:]</span>

    <span class="c1"># Save to cache based on selected heads</span>
    <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">):</span>
            <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_key_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_keys</span><span class="p">[:,</span> <span class="n">head_idx</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_key_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_keys</span><span class="p">[:,</span> <span class="nb">int</span><span class="p">(</span><span class="n">head</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.projected_query_vectors_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">projected_query_vectors_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">expand_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the query vectors of the heads and project them through the attention output matrix.
This shows the contribution that queries in each position could have to the residual stream through the attention mechanism.</p>
<p>Like other hooks, it saves the activations in the cache.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>b</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function (output of the query vectors)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="str">str</span>, <span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model. If "all" is passed, it will extract all the heads of the layer</p>
              </div>
            </td>
            <td>
                  <code>&#39;all&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>expand_head</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool to expand the head dimension when extracting the query vectors</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">projected_query_vectors_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_key_value_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_proj_weight</span><span class="p">,</span>
    <span class="n">out_proj_bias</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">expand_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the query vectors of the heads and project them through the attention output matrix.</span>
<span class="sd">    This shows the contribution that queries in each position could have to the residual stream through the attention mechanism.</span>

<span class="sd">    Like other hooks, it saves the activations in the cache.</span>

<span class="sd">    Args:</span>
<span class="sd">        b: the input of the hook function (output of the query vectors)</span>
<span class="sd">        layer: the layer of the model</span>
<span class="sd">        head: the head of the model. If &quot;all&quot; is passed, it will extract all the heads of the layer</span>
<span class="sd">        expand_head: bool to expand the head dimension when extracting the query vectors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the query vectors</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">queries</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, seq_len, num_heads*d_head)</span>

    <span class="c1"># Reshape the query vectors to have a separate dimension for the heads</span>
    <span class="n">queries</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">queries</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len (num_attention_heads d_heads) -&gt; batch num_attention_heads seq_len d_heads&quot;</span><span class="p">,</span>
        <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">num_attention_heads</span><span class="p">,</span>
        <span class="n">d_heads</span><span class="o">=</span><span class="n">d_head</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">queries</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">queries</span><span class="p">,</span>
        <span class="s2">&quot;batch num_head seq_len d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Reshape out_proj_weight to get the blocks for each head</span>
    <span class="n">out_proj_weight</span> <span class="o">=</span> <span class="n">out_proj_weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">num_attention_heads</span><span class="p">,</span>
        <span class="n">d_head</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Apply bias if present</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_proj_bias</span> <span class="o">=</span> <span class="n">out_proj_bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="c1"># Apply the projection for each head</span>
    <span class="n">projected_queries</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span>
        <span class="n">queries</span><span class="p">,</span>
        <span class="n">out_proj_weight</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">projected_queries</span> <span class="o">=</span> <span class="n">projected_queries</span> <span class="o">+</span> <span class="n">out_proj_bias</span>

    <span class="c1"># Rearrange the tensor to have dimensions that we prefer</span>
    <span class="n">projected_queries</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">projected_queries</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_model -&gt; batch num_head seq_len d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Process token indices</span>
    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="c1"># For each tuple, slice out the tokens and average over them</span>
        <span class="n">token_avgs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_tuple</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
            <span class="n">token_slice</span> <span class="o">=</span> <span class="n">projected_queries</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_tuple</span><span class="p">),</span> <span class="p">:]</span>
            <span class="n">token_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token_slice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">token_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_avg</span><span class="p">)</span>
        <span class="n">projected_queries</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">token_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">]</span>
        <span class="n">projected_queries</span> <span class="o">=</span> <span class="n">projected_queries</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:]</span>

    <span class="c1"># Save to cache based on selected heads</span>
    <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">):</span>
            <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_query_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_queries</span><span class="p">[</span>
                <span class="p">:,</span> <span class="n">head_idx</span>
            <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_query_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_queries</span><span class="p">[:,</span> <span class="nb">int</span><span class="p">(</span><span class="n">head</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.projected_value_vectors_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">projected_value_vectors_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">expand_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection
As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>b</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function. It's the output of the values vectors of the heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the state of the hook function. It's the state of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="str">str</span>, <span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model. If "all" is passed, it will extract all the heads of the layer</p>
              </div>
            </td>
            <td>
                  <code>&#39;all&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>expand_head</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like "value_L0H0", "value_L0H1", ...
            while if False, we will have only one key for each layer, like "value_L0" and the dimension of the head will be taken into account in the tensor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">projected_value_vectors_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_key_value_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_proj_weight</span><span class="p">,</span>
    <span class="n">out_proj_bias</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">expand_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection</span>
<span class="sd">    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</span>

<span class="sd">    Args:</span>
<span class="sd">        b: the input of the hook function. It&#39;s the output of the values vectors of the heads</span>
<span class="sd">        s: the state of the hook function. It&#39;s the state of the model</span>
<span class="sd">        layer: the layer of the model</span>
<span class="sd">        head: the head of the model. If &quot;all&quot; is passed, it will extract all the heads of the layer</span>
<span class="sd">        expand_head: bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like &quot;value_L0H0&quot;, &quot;value_L0H1&quot;, ...</span>
<span class="sd">                        while if False, we will have only one key for each layer, like &quot;value_L0&quot; and the dimension of the head will be taken into account in the tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># first get the values vectors</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads,seq_len, head_dim)</span>

    <span class="c1"># reshape the values vectors to have a separate dimension for the different heads</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">values</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len (num_key_value_heads d_heads) -&gt; batch num_key_value_heads seq_len d_heads&quot;</span><span class="p">,</span>
        <span class="n">num_key_value_heads</span><span class="o">=</span><span class="n">num_key_value_heads</span><span class="p">,</span>
        <span class="n">d_heads</span><span class="o">=</span><span class="n">d_head</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1">#        &quot;batch seq_len (num_key_value_heads d_heads) -&gt; batch seq_len num_key_value_heads d_heads&quot;,</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">num_attention_heads</span> <span class="o">//</span> <span class="n">num_key_value_heads</span><span class="p">)</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">values</span><span class="p">,</span>
        <span class="s2">&quot;batch num_head seq_len d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># reshape in order to get the blocks for each head</span>
    <span class="n">out_proj_weight</span> <span class="o">=</span> <span class="n">out_proj_weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">num_attention_heads</span><span class="p">,</span>
        <span class="n">d_head</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># apply bias if present (No in Chameleon)</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_proj_bias</span> <span class="o">=</span> <span class="n">out_proj_bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="c1"># apply the projection for each head</span>
    <span class="n">projected_values</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span>
        <span class="n">values</span><span class="p">,</span>
        <span class="n">out_proj_weight</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">projected_values</span> <span class="o">=</span> <span class="n">projected_values</span> <span class="o">+</span> <span class="n">out_proj_bias</span>

    <span class="c1"># rearrange the tensor to have dimension that we like more</span>
    <span class="n">projected_values</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">projected_values</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_model -&gt; batch num_head seq_len d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># slice for token index</span>
    <span class="c1"># Assume projected_values has shape [batch, num_heads, tokens, d_model]</span>
    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="c1"># For each tuple, slice the tokens along dimension -2 and average over that token slice.</span>
        <span class="n">token_avgs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_tuple</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
            <span class="c1"># Slice out the tokens for this tuple.</span>
            <span class="c1"># Using ellipsis ensures we index the last two dimensions correctly.</span>
            <span class="n">token_slice</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_tuple</span><span class="p">),</span> <span class="p">:]</span>
            <span class="c1"># Average over the token dimension (which is -2) while keeping that dimension.</span>
            <span class="n">token_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token_slice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">token_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_avg</span><span class="p">)</span>
        <span class="c1"># Concatenate the averaged slices along the token dimension (-2).</span>
        <span class="n">projected_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">token_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Flatten the list of token tuples into a single list of token indices.</span>
        <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">]</span>
        <span class="n">projected_values</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:]</span>

    <span class="c1"># Post-process the value vectors by selecting heads.</span>
    <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">):</span>
            <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_value_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[</span>
                <span class="p">:,</span> <span class="n">head_idx</span>
            <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_value_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[:,</span> <span class="nb">int</span><span class="p">(</span><span class="n">head</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.query_key_value_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">query_key_value_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">query_key_value_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">:</span> <span class="n">ActivationCache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="p">,</span>
    <span class="n">num_key_value_groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">hidden_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># cache[cache_key] = b.data.detach().clone()[..., token_index, :]</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">num_key_value_groups</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_attention_heads</span>
    <span class="p">):</span>  <span class="c1"># we are in kv group attention</span>
        <span class="c1"># we need to repeat the key and value states</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_attention_heads</span> <span class="o">//</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># check if we are using kv group attention</span>

    <span class="n">info_string</span> <span class="o">=</span> <span class="s2">&quot;Shape: batch seq_len d_head&quot;</span>

    <span class="n">heads</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))]</span> <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">head</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">:</span>
        <span class="c1"># Compute the group index for keys/values if needed.</span>
        <span class="n">group_idx</span> <span class="o">=</span> <span class="n">head_idx</span> <span class="o">//</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="c1"># Decide whether to use group_idx or head_idx based on cache_key.</span>
        <span class="k">if</span> <span class="s2">&quot;head_values_&quot;</span> <span class="ow">in</span> <span class="n">cache_key</span> <span class="ow">or</span> <span class="s2">&quot;head_keys_&quot;</span> <span class="ow">in</span> <span class="n">cache_key</span><span class="p">:</span>
            <span class="c1"># Select the slice corresponding to the group index.</span>
            <span class="n">tensor_slice</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="n">group_idx</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use the head index directly.</span>
            <span class="n">tensor_slice</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="n">head_idx</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

        <span class="c1"># Process the token indexes.</span>
        <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
            <span class="c1"># For each token tuple, average over the tokens.</span>
            <span class="c1"># Note: After slicing, the token dimension is the first dimension of tensor_slice,</span>
            <span class="c1"># i.e. tensor_slice has shape (batch, tokens, d_head) so we average along dim=1.</span>
            <span class="n">tokens_avgs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">token_tuple</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
                <span class="c1"># Slice tokens using the token_tuple.</span>
                <span class="n">token_subslice</span> <span class="o">=</span> <span class="n">tensor_slice</span><span class="p">[:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_tuple</span><span class="p">),</span> <span class="p">:]</span>
                <span class="c1"># Average over the token dimension (dim=1) and keep that dimension.</span>
                <span class="n">token_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token_subslice</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">tokens_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_avg</span><span class="p">)</span>
            <span class="c1"># Concatenate the averages along the token dimension (dim=1).</span>
            <span class="n">processed_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tokens_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Flatten the token indexes from the list of tuples.</span>
            <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">]</span>
            <span class="n">processed_tokens</span> <span class="o">=</span> <span class="n">tensor_slice</span><span class="p">[:,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Build a unique key for the cache by including layer and head information.</span>
        <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">cache</span><span class="o">.</span><span class="n">add_with_info</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">processed_tokens</span><span class="p">,</span> <span class="n">info_string</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.restore_same_args_kwargs_output" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">restore_same_args_kwargs_output</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Restore the structure of output, args, and kwargs after modification.
Args:
    b: The new tensor to insert.
    args: Original positional arguments.
    kwargs: Original keyword arguments.
    output: Original output from the hooked function.
Returns:
    The updated output, or (args, kwargs) if output is None.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">restore_same_args_kwargs_output</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Restore the structure of output, args, and kwargs after modification.</span>
<span class="sd">    Args:</span>
<span class="sd">        b: The new tensor to insert.</span>
<span class="sd">        args: Original positional arguments.</span>
<span class="sd">        kwargs: Original keyword arguments.</span>
<span class="sd">        output: Original output from the hooked function.</span>
<span class="sd">    Returns:</span>
<span class="sd">        The updated output, or (args, kwargs) if output is None.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">candidate_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">candidate_keys</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
                    <span class="n">kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
                    <span class="k">break</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
    <span class="k">return</span> <span class="n">b</span>  <span class="c1"># type:ignore</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.save_resid_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">save_resid_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">token_indexes</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Save the activations of the residual stream for the specified token indexes in the cache.
If avg is True, saves averaged activations for each token group.
Args:
    module: The module being hooked.
    args: Positional arguments.
    kwargs: Keyword arguments.
    output: Output from the module.
    cache: The cache object to store results.
    cache_key: The key under which to store the activations.
    token_indexes: List of token index groups.
    avg: Whether to average over each token group separately.</p>


            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_resid_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">:</span> <span class="n">ActivationCache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">token_indexes</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the activations of the residual stream for the specified token indexes in the cache.</span>
<span class="sd">    If avg is True, saves averaged activations for each token group.</span>
<span class="sd">    Args:</span>
<span class="sd">        module: The module being hooked.</span>
<span class="sd">        args: Positional arguments.</span>
<span class="sd">        kwargs: Keyword arguments.</span>
<span class="sd">        output: Output from the module.</span>
<span class="sd">        cache: The cache object to store results.</span>
<span class="sd">        cache_key: The key under which to store the activations.</span>
<span class="sd">        token_indexes: List of token index groups.</span>
<span class="sd">        avg: Whether to average over each token group separately.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="c1"># slice the tensor to get the activations of the token we want to extract</span>
    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="n">token_avgs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_index</span> <span class="ow">in</span> <span class="n">token_indexes</span><span class="p">:</span>
            <span class="n">slice_</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[</span><span class="o">...</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_index</span><span class="p">),</span> <span class="p">:]</span>
            <span class="n">token_avgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">slice_</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># cache[cache_key] = torch.cat(token_avgs, dim=-2)</span>
        <span class="n">cache</span><span class="o">.</span><span class="n">add_with_info</span><span class="p">(</span>
            <span class="n">cache_key</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">token_avgs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">),</span>
            <span class="s2">&quot;Shape: batch avg_over_target_token_position, d_model&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">flatten_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">token_indexes</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[</span><span class="o">...</span><span class="p">,</span> <span class="n">flatten_indexes</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.50899def.min.js"></script>
      
    
  </body>
</html>